

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Chapter 15 - Estimating Populations &#8212; Pomona Psych 158 Online Textbook</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapter-15';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Pomona College Psych 158 Online Textbook
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Unit 1 Describing Data</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-1.ipynb">Chapter 1 - Intro to Doing Statistics</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-2.ipynb">Chapter 2 - Statistical Reasoning</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-3.ipynb">Chapter 3 - What are Data</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-4.ipynb">Chapter 4 - Organizing Data</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-5.ipynb">Chapter 5 - Describing Data</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-6.ipynb">Chapter 6 - Variation in Multiple Variables</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-7.ipynb">Chapter 7 - Principles of Data Visualization</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Unit 2 - Modeling Data</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-8.ipynb">Chapter 8 - Where Data Come From</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-9.ipynb">Chapter 9 - Modeling the Data Generation Process</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-10.ipynb">Chapter 10 - Quantifying Model Error</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-11.ipynb">Chapter 11 - Adding an Explanatory Variable</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-12.ipynb">Chapter 12 - Quantitative Predictor Models</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-13.ipynb">Chapter 13 - Multivariable Models</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-14.ipynb">Chapter 14 - Models with Moderation</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Unit 3 - Evaluating Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-15.ipynb">Chapter 15 - Estimating Populations</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-16.ipynb">Chapter 16 - Significance Testing</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-17.ipynb">Chapter 17 - Significance Testing Whole Models</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-18.ipynb">Chapter 18 - Effect Sizes &amp; Power</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-19.ipynb">Chapter 19 - Alternate Approaches - Traditional Inference Methods</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-20.ipynb">Chapter 20 - Alternate Approaches - Bayesian Statistics</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-21.ipynb">Chapter 21 - Bias due to Improper Model Building</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-22.ipynb">Chapter 22 - Bias due to Improper Model Selection</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/smburns47/Psyc158/main?urlpath=tree/chapter-15.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onBinder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="initThebeSBT()"
  class="btn btn-sm btn-launch-thebe dropdown-item"
  title="Launch Thebe"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="btn__text-container">Live Code</span>
</button>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/smburns47/Psyc158" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/smburns47/Psyc158/issues/new?title=Issue%20on%20page%20%2Fchapter-15.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/chapter-15.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Chapter 15 - Estimating Populations</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluating-model-performance">15.1 Evaluating model performance</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sampling-distributions">15.2 Sampling distributions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-importance-of-sample-size">15.3 The importance of sample size</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-central-limit-theorem">15.4 The Central Limit Theorem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-population-parameters">15.5 Estimating population parameters</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-the-population-mean">Estimating the population mean</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-the-population-standard-deviation">Estimating the population standard deviation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sampling-distributions-of-model-outputs">15.6 Sampling distributions of model outputs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#confidence-intervals">15.7 Confidence intervals</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pitfalls-of-interpreting-confidence-intervals">Pitfalls of interpreting confidence intervals</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-uncertainty">15.8 Visualizing uncertainty</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#chapter-summary">Chapter summary</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <p><a class="reference external" href="https://www.shannonmburns.com/Psyc158/intro.html">Back to Table of Contents</a></p>
<p><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-15.ipynb">Previous: Chapter 14 - Models with Moderation</a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Run this first so it&#39;s ready by the time you need it</span>
<span class="nf">install.packages</span><span class="p">(</span><span class="s">&quot;dplyr&quot;</span><span class="p">)</span>
<span class="nf">install.packages</span><span class="p">(</span><span class="s">&quot;ggformula&quot;</span><span class="p">)</span>
<span class="nf">install.packages</span><span class="p">(</span><span class="s">&quot;interactions&quot;</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">dplyr</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">ggformula</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">interactions</span><span class="p">)</span>
<span class="n">studentdata</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">read.csv</span><span class="p">(</span><span class="s">&quot;https://raw.githubusercontent.com/smburns47/Psyc158/main/studentdata.csv&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section class="tex2jax_ignore mathjax_ignore" id="chapter-15-estimating-populations">
<h1>Chapter 15 - Estimating Populations<a class="headerlink" href="#chapter-15-estimating-populations" title="Permalink to this heading">#</a></h1>
<section id="evaluating-model-performance">
<h2>15.1 Evaluating model performance<a class="headerlink" href="#evaluating-model-performance" title="Permalink to this heading">#</a></h2>
<p>We’ve spent a great deal of time so far learning about how to pull insights from sets of data - what the distribution of a variable looks like, how to predict new values based on other variables, etc. Ideally, these insights are useful not just for describing <em>this</em> dataset, but for many others you could possibly collect. We hope that our statistical results are good estimates of how things really are in the wider population. That way, we can make more general claims about broad categories that we’re interested in, like human psychology.</p>
<p>But we usually don’t have access to the entire population. Instead, we have to collect a smaller sample and hope that it is representative. In a sample like the <code class="docutils literal notranslate"><span class="pre">studentdata</span></code> dataset, which collected information from students in statistics class, we might find that we can sort of predict someone’s thumb length from how tall they are:</p>
<div class="math notranslate nohighlight">
\[\hat{Thumb_i} = 2.84 + 0.87Height_i\]</div>
<p>By fitting this linear model, we predict that someone who is 60 inches tall will have a thumb length of 55.04mm (2.84 + 0.87*60), and for every additional inch of height they have their thumb length will be 0.87mm longer. Now these guesses will almost certainly be wrong - the RMSE of this model is 9.12, meaning on average our guess will be off by about 9mm. But basing our guesses off of someone’s height makes our guesses at least better than just knowing the mean of thumb (12% better in fact, since the PRE score is 0.12).</p>
<p>Those RMSE and PRE scores are both measures of model performance called effect sizes. We will return to those in a few chapters, but it’s an important concept to start getting familiar with because it helps us decide what to <em>do</em> with our statistical model. Is it good enough? Am I confident in my predictions?</p>
<p>If we’re okay with guesses that are on average 9mm away, we can stop there and move on with our day. If we want to make even better guesses, we can try to build a more complex model with additional predictors. Ultimately, how confident we are in our predictions is the deciding factor for how good our statistical models are. We build psychological theories about relationships between variables in order to find the predictors that will improve predictions about how people think, feel, and behave.</p>
<p>With a large PRE score or low RMSE, our predictions are quite good. We are confident that the true values will not be too different… in this dataset, specifically. But one additional layer of uncertainty still at play is how good will this model be in <em>other</em> samples, or in the whole population?</p>
</section>
<section id="sampling-distributions">
<h2>15.2 Sampling distributions<a class="headerlink" href="#sampling-distributions" title="Permalink to this heading">#</a></h2>
<p>To answer this question, we must first remember that any one sample we collect won’t look perfectly similar to the population. If it’s not a representative sample, it’ll be different in important ways. But even if it’s sampled randomly, the law of large numbers tells us that randomness will cause some variations, particularly if the sample is much smaller than the population (which psychology samples always are, if our population of interest is the billions of humans who have lived or will ever live). Thus, the models we build using that sample can <em>themselves</em> vary sample to sample. The sample statistics won’t be the same as the population parameters, or across different datasets.</p>
<p>For example, let’s say we know the population mean of height for all Claremont College students is <span class="math notranslate nohighlight">\(\mu = 66\)</span> inches, and the standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span> is 3.5 inches. Then, a researcher comes along and picks 5 different samples of 50 students each, creating the following sample statistics:</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Sample number</p></th>
<th class="head text-center"><p>Height mean</p></th>
<th class="head text-center"><p>Height SD</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p>1</p></td>
<td class="text-center"><p>65.65</p></td>
<td class="text-center"><p>3.13</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>2</p></td>
<td class="text-center"><p>67.04</p></td>
<td class="text-center"><p>3.73</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>3</p></td>
<td class="text-center"><p>65.94</p></td>
<td class="text-center"><p>3.70</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>4</p></td>
<td class="text-center"><p>64.37</p></td>
<td class="text-center"><p>4.04</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>5</p></td>
<td class="text-center"><p>66.04</p></td>
<td class="text-center"><p>3.26</p></td>
</tr>
</tbody>
</table>
<p>The sample mean and standard deviation are similar but not exactly equal to the population values. When statistics vary across samples, this is called <strong>sampling error</strong>. There’s <em>variation</em> in the many sample estimates around the true population parameter. Where else have we seen variation around a central tendency before?</p>
<p>Now let’s take a large number of samples (say, 5,000) of 50 individuals and compute the mean for each of them. The result is a set of 5,000 sample statistics - kinda sounds like a dataset, doesn’t it?</p>
<p>We’ve spent a long time working with distributions of data around the mean of those data. We measured that central tendency because it is typically the best single number to use to characterize the value of data in a distribution.</p>
<p>We can extend that logic to understanding how the estimates we get across several samples relates to the true population parameter. We can make a distribution of sample <em>statistics</em>, rather than only raw observations. In this case, each statistic (e.g. the mean of each separate sample) is a unique entry in the distribution. This is a <strong>sampling distribution</strong> - a distribution of sampling statistics. Specifically, this is a sampling distribution of the mean (since that’s the statistic we’re calculating).</p>
<img src="images/ch16-sampdist.png"  width="750">
<p>The gray histogram above shows each raw observation for height in the dataset, and the blue histogram shows the mean values for each 50-person sample. The sample means vary somewhat, but something to notice is that overall they are centered around the population mean. The average of the 5,000 sample means is very close to the true population mean.</p>
<p>Technically you can make a sampling distribution for any statistic you can think of - the mean, a model coefficient <span class="math notranslate nohighlight">\(b_1\)</span>, an effect size like PRE, etc. It would be a distribution of those values calculated on different samples.</p>
<p>This is how we go from being confident about predictions in our samples, to being confident about how well those predictions <em>generalize</em> to other samples and the population. We build a model within a sample, and then ask ourselves: given an assumption about the population parameter, what other sorts of estimates from other sorts of samples would we be likely to find? Is our estimate in that expected sampling distribution? And across all of those hypothetical samples, would our model generally be good?</p>
<p>The process of asking these questions is called <strong>hypothesis testing</strong>, and we will dedicate the rest of the course to doing it. Hypothesis testing is when you make a hypothesis about what the population truly looks like, and then decide how likely that image of the world matches up with the dataset you’ve collected. It is an exercise in hypotheticals, reasoning about the range of conclusions you could make based on a variety of data samples you could collect.</p>
<p>To prepare for asking these questions, let’s learn more about the nature of sampling distributions and how they relate to the population parameter.</p>
</section>
<section id="the-importance-of-sample-size">
<h2>15.3 The importance of sample size<a class="headerlink" href="#the-importance-of-sample-size" title="Permalink to this heading">#</a></h2>
<p>You probably have some intuition that statistical inferences are better the bigger your sample size. In the popular conscious, it seems like “sample size” ranks right under “correlation is not causation” in terms of critiques about psychology research. Sample size is indeed an important concern, and in this section you’ll learn why.</p>
<p>In the example above, each of our samples was 50 people. While we typically don’t have the time or funding to conduct 50 iterations of a research experiment, we can actually use simulation to tell us what it might look like if we did.</p>
<p>One option is to use the random sampling functions we’ve learned about before, that draw samples from known distributions - <code class="docutils literal notranslate"><span class="pre">rnorm()</span></code>, <code class="docutils literal notranslate"><span class="pre">runif()</span></code>, etc.</p>
<p>What if we don’t have a good idea about what the population looks like though? What if we only have our own subsample of data?</p>
<p>We can actually still do a useful simulation here. We can take our dataset (like <code class="docutils literal notranslate"><span class="pre">studentdata</span></code>), and just pretend that <em>that</em> is the whole population of data. Then we can repeatedly draw random samples from it, with replacement. This technique is called <strong>bootstrapping</strong>, and helps us make inferences about sampling distributions even when we don’t know the population parameter. Essentially, we treat our data distribution as a probability distribution and draw many new samples from that.</p>
<p>Remember when we learned to use the <code class="docutils literal notranslate"><span class="pre">sample()</span></code> function to collect samples from a probability distribution? First we defined a sample space, e.g the six side of a die <code class="docutils literal notranslate"><span class="pre">c(1,2,3,4,5,6)</span></code>. Then we said how many items we want to draw from that sample space, and whether or not we want to draw with replacement (i.e. allow for us to draw the same item multiple times).</p>
<p>In bootstrapping, we do the same thing but with a data distribution as our sample space:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">#drawing a sample of 50 heights from the studentdata dataset</span>
<span class="c1">#if we leave out the probs argument, probability of each item will correspond to </span>
<span class="c1">#its frequency in the dataset</span>
<span class="n">bootstrap_sample</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sample</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">studentdata</span><span class="o">$</span><span class="n">Height</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">50</span><span class="p">,</span><span class="w"> </span><span class="n">replace</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span>
<span class="n">bootstrap_sample</span>
</pre></div>
</div>
</div>
</div>
<p>If we calculate the mean of this sample, we come up with one statistical estimate:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">mean</span><span class="p">(</span><span class="n">bootstrap_sample</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Compare that to the true “population” mean (mean of the entire <code class="docutils literal notranslate"><span class="pre">Height</span></code> variable):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">mean</span><span class="p">(</span><span class="n">studentdata</span><span class="o">$</span><span class="n">Height</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Our sample estimate isn’t exactly the same, but it’s pretty close.</p>
<p>Now let’s scale this up and simulate many samples of heights - say, 1,000 samples. Complete the code below to make this simulation run without errors.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">#creating an empty vector of 1000 spots</span>
<span class="n">bootstrap_means50</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">vector</span><span class="p">(</span><span class="n">length</span><span class="o">=</span><span class="m">1000</span><span class="p">)</span>

<span class="c1">#generate 1000 unique samples, saving each mean</span>
<span class="nf">for </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="m">1000</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">#draw a sample of 50 heights from studentdata$Height, replace = TRUE</span>
<span class="w">    </span><span class="n">bootstrap_sample</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="c1">#YOUR CODE HERE</span>
<span class="w">    </span><span class="n">m</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">mean</span><span class="p">(</span><span class="n">bootstrap_sample</span><span class="p">)</span>
<span class="w">    </span><span class="n">bootstrap_means50</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">m</span>
<span class="p">}</span>

<span class="n">bootstrap_means50</span><span class="p">[</span><span class="m">1</span><span class="o">:</span><span class="m">10</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>If we turn this set of means into a dataframe, we can plot a histogram and see how these means vary. We’ll also add a vertical line at the population mean, to see how these sample means compare to it.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">bootstrap_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">data.frame</span><span class="p">(</span><span class="n">bootstrap_means50</span><span class="p">)</span>
<span class="nf">gf_histogram</span><span class="p">(</span><span class="o">~</span><span class="w"> </span><span class="n">bootstrap_means50</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="o">=</span><span class="n">bootstrap_df</span><span class="p">,</span><span class="w"> </span><span class="n">fill</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;red&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="nf">gf_refine</span><span class="p">(</span><span class="nf">coord_cartesian</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">58</span><span class="p">,</span><span class="m">77</span><span class="p">)))</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="nf">gf_vline</span><span class="p">(</span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">xintercept</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">mean</span><span class="p">(</span><span class="n">studentdata</span><span class="o">$</span><span class="n">Height</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>Compare the spread of this sampling distribution to the distribution of raw values in <code class="docutils literal notranslate"><span class="pre">studentdata$Height</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">gf_histogram</span><span class="p">(</span><span class="o">~</span><span class="w"> </span><span class="n">Height</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="o">=</span><span class="n">studentdata</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">    </span><span class="nf">gf_refine</span><span class="p">(</span><span class="nf">coord_cartesian</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">58</span><span class="p">,</span><span class="m">77</span><span class="p">)))</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="nf">gf_vline</span><span class="p">(</span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">xintercept</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">mean</span><span class="p">(</span><span class="n">studentdata</span><span class="o">$</span><span class="n">Height</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>There’s variability among the sample means such that nearly all are different from the “population” mean, but any one sample mean gets pretty close to the population parameter. Further, if we take the mean of <em>means</em> and compare it to the population parameter:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">mean</span><span class="p">(</span><span class="n">bootstrap_means50</span><span class="p">)</span>
<span class="nf">mean</span><span class="p">(</span><span class="n">studentdata</span><span class="o">$</span><span class="n">Height</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>They are nearly identical.</p>
<p>Now what if we collected just as many samples, but small ones? Edit this code to do the same sampling procedure as above, but with <code class="docutils literal notranslate"><span class="pre">size</span> <span class="pre">=</span> <span class="pre">5</span></code> for each sample.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">#creating an empty vector of 1000 spots</span>
<span class="n">bootstrap_means5</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">vector</span><span class="p">(</span><span class="n">length</span><span class="o">=</span><span class="m">1000</span><span class="p">)</span>

<span class="c1">#generate 1000 unique samples, saving each mean</span>
<span class="nf">for </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="m">1000</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">#draw a sample of 5 heights from studentdata$Height, replace = TRUE</span>
<span class="w">    </span><span class="n">bootstrap_sample</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="c1">#YOUR CODE HERE</span>
<span class="w">    </span><span class="n">m</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">mean</span><span class="p">(</span><span class="n">bootstrap_sample</span><span class="p">)</span>
<span class="w">    </span><span class="n">bootstrap_means5</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">m</span>
<span class="p">}</span>

<span class="n">bootstrap_df</span><span class="o">$</span><span class="n">bootstrap_means5</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">bootstrap_means5</span>

<span class="c1">#plotting distribution of n=50 samples</span>
<span class="nf">gf_histogram</span><span class="p">(</span><span class="o">~</span><span class="w"> </span><span class="n">bootstrap_means50</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="o">=</span><span class="n">bootstrap_df</span><span class="p">,</span><span class="w"> </span><span class="n">fill</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;red&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="nf">gf_refine</span><span class="p">(</span><span class="nf">coord_cartesian</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">58</span><span class="p">,</span><span class="m">77</span><span class="p">)))</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="nf">gf_vline</span><span class="p">(</span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">xintercept</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">mean</span><span class="p">(</span><span class="n">studentdata</span><span class="o">$</span><span class="n">Height</span><span class="p">))</span>

<span class="c1">#plotting distribution of n=5 samples</span>
<span class="nf">gf_histogram</span><span class="p">(</span><span class="o">~</span><span class="w"> </span><span class="n">bootstrap_means5</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="o">=</span><span class="n">bootstrap_df</span><span class="p">,</span><span class="w"> </span><span class="n">fill</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;blue&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="nf">gf_refine</span><span class="p">(</span><span class="nf">coord_cartesian</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">58</span><span class="p">,</span><span class="m">77</span><span class="p">)))</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="nf">gf_vline</span><span class="p">(</span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">xintercept</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">mean</span><span class="p">(</span><span class="n">studentdata</span><span class="o">$</span><span class="n">Height</span><span class="p">))</span>

<span class="c1">#distribution of population data</span>
<span class="nf">gf_histogram</span><span class="p">(</span><span class="o">~</span><span class="w"> </span><span class="n">Height</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="o">=</span><span class="n">studentdata</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="nf">gf_refine</span><span class="p">(</span><span class="nf">coord_cartesian</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">58</span><span class="p">,</span><span class="m">77</span><span class="p">)))</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="nf">gf_vline</span><span class="p">(</span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">xintercept</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">mean</span><span class="p">(</span><span class="n">studentdata</span><span class="o">$</span><span class="n">Height</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>This sampling distribution is wider than the first - on average, each sample mean is farther away from the true population parameter. However, the mean of <em>means</em> is still quite close:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">mean</span><span class="p">(</span><span class="n">bootstrap_means5</span><span class="p">)</span>
<span class="nf">mean</span><span class="p">(</span><span class="n">studentdata</span><span class="o">$</span><span class="n">Height</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Finally, let’s take this exercise to the extreme with a sample size that is as small as possible. Modify the code to draw 1000 samples of only 1 data point each.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">#creating an empty vector of 1000 spots</span>
<span class="n">bootstrap_means1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">vector</span><span class="p">(</span><span class="n">length</span><span class="o">=</span><span class="m">1000</span><span class="p">)</span>

<span class="c1">#generate 1000 unique samples, saving each mean</span>
<span class="nf">for </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="m">1000</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">#draw a sample of 1 height value from fingers$Height, replace = TRUE</span>
<span class="w">    </span><span class="n">bootstrap_sample</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="c1">#YOUR CODE HERE</span>
<span class="w">    </span><span class="n">m</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">mean</span><span class="p">(</span><span class="n">bootstrap_sample</span><span class="p">)</span>
<span class="w">    </span><span class="n">bootstrap_means1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">m</span>
<span class="p">}</span>

<span class="n">bootstrap_df</span><span class="o">$</span><span class="n">bootstrap_means1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">bootstrap_means1</span>

<span class="c1">#plotting distribution of n=50 samples</span>
<span class="nf">gf_histogram</span><span class="p">(</span><span class="o">~</span><span class="w"> </span><span class="n">bootstrap_means50</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="o">=</span><span class="n">bootstrap_df</span><span class="p">,</span><span class="w"> </span><span class="n">fill</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;red&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="nf">gf_refine</span><span class="p">(</span><span class="nf">coord_cartesian</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">58</span><span class="p">,</span><span class="m">77</span><span class="p">)))</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="nf">gf_vline</span><span class="p">(</span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">xintercept</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">mean</span><span class="p">(</span><span class="n">studentdata</span><span class="o">$</span><span class="n">Height</span><span class="p">))</span>

<span class="c1">#plotting distribution of n=5 samples</span>
<span class="nf">gf_histogram</span><span class="p">(</span><span class="o">~</span><span class="w"> </span><span class="n">bootstrap_means5</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="o">=</span><span class="n">bootstrap_df</span><span class="p">,</span><span class="w"> </span><span class="n">fill</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;blue&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="nf">gf_refine</span><span class="p">(</span><span class="nf">coord_cartesian</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">58</span><span class="p">,</span><span class="m">77</span><span class="p">)))</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="nf">gf_vline</span><span class="p">(</span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">xintercept</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">mean</span><span class="p">(</span><span class="n">studentdata</span><span class="o">$</span><span class="n">Height</span><span class="p">))</span>

<span class="c1">#plotting distribution of n=1 samples</span>
<span class="nf">gf_histogram</span><span class="p">(</span><span class="o">~</span><span class="w"> </span><span class="n">bootstrap_means1</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="o">=</span><span class="n">bootstrap_df</span><span class="p">,</span><span class="w"> </span><span class="n">fill</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;orange&quot;</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="nf">gf_refine</span><span class="p">(</span><span class="nf">coord_cartesian</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">58</span><span class="p">,</span><span class="m">77</span><span class="p">)))</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="nf">gf_vline</span><span class="p">(</span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">xintercept</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">mean</span><span class="p">(</span><span class="n">studentdata</span><span class="o">$</span><span class="n">Height</span><span class="p">))</span>

<span class="c1">#distribution of population data</span>
<span class="nf">gf_histogram</span><span class="p">(</span><span class="o">~</span><span class="w"> </span><span class="n">Height</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="o">=</span><span class="n">studentdata</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="nf">gf_refine</span><span class="p">(</span><span class="nf">coord_cartesian</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">58</span><span class="p">,</span><span class="m">77</span><span class="p">)))</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="nf">gf_vline</span><span class="p">(</span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">xintercept</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">mean</span><span class="p">(</span><span class="n">studentdata</span><span class="o">$</span><span class="n">Height</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>This reveals a sampling distribution that approaches the same shape as the population distribution, since each “sample” is just one data point from the population. And yet, when comparing the mean of this wide sampling distribution to the population mean:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">mean</span><span class="p">(</span><span class="n">bootstrap_means1</span><span class="p">)</span>
<span class="nf">mean</span><span class="p">(</span><span class="n">studentdata</span><span class="o">$</span><span class="n">Height</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Based on this demonstration, we can see that if you only have a few observations in a sample, any one sample mean is unlikely to be accurate: if you replicate a small experiment and recalculate the mean you’ll get a very different answer. The sampling distribution is quite wide. This is why statistical answers from studies with a small sample can be misleading.</p>
<p>In contrast, if you run a large experiment and replicate it with another large sample, you’ll probably get nearly the same answer you got last time. The sampling distribution will be very narrow. If we took this to the extreme other side and drew samples that were infinitely large, we’d eventually create a sampling distribution that was just a single line at the population mean. This would be because we’d be taking samples over and over that <em>are</em> the population.</p>
<p>Since we don’t have this full population though, only smaller samples, we can quantify the variation in the sampling distribution by calculating the standard deviation of the sampling distribution. This is referred to as the <strong>standard error.</strong> We were exploring the mean as a statistic above, but you can calculate the standard error of any statistic. The standard error of a statistic is often denoted SE. Think of it as how far a sample estimate typically is from the true population parameter.</p>
<p>Sample size matters for collecting data samples because as the sample size gets larger, the standard error of the sampling distribution gets smaller. Our sample estimates are on average closer to the true population parameter.</p>
</section>
<section id="the-central-limit-theorem">
<h2>15.4 The Central Limit Theorem<a class="headerlink" href="#the-central-limit-theorem" title="Permalink to this heading">#</a></h2>
<p>Despite how poorly a single small sample can do on telling you about a population parameter, it’s important to remember that several samples together, even if very small, will stack up into a distribution that is centered on the true population parameter. That’s why, no matter how small our samples were in the previous demonstration, the mean of means was always be very close to the population mean.</p>
<p>On the basis of what we’ve seen so far, it seems like we have evidence for the following claims about the
sampling distribution of the mean:</p>
<ul class="simple">
<li><p>The mean of the sampling distribution is the same as the mean of the population</p></li>
<li><p>The standard deviation of the sampling distribution (i.e., the standard error) gets smaller as sample size increases</p></li>
</ul>
<p>As it happens, not only are these statements true, there is a very famous theorem in statistics that proves them, known as the <strong>Central Limit Theorem.</strong> We won’t spend time here on the <a class="reference external" href="https://en.wikipedia.org/wiki/Central_limit_theorem#Proof_of_classical_CLT">mathematical proof</a> that establishes this - our simulations above are demonstration enough for our purposes. But from this proof we can get specific equations for calculating the standard error.</p>
<p>When the mean is our sample estimate of interest, the Central Limit Theorem tells us, for a population with mean <span class="math notranslate nohighlight">\(\mu\)</span> and standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span>, that the sampling distribution of the mean also has mean <span class="math notranslate nohighlight">\(\mu\)</span>, and the standard error of the mean (SEM) is</p>
<div class="math notranslate nohighlight">
\[SEM = \frac{σ}{\sqrt{N}}\]</div>
<p>where N is the size of a sample. This says that, when sampling from a population with standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span>, the means of all samples of size N will vary around the true mean by the SEM amount on average. Further, because we divide the population standard devation <span class="math notranslate nohighlight">\(\sigma\)</span> by the square root of the sample size N, the SEM gets smaller as the sample size increases.</p>
<p>This result is useful for all sorts of things. It tells us why large experiments are more reliable than small ones, and because it gives us an explicit formula for the standard error it tells us how much <em>more</em> reliable a large experiment is than a small one.</p>
</section>
<section id="estimating-population-parameters">
<h2>15.5 Estimating population parameters<a class="headerlink" href="#estimating-population-parameters" title="Permalink to this heading">#</a></h2>
<p>In the simulations in the previous sections, we knew the population parameter ahead of time. This is helpful for learning about statistics, but of course the most interesting things to do research on are the things we don’t already know about; that which we don’t know the population distribution for. So if we want to calculate standard error (how far away our estimate of a mean is likely to be from the population mean) with an equation like <span class="math notranslate nohighlight">\(\frac{σ}{\sqrt{N}}\)</span>, how do we do that when we don’t know <span class="math notranslate nohighlight">\(\mu\)</span> or <span class="math notranslate nohighlight">\(\sigma\)</span>?</p>
<p>For instance, suppose you wanted to measure the IQ of Claremont College students. IQ in the general population is known - it has a mean of 100 and a standard deviation of 15. But maybe Claremont College students are a different sort of population, with a different mean and SD. So to find out the IQ of the kind of students who come to the 5Cs, we’re going to have to estimate the population parameters from a sample of data. So how do we do this?</p>
<section id="estimating-the-population-mean">
<h3>Estimating the population mean<a class="headerlink" href="#estimating-the-population-mean" title="Permalink to this heading">#</a></h3>
<p>Suppose we camp outside Frary and ask 100 students to take an IQ test for us. The average IQ score among these people turns out to be <span class="math notranslate nohighlight">\(\bar{X}=102.5\)</span>. So what is the true mean IQ for the entire population of Claremont College students? Obviously, we don’t know the answer to that question. It could be 97.2, or 108. We only have one sample, so we cannot give a definitive answer. Nevertheless, right now our “best guess” is 102.5. That’s the essence of statistical
estimation: giving a best guess.</p>
<p>In this example, estimating the unknown poulation parameter for the mean is straightforward. We calculate the sample mean, and we use that as an estimate of the population mean. We calculate <span class="math notranslate nohighlight">\(\bar{X}\)</span>, and treat that number as our best guess about the value of <span class="math notranslate nohighlight">\(\mu\)</span>. Much like we gave hats to symbols in the modeling chapters to designate the difference between predicted values and true values, we can do so here to designate what our predictions are of the population parameter, based on a sample of data.</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Symbol</p></th>
<th class="head text-center"><p>What is it?</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p><span class="math notranslate nohighlight">\(\bar{X}\)</span></p></td>
<td class="text-center"><p>Sample mean</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><span class="math notranslate nohighlight">\(\mu\)</span></p></td>
<td class="text-center"><p>True population mean</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p><span class="math notranslate nohighlight">\(\hat{\mu}\)</span></p></td>
<td class="text-center"><p>Prediction of the population mean</p></td>
</tr>
</tbody>
</table>
<p>When we have one data sample, <span class="math notranslate nohighlight">\(\bar{X}\)</span> is thus used as <span class="math notranslate nohighlight">\(\hat{\mu}\)</span> but we keep in mind that this is likely to vary on average from the true <span class="math notranslate nohighlight">\(\mu\)</span> by the standard error.</p>
</section>
<section id="estimating-the-population-standard-deviation">
<h3>Estimating the population standard deviation<a class="headerlink" href="#estimating-the-population-standard-deviation" title="Permalink to this heading">#</a></h3>
<p>So far, estimation seems pretty simple - just take a sample estimate and treat it as the population parameter. So you might be wondering why we forced you to read through all that stuff about sampling theory before getting here. In the case of the mean, our sample statistic (i.e. <span class="math notranslate nohighlight">\(\bar{X}\)</span>) turned out to be the best approximation of the true population parameter <span class="math notranslate nohighlight">\(\mu\)</span> that we could come up with given our data. However, that’s not always true. To see this, let’s think about how to construct an estimate of the population standard deviation, which we’ll denote <span class="math notranslate nohighlight">\(\hat{\sigma}\)</span>. What shall we use as our estimate in this case?</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Symbol</p></th>
<th class="head text-center"><p>What is it?</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p><span class="math notranslate nohighlight">\(s\)</span></p></td>
<td class="text-center"><p>Sample sd</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><span class="math notranslate nohighlight">\(\sigma\)</span></p></td>
<td class="text-center"><p>True population sd</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p><span class="math notranslate nohighlight">\(\hat{\sigma}\)</span></p></td>
<td class="text-center"><p>Estimate of the population sd</p></td>
</tr>
</tbody>
</table>
<p>Your first thought might be that we could do the same thing we did when estimating the mean, and just use the sample
statistic as our estimate. That’s almost the right thing to do, but not quite.</p>
<p>Here’s why. Suppose we have a sample that contains a single IQ observation of 98. It has a sample mean of 98, and because every observation in this sample is equal to the sample mean (obviously!) it has a sample standard deviation of 0: the sample contains a single observation and therefore there is no variation observed within the sample.</p>
<p>But as an estimate of the population standard deviation, this may feel completely wrong. Knowing that data implies variability, the only reason that we don’t see any variability in the sample is that the sample is too small to display any variation, not because everyone has an IQ of 98. So, if you have a sample size of N=1, it feels like the right answer about the population standard deviation is just to say “no idea at all”.</p>
<p>Suppose we now make a second observation. The dataset now has N=2 observations, and the complete sample now contains the observations 98 and 100. This time around, our sample is just large enough for us to be able to observe some variability: two observations is the bare minimum number needed for any variability to be observed. For our new dataset, the sample mean is <span class="math notranslate nohighlight">\(\bar{X} = 99\)</span>, and the sample standard deviation is <span class="math notranslate nohighlight">\(s = 1\)</span>.</p>
<p>Now that we have variation in our sample, is <em>this</em> estimate of the standard deviation going to be more reliable? Or will it also be too small, like 0 was too small?</p>
<p>We can use R to simulate the results of many samples to demonstrate this. Given the true population mean of IQ is 100 and the standard deviation is 15, we can use the <code class="docutils literal notranslate"><span class="pre">rnorm()</span></code> function to generate the results of an experiment in which we measure N=2 IQ scores, and calculate the sample standard deviation. If we do this over and over again, and plot a histogram of these sample standard deviations, we get the sampling distribution of the standard deviation (see figure below).</p>
<img src="images/ch16-sdestimate.png"  width="350">
<p>Even though the true population standard deviation is 15, the average of the sample standard deviations is only 8.5. Notice that this is a very different result to what we found when we plotted the sampling distribution of the mean of <code class="docutils literal notranslate"><span class="pre">Height</span></code> in <code class="docutils literal notranslate"><span class="pre">studentdata</span></code>. In that sampling distribution, the population mean was 65.95, and the mean of the sample means was always close to that.</p>
<p>Now let’s extend the simulation. Instead of restricting ourselves to the situation where we have a
sample size of N=2, let’s repeat the exercise again for sample sizes from 1 to 10. If we plot the average sample mean and average sample standard deviation as a function of sample size, you get the results shown in this next figure.</p>
<img src="images/ch16-estimatesim.png"  width="650">
<p>On the left hand side (panel A) is the average sample mean for each sample size, and on the right hand side (panel B) is the average standard deviation for each sample size. The two plots are quite different: no matter the sample size, the average of the sampling distribution of the mean is equal to the population mean. This means it is an <strong>unbiased estimator</strong>. This is the reason why your best estimate for the population mean is the sample mean.</p>
<p>The plot on the right shows that the average of the sampling distribution of the standard deviation is always smaller than the population standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span> for small sample sizes. No matter how many samples you collect, the central tendency of that sampling distribution will be systematically wrong. This means it is a <strong>biased estimator.</strong> In other words, if we want to make a “best guess” <span class="math notranslate nohighlight">\(\hat{\sigma}\)</span> about the value of the population standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span>, we should make sure our guess is a little bit larger than the sample standard deviation <span class="math notranslate nohighlight">\(s\)</span>.</p>
<p>The fix to this systematic bias, we rely on degrees of freedom! If you recall from Chapter 5, the sample variance is defined to be the average of the squared deviations from the sample mean, with one change. That is:</p>
<div class="math notranslate nohighlight">
\[s^2 = \frac{1}{N-1}\sum_{i=1}^{N}(X_i-\bar{X})^2\]</div>
<p>where the sum of the squared deviations are divided by N-1 instead of N, like in a normal average. As it turns out, this change is all we need to do to make variance an unbiased estimator. This is also true in the equation for standard deviation:</p>
<div class="math notranslate nohighlight">
\[s = \sqrt{\frac{1}{N-1}\sum_{i=1}^{N}(X_i-\bar{X})^2}\]</div>
<p>When calculating something like mean squared error in a statistical model, we generalize this to:</p>
<div class="math notranslate nohighlight">
\[MSE = \sqrt{\frac{1}{N-k}\sum_{i=1}^{N}(Y_i-\hat{Y_i})^2}\]</div>
<p>Where k is the number of parameters in the model. This is why we use degrees of freedom when making statistical estimates: dividing by a slightly smaller number helps us fix how much estimates of standard error undershoot the actual standard error.</p>
<p>When we want to know the standard error of the mean (how far off one sample mean of size N is likely to be from the true population mean), we need to use the population parameter <span class="math notranslate nohighlight">\(\sigma\)</span> in the calculation. But we don’t know it for sure, so our next best option is to use an <em>estimate</em> of the population standard deviation, calculated from our sample standard deviation and degrees of freedom:</p>
<div class="math notranslate nohighlight">
\[SEM = \frac{\hat{σ}}{\sqrt{N}}\]</div>
</section>
</section>
<section id="sampling-distributions-of-model-outputs">
<h2>15.6 Sampling distributions of model outputs<a class="headerlink" href="#sampling-distributions-of-model-outputs" title="Permalink to this heading">#</a></h2>
<p>The mean and standard deviation aren’t the only sampling distributions we use. All sample statistics have a counterpart parameter that describes the true population, and thus a distribution of estimates can be built from many samples.</p>
<p>Another sampling distribution that will be relevant to us is the distribution of coefficients from a statistical model. Let’s fit a simple linear regression predicting thumb length from height in the <code class="docutils literal notranslate"><span class="pre">studentdata</span></code> dataset and look at the estimate of <span class="math notranslate nohighlight">\(b_1\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">#fit a linear model of Thumb predicted by Height in the studentdata dataset</span>
<span class="n">height_model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="c1">#YOUR CODE HERE)</span>
<span class="n">height_model</span>
<span class="nf">gf_point</span><span class="p">(</span><span class="n">Thumb</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">Height</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="o">=</span><span class="n">studentdata</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="nf">gf_lm</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>The estimate for <span class="math notranslate nohighlight">\(b_1\)</span> is 0.87, indicating that a regression line best fits <em>these data</em> with the least error if it has a slope of 0.87. For every one-inch increase in someone’s height, we’d predict that their thumb length would be 0.87mm longer. But now let’s treat <code class="docutils literal notranslate"><span class="pre">studentdata$Height</span></code> as a population again, and bootstrap some separate samples in which we fit a model for each. We’ll use the function <code class="docutils literal notranslate"><span class="pre">sample_n()</span></code> from the <code class="docutils literal notranslate"><span class="pre">dplyr</span></code> package in order to sample entire rows of a data frame.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">#creating an empty vector of 1000 spots</span>
<span class="n">bootstrap_b1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">vector</span><span class="p">(</span><span class="n">length</span><span class="o">=</span><span class="m">1000</span><span class="p">)</span>

<span class="c1">#generate 1000 unique samples, saving each mean</span>
<span class="nf">for </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="m">1000</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">#draw a sample of 50 height value from studentdata$Height, replace = TRUE</span>
<span class="w">    </span><span class="n">bootstrap_sample</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sample_n</span><span class="p">(</span><span class="n">studentdata</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="o">=</span><span class="m">50</span><span class="p">,</span><span class="w"> </span><span class="n">replace</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span>
<span class="w">    </span><span class="n">bootstrap_model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">lm</span><span class="p">(</span><span class="n">Thumb</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">Height</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="o">=</span><span class="n">bootstrap_sample</span><span class="p">)</span>
<span class="w">    </span><span class="n">bootstrap_b1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">bootstrap_model</span><span class="o">$</span><span class="n">coefficients</span><span class="p">[[</span><span class="m">2</span><span class="p">]]</span>
<span class="p">}</span>

<span class="n">bootstrap_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">data.frame</span><span class="p">(</span><span class="n">bootstrap_b1</span><span class="p">)</span>

<span class="c1">#plotting distribution of n=50 samples</span>
<span class="nf">gf_histogram</span><span class="p">(</span><span class="o">~</span><span class="w"> </span><span class="n">bootstrap_b1</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="o">=</span><span class="n">bootstrap_df</span><span class="p">,</span><span class="w"> </span><span class="n">fill</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;red&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>From this, we can clearly see that models built on different data will come up with different coefficient estimates. Some will say that <code class="docutils literal notranslate"><span class="pre">Thumb</span></code> and <code class="docutils literal notranslate"><span class="pre">Height</span></code> will relate to each other in much the same way as they do in the “population”:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">set.seed</span><span class="p">(</span><span class="m">23</span><span class="p">)</span>
<span class="n">bootstrap_sample</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sample_n</span><span class="p">(</span><span class="n">studentdata</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="o">=</span><span class="m">50</span><span class="p">,</span><span class="w"> </span><span class="n">replace</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span>
<span class="nf">lm</span><span class="p">(</span><span class="n">Thumb</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">Height</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="o">=</span><span class="n">bootstrap_sample</span><span class="p">)</span>
<span class="nf">gf_point</span><span class="p">(</span><span class="n">Thumb</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">Height</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="o">=</span><span class="n">bootstrap_sample</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="nf">gf_lm</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>But others will over-estimate the model slope:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">set.seed</span><span class="p">(</span><span class="m">735</span><span class="p">)</span>
<span class="n">bootstrap_sample</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sample_n</span><span class="p">(</span><span class="n">studentdata</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="o">=</span><span class="m">50</span><span class="p">,</span><span class="w"> </span><span class="n">replace</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span>
<span class="nf">lm</span><span class="p">(</span><span class="n">Thumb</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">Height</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="o">=</span><span class="n">bootstrap_sample</span><span class="p">)</span>
<span class="nf">gf_point</span><span class="p">(</span><span class="n">Thumb</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">Height</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="o">=</span><span class="n">bootstrap_sample</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="nf">gf_lm</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>While still others will even flip the direction of the effect, such that we predict someone’s thumb would be <em>shorter</em> for each one-inch increase in height:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">set.seed</span><span class="p">(</span><span class="m">352</span><span class="p">)</span>
<span class="n">bootstrap_sample</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sample_n</span><span class="p">(</span><span class="n">studentdata</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="o">=</span><span class="m">50</span><span class="p">,</span><span class="w"> </span><span class="n">replace</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span>
<span class="nf">lm</span><span class="p">(</span><span class="n">Thumb</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">Height</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="o">=</span><span class="n">bootstrap_sample</span><span class="p">)</span>
<span class="nf">gf_point</span><span class="p">(</span><span class="n">Thumb</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">Height</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="o">=</span><span class="n">bootstrap_sample</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="nf">gf_lm</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>This tells us that the estimate of the coefficient from one dataset may not be the true population coefficient, and is likely to vary around the population parameter by the amount of the standard error. The equation for the standard error of a regression coefficient is a more complicated equation than those for the standard error of the mean so we won’t make you learn it, but it is again a function of sample size. You can actually find it directly in R with a new function that works on model objects, <code class="docutils literal notranslate"><span class="pre">summary()</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">summary</span><span class="p">(</span><span class="n">height_model</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>This gives you a lot more information about a model than just its coefficients. For our purposes right now, look at the table under the heading “Coefficients:”. Each row is for each parameter estimated by our model - the intercept <span class="math notranslate nohighlight">\(b_0\)</span> and the effect of <code class="docutils literal notranslate"><span class="pre">Height</span></code> <span class="math notranslate nohighlight">\(b_1\)</span>. The first column, “Estimate” shows you the same coefficient estimate as you would get from simply typing the model object name into the console window.</p>
<p>Some new information is in the next column over, called “Std. Error”. This stands for the standard error of the estimate, or how much this estimate is likely to vary on average from the population coefficient based on the sample size for these particular data.</p>
</section>
<section id="confidence-intervals">
<h2>15.7 Confidence intervals<a class="headerlink" href="#confidence-intervals" title="Permalink to this heading">#</a></h2>
<p>All of this is to say, our sample estimates are only that - estimates. They are likely to be more similar to the true population parameter the larger our samples are, but there’s still variation among estimates from different data samples. So how well can we ever trust one particular sample estimate from our one humble research study? How certain can we be that our estimate of the mean, model coefficient, etc. is close to the population estimate? It would be nice to say that we are reasonably confident that the true parameter is in some window around this estimate.</p>
<p>In fact, there is a way we can say exactly that and find an interval of values around our estimate that expresses a degree of confidence that the true parameter is somewhere in that interval.</p>
<p>Our understanding of sampling distributions now will help us construct this concept. Let’s start out with an estimate of the <span class="math notranslate nohighlight">\(b_1\)</span> coefficient in a general linear model. Suppose the true population coefficient <span class="math notranslate nohighlight">\(\beta_1\)</span> is 0. Due to sampling error, if we collect samples that are smaller than the population size, there is a range of <span class="math notranslate nohighlight">\(b_1\)</span> estimates that might happen even with the same population parameter. You can see this possible range of <span class="math notranslate nohighlight">\(b_1\)</span> estimates in the sampling distribution below. Each value in this distribution is one <span class="math notranslate nohighlight">\(b_1\)</span> value that would be estimated from a random subsample of the population.</p>
<p>Note that this sampling distribution is shaped normally. Some <span class="math notranslate nohighlight">\(b_1\)</span> estimates are more likely to occur than others. Thus you can think of the sampling distribution also like a probability distribution centered on the population parameter value. In this way, getting a sample whose <span class="math notranslate nohighlight">\(b_1\)</span> estimate is 1 (where the distribution has higher density) is more likely to occur. Values that are far away from 0, like 6 (in the tails of the distribution), are less likely to occur.</p>
<img src="images/ch15-betadist0.png" width="400">
<p>Now instead imagine that the true population coefficient β<sub>1</sub> is actually 10. Below is the visualization of this situation. In this case it is still possible to get <span class="math notranslate nohighlight">\(b_1\)</span> estimates of 1 or 6, but a 6 is more likely and a 1 is less likely.</p>
<img src="images/ch15-betadist10.png" width="450">
<p>The width of these sampling distributions is determined by the standard error of the estimate, which is related to sample size. In these sampling distributions, even though a <span class="math notranslate nohighlight">\(b_1\)</span> of 6 does not exactly match the population parameter, it is still not that unusual to draw a sample with that as the estimate.</p>
<p>Now, remember that we don’t actually know what the population parameter is. We only know the <span class="math notranslate nohighlight">\(b_1\)</span> estimate we’ve gotten for our particular sample. Thus if we calculate <span class="math notranslate nohighlight">\(b_1 = 6\)</span>, from our musings so far we can see that it <em>could have</em> been produced by <span class="math notranslate nohighlight">\(\beta_1 = 0\)</span> with reasonable likelihood. It also <em>could have</em> been produced by <span class="math notranslate nohighlight">\(\beta_1 = 10\)</span> with reasonable likelihood. And these are just two of the many possible parameters that could have produced the sample estimate of 6.</p>
<p>However, consider a population parameter like -10:</p>
<img src="images/ch15-betadist-10.png" width="450">
<p>Getting a sample estimate <span class="math notranslate nohighlight">\(b_1 = 6\)</span> out of this population parameter is very unlikely. It’s still technically possible because the range of the normal distribution stretches from <span class="math notranslate nohighlight">\(-\infty\)</span> to <span class="math notranslate nohighlight">\(\infty\)</span>, but it’s so unlikely that we would be very surprised to see that happen.</p>
<p>Thus, there is a range of <span class="math notranslate nohighlight">\(\beta_1\)</span> parameters that we think can produce a <span class="math notranslate nohighlight">\(b_1 = 6\)</span> estimate a reasonable amount of the time. There are also <span class="math notranslate nohighlight">\(\beta_1\)</span> values that we think are very unlikely to ever produce a <span class="math notranslate nohighlight">\(b_1 = 6\)</span> estimate.</p>
<p>This range of <span class="math notranslate nohighlight">\(\beta_1\)</span> parameters that might reasonably produce our sample’s <span class="math notranslate nohighlight">\(b_1\)</span> estimate is the <strong>confidence interval</strong> of the estimate. Confidence intervals allow us to quantify the expected variation in a sample estimate and make statements such as, “We are 95% confident that the true population parameter falls between these two values.” In order to make such a statement, we need a way to find a lower bound and an upper bound of <span class="math notranslate nohighlight">\(\beta_1\)</span> values that are likely to produce our <span class="math notranslate nohighlight">\(b_1\)</span> estaimate.</p>
<p>We know from our discussion of standard deviation in Chapter 5 that 95% of a distribution will fall within 2 standard deviations of the distribution’s mean (it’s actually 1.96 standard deviations, to be precise). Applied to a sampling distribution, we can find the range of <span class="math notranslate nohighlight">\(b_1\)</span> estimates that 95% of the time will be generated by the sampling distribution of <span class="math notranslate nohighlight">\(\beta_1\)</span>. To put that in mathematical terms:</p>
<div class="math notranslate nohighlight">
\[\beta_1 - (1.96 * SE) \leq b_1 \leq \beta_1 + (1.96 * SE)\]</div>
<p>where the SE is the standard error (aka the standard deviation of the sampling distribution). This equation tells us that, for 100 sample estimates <span class="math notranslate nohighlight">\(b_1\)</span>, 95 of them are expected to fall between <span class="math notranslate nohighlight">\(\beta_1 - (1.96 * SE)\)</span> and <span class="math notranslate nohighlight">\(\beta_1 + (1.96 * SE)\)</span>.</p>
<p>However, that’s not answering the question that we’re actually interested in. The equation above tells us what we should expect about <span class="math notranslate nohighlight">\(b_1\)</span> estimates, given that we know what the population parameter is. What we want is to have this work the other way around: we want to know what we should believe about the population parameter, given that we have observed a particular sample estimate.</p>
<p>So to answer this question instead, we do some algebra. We first find the lower bound <span class="math notranslate nohighlight">\(\beta_{low}\)</span> where a specific <span class="math notranslate nohighlight">\(b_1\)</span> is at most 1.96 SE’s above the population parameter.</p>
<div class="math notranslate nohighlight">
\[b_1 = \beta_{low} + 1.96*SE\]</div>
<p>Rearranging this to solve for <span class="math notranslate nohighlight">\(\beta_{low}\)</span>, we find:</p>
<div class="math notranslate nohighlight">
\[\beta_{low} = b_1 - 1.96*SE\]</div>
<p>This means that the lowest <span class="math notranslate nohighlight">\(\beta_1\)</span> that is still likely to produce our specific estimate of <span class="math notranslate nohighlight">\(b_1\)</span> is <span class="math notranslate nohighlight">\(b_1 - (1.96*SE)\)</span>.</p>
<p>The upper bound is the <span class="math notranslate nohighlight">\(\beta_1\)</span> where <span class="math notranslate nohighlight">\(b_1\)</span> is at most 1.96 SE’s <em>below</em> the population parameter.</p>
<div class="math notranslate nohighlight">
\[b_1 = \beta_{high} - 1.96*SE\]</div>
<p>And rearranged:</p>
<div class="math notranslate nohighlight">
\[\beta_{high} = b_1 + 1.96*SE\]</div>
<p>This means that the highest <span class="math notranslate nohighlight">\(\beta_1\)</span> that is still likely to produce our specific estimate of <span class="math notranslate nohighlight">\(b_1\)</span> is <span class="math notranslate nohighlight">\(b_1 + (1.96*SE)\)</span>.</p>
<p>Thus, if the true <span class="math notranslate nohighlight">\(\beta_1\)</span> is anything in this range, <span class="math notranslate nohighlight">\(b_1\)</span> is a reasonably likely outcome.</p>
<img src="images/ch15-cirange.png" width="600">
<p>Since the specific values in a confidence interval are calculated based on an estimate <span class="math notranslate nohighlight">\(b_1\)</span>, you should think of the specific range of a confidence interval as being an estimate too. If we were to collect 100 samples, we would get 100 unique versions of this interval, since we’d have 100 unique values of <span class="math notranslate nohighlight">\(b_1\)</span>. But 95 out of 100 of these confidence intervals (95% of them) would contain the true population parameter. This is why we say we are 95% confident that a particular interval we calculate for a particular sample contains the population parameter. We refer to this range as a <strong>95% confidence interval</strong>, denoted CI<sub>95</sub>. We can write this as our general formula for the 95% confidence interval:</p>
<div class="math notranslate nohighlight">
\[CI_{95} = b \pm (1.96*SE)\]</div>
<p>Where <span class="math notranslate nohighlight">\(b\)</span> is some statistic estimate. When reporting statistical results in text, we usually include the coefficient estimate followed by the 95% CI in brackets. I.e., “the estimate of the effect of Height is <span class="math notranslate nohighlight">\(b_1 = 0.962\)</span>, 95% CI [0.603, 1.321]”.</p>
<p>Note that there’s nothing special about the number 1.96 here. That’s just the number of standard deviations between which 95% of a distribution can be found. We’re using 95% to represent a reasonable probability of getting our sample estimate, but that’s just by convention (which we’ll discuss more next chapter). If we wanted a 70% confidence interval instead, where we’re 70% confident that the interval contains the population parameter, we could have used the <code class="docutils literal notranslate"><span class="pre">qnorm()</span></code> function to calculate the 15th and 85th quantiles:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">#2.5th and 97.5th quantiles of the normal distribution </span>
<span class="nf">qnorm</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">0.025</span><span class="p">,</span><span class="m">0.975</span><span class="p">))</span>

<span class="c1">#15th and 85th quantiles of the normal distribution</span>
<span class="nf">qnorm</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">0.15</span><span class="p">,</span><span class="m">0.85</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>So the formula for CI<sub>70</sub> would be the same as the formula for CI<sub>95</sub> except that we’d use 1.036 as our magic number rather than 1.96. This would be a narrower range of values than the CI<sub>95</sub>, because to increase our confidence that the population parameter is covered by our interval, we need to include more possibilities (a larger range) in that interval.</p>
<p>Additionally, because the SE is included in the equation, we can conclude that smaller sample sizes (and thus larger SE) lead to a wider confidence interval for the same level of confidence, and larger sample sizes lead to a narrower confidence interval. In other words, we can be 95% confident a population parameter is in a narrower range of values with larger sample sizes.</p>
<img src="images/ch16-cis.png" width="700">
<p><em>95% confidence intervals. The top (panel A) shows 50 simulated replications of an experiment in which we measure the IQs of 10 people. The dot marks the location of the sample mean, and the line shows the 95% confidence interval. In total 47 of the 50 confidence intervals do contain the true mean (i.e., 100), but the three intervals marked with asterisks do not. The lower graph (panel B) shows a similar simulation, but this time we simulate replications of an experiment that measures the IQs of 25 people. The sample means are generally closer to the population mean, so the CIs can be narrower in order for ~95% of them to contain the true mean and 5% of them to not.</em></p>
<p>To calculate the confidence interval of a model coefficient estimate, we can use the Std. Error value in the <code class="docutils literal notranslate"><span class="pre">summary()</span></code> output:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">summary</span><span class="p">(</span><span class="n">height_model</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We can use this to calculate the 95% confidence interval of each parameter. For <span class="math notranslate nohighlight">\(b_0\)</span>, it would be:</p>
<div class="math notranslate nohighlight">
\[b_0 - (1.96 * SE) \leq \beta_0 \leq b_0 + (1.96 * SE)\]</div>
<div class="math notranslate nohighlight">
\[2.8432 - (1.96 * 12.6549) \leq \beta_0 \leq 2.8432 + (1.96 * 12.6549)\]</div>
<div class="math notranslate nohighlight">
\[b_0 = 2.84 \ [-21.96, 27.65]\]</div>
<p>For <span class="math notranslate nohighlight">\(b_1\)</span> the 95% confidence interval is:</p>
<div class="math notranslate nohighlight">
\[b_1 - (1.96 * SE) \leq \beta_1 \leq b_1 + (1.96 * SE)\]</div>
<div class="math notranslate nohighlight">
\[0.8715 - (1.96 * 0.1914) \leq \beta_1 \leq 0.8715 + (1.96 * 0.1914)\]</div>
<div class="math notranslate nohighlight">
\[b_1 = 0.87 \ [0.50, 1.25]\]</div>
<p>However, in practice it’s actually better to use the function <code class="docutils literal notranslate"><span class="pre">confint()</span></code> directly for this rather than multiplying out 1.96*SE, since the SE shown in the <code class="docutils literal notranslate"><span class="pre">summary()</span></code> table is rounded and we don’t want to propogate rounding errors. To use <code class="docutils literal notranslate"><span class="pre">confint()</span></code>, first pass it a model object, the name of the coefficient you want to get the confidence interval for, and the level of confidence you want.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">#For the intercept:</span>
<span class="nf">confint</span><span class="p">(</span><span class="n">height_model</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;(Intercept)&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">level</span><span class="o">=</span><span class="m">0.95</span><span class="p">)</span>

<span class="c1">#For the Height coefficient: </span>
<span class="nf">confint</span><span class="p">(</span><span class="n">height_model</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;Height&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">level</span><span class="o">=</span><span class="m">0.95</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>One thing you should notice is that the confidence interval for <span class="math notranslate nohighlight">\(b_1\)</span> in our data spans only positive numbers and doesn’t intersect with 0. This means that, even if the true effect <span class="math notranslate nohighlight">\(\beta_1\)</span> is not the same as our estimate, we are quite confident that it is at least a positive number - that is, as height values go up, estimates of thumb length will also go up. In general, we are confident that there is a positive effect of Height. Hold onto this thought for next chapter.</p>
<section id="pitfalls-of-interpreting-confidence-intervals">
<h3>Pitfalls of interpreting confidence intervals<a class="headerlink" href="#pitfalls-of-interpreting-confidence-intervals" title="Permalink to this heading">#</a></h3>
<p>The hardest thing about confidence intervals is remembering specifically what they mean. There are two major errors people often make when interpreting them.</p>
<p>First, when you see something like 95% CI [0.606, 1.318], sometimes the first instinct is to treat it as <em>the</em> interval in which future sample estimate can be found. This is incorrect. We have to remember that the numbers in this specific interval are based on one sample estimate. If we had a different sample instead, we would compute a different confidence interval. Thus CIs are themselves estimates that vary sample to sample. You can imagine your first sample of data as being pretty unsual by chance (e.g., when measuring the height of students but somehow collecting data only from the basketball team). Other samples, which will probably be closer to the true mean of height, won’t often be in the CI calculated for this wonky sample. It is not that 95% of sample estimates will be in this interval, but the range of population parameters with a 95% chance of producing <em>this</em> estimate.</p>
<p>The second common error is to say that there is a 95% <em>probability</em> that the population parameter is in this interval. This is also incorrect. The correct statement is that we are 95% <em>confident</em> that the population parameter is in this interval. This is an incredibly pedantic distinction, but it matters because of the way probability is calculated. The first sentence implies that, given a particular value of b, a β parameter is 95% likely to be in this range. In other words, the probability of β, given b. In mathematics we’d write this as:</p>
<div class="math notranslate nohighlight">
\[P(\beta | b)\]</div>
<p>The “|” is the symbol for saying “given”. This is called a <strong>conditional probability</strong> - the probability of β is <em>conditional</em> on the state of b.</p>
<p>But the true meaning of a confidence interval is that there’s a 95% probability of getting this b estimate, given that the particular value of β is in this range. Written mathematically:</p>
<div class="math notranslate nohighlight">
\[P(b | \beta)\]</div>
<p>We will learn more about conditional probability in chapter 20. The thing to know right now is that these probabilities are different, and come out to different values. Thus, we should be careful about which we are talking about. The <em>wrong</em> statement is “there is a 95% probability that β is in this range”. The correct statement is “βs in this range have a 95% probability of producing our statistic estimate.” To get around this issue, we can instead be more general and just say “we are 95% <em>confident</em> that β is in this range.” This way you don’t have to deal with probabilities at all, and stay away from the trickiness of conditional probabilities.</p>
</section>
</section>
<section id="visualizing-uncertainty">
<h2>15.8 Visualizing uncertainty<a class="headerlink" href="#visualizing-uncertainty" title="Permalink to this heading">#</a></h2>
<p>As we explored extensively in chapter 7, visualizing data is an important step for communicating the results of all your analyses. For communicating model estimates, we’ve learned how to plot boxplots, simple regressions, and interactions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">#plotting group differences, showing data points</span>
<span class="nf">gf_boxplot</span><span class="p">(</span><span class="n">Thumb</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">Sex</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">studentdata</span><span class="p">,</span><span class="w"> </span><span class="n">color</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">Sex</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="nf">gf_jitter</span><span class="p">(</span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">width</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.3</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.5</span><span class="p">)</span>

<span class="c1">#plotting a regression, showing data points</span>
<span class="nf">gf_point</span><span class="p">(</span><span class="n">Thumb</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">Height</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">studentdata</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">    </span><span class="nf">gf_lm</span><span class="p">()</span>

<span class="c1">#plotting an interaction</span>
<span class="n">interactionmodel</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">lm</span><span class="p">(</span><span class="n">Thumb</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">Sex</span><span class="o">*</span><span class="n">Height</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">studentdata</span><span class="p">)</span>
<span class="nf">interact_plot</span><span class="p">(</span><span class="n">interactionmodel</span><span class="p">,</span><span class="w"> </span><span class="n">pred</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Height</span><span class="p">,</span><span class="w"> </span><span class="n">modx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Sex</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Each of these plots communicate model estimates we made based on a sample of data. In the boxplot, we can see the mean Thumb length per sex group. In the scatter plot, we see the intercept and slope of the best fitting regression line for explaining Thumb length based on Height. In the interaction plot, we see this Thumb ~ Height regression line for each category of sex. In each of these plots, we are visualizing the unstandardized effect size of the predictor in our model - either as the difference in boxplot locations, or the slope of the regression line.</p>
<p>However, we’ve also learned how our estimates might not be the true value of a population parameter. There’s some uncertainty around what that actual value is, and our estimate might not be correct. Thus, it’s important to include that uncertainty in our visualizations. In each of the plot types above, there is a way to add the confidence intervals of an estimate to the plot.</p>
<p>For the boxplot, there actually is already some uncertainty communicated via the size of the box. Recall that this is the interquartile range (IQR) of the data in each category. But since confidence intervals tell us about the likely population parameters that produced this data, not just about the variation of the data itself, we can make a version of this plot that shows the range of confidence intervals instead.</p>
<p>Using <code class="docutils literal notranslate"><span class="pre">ggformula</span></code>, we’re going to do this with a jitter plot divided up by group that adds on summary information using the <code class="docutils literal notranslate"><span class="pre">gf_summary()</span></code> function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">gf_jitter</span><span class="p">(</span><span class="n">Thumb</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">Sex</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">studentdata</span><span class="p">,</span><span class="w"> </span><span class="n">color</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">Sex</span><span class="p">,</span><span class="w"> </span><span class="n">width</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.1</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.3</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">gf_summary</span><span class="p">(</span><span class="n">fun.data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;mean_cl_boot&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="n">color</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">Sex</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>This summary function adds information that is generated by a background function called “mean_cl_boot”, which calculates the confidence level of each group mean via bootstrapping. It essentally adds a dot for mean estimate of each group, and a line that shows the width of the 95% confidence interval. By changing the <code class="docutils literal notranslate"><span class="pre">size</span></code>, <code class="docutils literal notranslate"><span class="pre">linewidth</span></code>, and <code class="docutils literal notranslate"><span class="pre">color</span></code> arguments, you can control what these lines look like:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">gf_jitter</span><span class="p">(</span><span class="n">Thumb</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">Sex</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">studentdata</span><span class="p">,</span><span class="w"> </span><span class="n">color</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">Sex</span><span class="p">,</span><span class="w"> </span><span class="n">width</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.1</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.3</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">  </span><span class="nf">gf_summary</span><span class="p">(</span><span class="n">fun.data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;mean_cl_boot&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">linewidth</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1.2</span><span class="p">,</span><span class="w"> </span><span class="n">color</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;black&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>These parts of a plot are called <strong>error bars</strong>. Compared to generating the <code class="docutils literal notranslate"><span class="pre">summary()</span></code> and <code class="docutils literal notranslate"><span class="pre">confint()</span></code> outputs for a model and interpreting the numbers, adding these to a plot is a very quick way to communicate model estimates and the uncertainty around them.</p>
<p>In a regression plot, the estimate of an effect is the slope of the regression line. To communicate the range of slope parameters that are likely to produce our specific estimate, we can add a <strong>confidence band</strong> to the <code class="docutils literal notranslate"><span class="pre">gf_lm()</span></code> layer:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">gf_point</span><span class="p">(</span><span class="n">Thumb</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">Height</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">studentdata</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">    </span><span class="nf">gf_lm</span><span class="p">(</span><span class="n">interval</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;confidence&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We could tilt the regression line like a seesaw within this band, and that would reflect the range of the 95% confidence interval around the slope estimate.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">interact_plot()</span></code> function also allows us to add confidence bands using the pair of arguments <code class="docutils literal notranslate"><span class="pre">interval</span> <span class="pre">=</span> <span class="pre">TRUE</span></code> and <code class="docutils literal notranslate"><span class="pre">int.width</span> <span class="pre">=</span> <span class="pre">0.95</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">interact_plot</span><span class="p">(</span><span class="n">interactionmodel</span><span class="p">,</span><span class="w"> </span><span class="n">pred</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Height</span><span class="p">,</span><span class="w"> </span><span class="n">modx</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">Sex</span><span class="p">,</span>
<span class="w">             </span><span class="n">interval</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">,</span><span class="w"> </span><span class="n">int.width</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.95</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Lastly, we’ve already explored in chapter 13 how it is difficult to visualize relationships between more than two variables. With three we can change the color of points or show scatter plots between one variable and the outcome residuals after regressing out all other predictors, but more than three variables can get even more challenging. In the case where there are several predictors, researchers often make a more simple plot that is called a <strong>forest plot.</strong> Rather than showing the raw data, a forest plot depicts the b estimates for every predictor in a model with error bars around it.</p>
<p>In R there are several ways to do this, and they all take a bit of work to set up. Essentially, you fit a multivariable model, save the b estimates and confidence intervals for each variable to a separate data frame, and then make a plot out of that. We’ll use the <code class="docutils literal notranslate"><span class="pre">forestplot</span></code> package to make one here. Read each line of this code slowly and make sure you understand what it is doing.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">#install.packages(&quot;forestplot&quot;)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">forestplot</span><span class="p">)</span>

<span class="n">multimodel</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">lm</span><span class="p">(</span><span class="n">Thumb</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">Height</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">Siblings</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">Pinkie</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">Middle</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">studentdata</span><span class="p">)</span>
<span class="n">bs</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">multimodel</span><span class="o">$</span><span class="n">coefficients</span><span class="p">[</span><span class="m">2</span><span class="o">:</span><span class="m">5</span><span class="p">]</span><span class="w"> </span><span class="c1">#saving bs of all predictors</span>
<span class="n">predictors</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s">&quot;Height&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;Siblings&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;Pinkie length&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;Middle finger length&quot;</span><span class="p">)</span><span class="w"> </span><span class="c1">#vector of predictor names</span>
<span class="n">CI</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">confint</span><span class="p">(</span><span class="n">multimodel</span><span class="p">)</span>
<span class="n">upper_ci</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">CI</span><span class="p">[</span><span class="m">2</span><span class="o">:</span><span class="m">5</span><span class="p">,</span><span class="m">2</span><span class="p">]</span><span class="w"> </span><span class="c1">#place to hold all upper CI values</span>
<span class="n">lower_ci</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">CI</span><span class="p">[</span><span class="m">2</span><span class="o">:</span><span class="m">5</span><span class="p">,</span><span class="m">1</span><span class="p">]</span><span class="w"> </span><span class="c1">#place to hold all lower CI values</span>

<span class="n">forest_data</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">data.frame</span><span class="p">(</span><span class="n">mean</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bs</span><span class="p">,</span><span class="w">             </span><span class="c1">#the point to visualize on plot</span>
<span class="w">                         </span><span class="n">upper</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">upper_ci</span><span class="p">,</span><span class="w">       </span><span class="c1">#the value of the top error bar</span>
<span class="w">                         </span><span class="n">lower</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">lower_ci</span><span class="p">,</span><span class="w">       </span><span class="c1">#the value of the lower error bar</span>
<span class="w">                         </span><span class="n">predictors</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">predictors</span><span class="p">)</span><span class="c1">#names of predictors to put on plot</span>

<span class="nf">forestplot</span><span class="p">(</span><span class="n">forest_data</span><span class="p">,</span><span class="w"> </span><span class="n">labeltext</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">predictors</span><span class="p">,</span><span class="w"> </span><span class="n">boxsize</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.08</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>This plot ultimately gives you a “forest” of b estimates and the confidence interval around them, all positioned relative to each other. This way you can easily see how big the b estimates are, how wide their confidence intervals are, and which confidence intervals overlap with 0 or not.</p>
</section>
<section id="chapter-summary">
<h2>Chapter summary<a class="headerlink" href="#chapter-summary" title="Permalink to this heading">#</a></h2>
<p>After reading this chapter, you should be able to:</p>
<ul class="simple">
<li><p>Define a sampling distribution</p></li>
<li><p>Describe the relationship between sample size and the shape/center of the sampling distribution</p></li>
<li><p>Define the standard error</p></li>
<li><p>Calculate a 95% confidence interval</p></li>
<li><p>Interpret the meaning of a confidence interval of an estimate</p></li>
<li><p>Add confidence intervals to model visualizations</p></li>
</ul>
<p><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-16.ipynb">Next: Chapter 16 - Significance Testing</a></p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "smburns47/Psyc158",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "r"
        },
        kernelOptions: {
            name: "ir",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'ir'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluating-model-performance">15.1 Evaluating model performance</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sampling-distributions">15.2 Sampling distributions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-importance-of-sample-size">15.3 The importance of sample size</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-central-limit-theorem">15.4 The Central Limit Theorem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-population-parameters">15.5 Estimating population parameters</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-the-population-mean">Estimating the population mean</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#estimating-the-population-standard-deviation">Estimating the population standard deviation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sampling-distributions-of-model-outputs">15.6 Sampling distributions of model outputs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#confidence-intervals">15.7 Confidence intervals</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pitfalls-of-interpreting-confidence-intervals">Pitfalls of interpreting confidence intervals</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-uncertainty">15.8 Visualizing uncertainty</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#chapter-summary">Chapter summary</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Shannon Burns
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>