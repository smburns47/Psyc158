

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Chapter 16 - Significance Testing &#8212; Pomona Psych 158 Online Textbook</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapter-16';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Pomona College Psych 158 Online Textbook
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Unit 1 Describing Data</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-1.ipynb">Chapter 1 - Introduction to Statistical Thinking</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-2.ipynb">Chapter 2 - What are Data</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-3.ipynb">Chapter 3 - Organizing Data</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-4.ipynb">Chapter 4 - Cleaning Data</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-5.ipynb">Chapter 5 - Describing Data</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-6.ipynb">Chapter 6 - Variation in Multiple Variables</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-7.ipynb">Chapter 7 - Principles of Data Visualization</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Unit 2 - Modeling Data</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-8.ipynb">Chapter 8 - Where Data Come From</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-9.ipynb">Chapter 9 - Modeling the Data Generation Process</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-10.ipynb">Chapter 10 - Quantifying Model Error</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-11.ipynb">Chapter 11 - Adding an Explanatory Variable</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-12.ipynb">Chapter 12 - Quantitative Predictor Models</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-13.ipynb">Chapter 13 - Multivariable Models</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-14.ipynb">Chapter 14 - Models with Moderation</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Unit 3 - Evaluating Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-15.ipynb">Chapter 15 - Estimating Populations</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-16.ipynb">Chapter 16 - Significance Testing</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-17.ipynb">Chapter 17 - Significance Testing Whole Models</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-18.ipynb">Chapter 18 - Effect Sizes &amp; Statistical Power</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-19.ipynb">Chapter 19 - Model Bias</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-20.ipynb">Chapter 20 - Alternate Approaches - Traditional Statistical Tools</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-21.ipynb">Chapter 21 - Lying with Statistics</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/smburns47/Psyc158/main?urlpath=tree/chapter-16.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onBinder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="initThebeSBT()"
  class="btn btn-sm btn-launch-thebe dropdown-item"
  title="Launch Thebe"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="btn__text-container">Live Code</span>
</button>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/smburns47/Psyc158" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/smburns47/Psyc158/issues/new?title=Issue%20on%20page%20%2Fchapter-16.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/chapter-16.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Chapter 16 - Significance Testing</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-null-hypothesis">16.1 The null hypothesis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sampling-distribution-of-an-estimate">16.2 Sampling distribution of an estimate</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#significance-testing">16.3 Significance testing</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-p-value">16.4 The p-value</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-t-distribution">16.5 The t-distribution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#things-that-affect-the-p-value">16.5 Things that affect the p-value</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-limits-of-significance-testing">16.6 The limits of significance testing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#type-i-and-type-ii-error">Type I and Type II error</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#can-only-reject-the-null-not-support-it">Can only reject the null, not support it</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#significance-is-a-binary-decision">Significance is a binary decision</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-of-data-not-population">Probability of data, not population</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#p-values-and-sample-size">P-values and sample size</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#significant-effects-in-practice">16.7 Significant effects in practice</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#effects-in-the-simple-model">Effects in the simple model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#effects-in-the-multivariable-model">Effects in the multivariable model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#effects-in-interaction-models">Effects in interaction models</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#frequentist-vs-bayesian-hypothesis-testing">16.8 Frequentist vs. Bayesian hypothesis testing</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#chapter-summary">Chapter summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#new-concepts">New concepts</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <p><a class="reference external" href="https://www.shannonmburns.com/Psyc158/intro.html">Back to Table of Contents</a></p>
<p><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-15.ipynb">Previous: Chapter 15 - Estimating Populations</a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Run this first so it&#39;s ready by the time you need it</span>
<span class="nf">install.packages</span><span class="p">(</span><span class="s">&quot;ggformula&quot;</span><span class="p">)</span>
<span class="nf">install.packages</span><span class="p">(</span><span class="s">&quot;dplyr&quot;</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">ggformula</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">dplyr</span><span class="p">)</span>
<span class="n">studentdata</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">read.csv</span><span class="p">(</span><span class="s">&quot;https://raw.githubusercontent.com/smburns47/Psyc158/main/studentdata.csv&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section class="tex2jax_ignore mathjax_ignore" id="chapter-16-significance-testing">
<h1>Chapter 16 - Significance Testing<a class="headerlink" href="#chapter-16-significance-testing" title="Permalink to this heading">#</a></h1>
<section id="the-null-hypothesis">
<h2>16.1 The null hypothesis<a class="headerlink" href="#the-null-hypothesis" title="Permalink to this heading">#</a></h2>
<p>In this course we have discussed how we use statistics to test pre-existing hypotheses about data. So far, our hypotheses have only been as specific as “a predictor explains some of the variance in an outcome.”</p>
<p>But we can define a more formal hypothesis than that. The strictest definition of a hypothesis is a pre-defined statement about what value a model coefficient will have, once we fit the model. Once we have a formal hypothesis about what the population parameter is, we can judge whether or not we think a sample of data likely came from that population or not.</p>
<p>For example, rather than hypothesizing generally that someone’s height is related to their thumb length, a formal hypothesis would be “the effect of height for predicting thumb length is 0.9.” In Frequentist statistics, we assume there is one population parameter from which random samples are derived. So we can collect some data, fit a model, build a confidence interval, and check if the estimated interval includes our hypothesized 0.9 parameter. If it does, then we would say the data is consistent with the proposed population - some data like this is likely to be generated by a population with a coefficient of 0.9. However if 0.9 is not in the confidence interval, we would be very surprised to hear that this data was indeed generated by this parameter. Only a very few rare samples would look like ours, if 0.9 were the true coefficient. We would think it more likely that a different sort of population generated the data.</p>
<p>Try thinking of some formal hypotheses for other research questions. E.g., predicting someone’s resting heart rate based on how many hours during the week they spend in the gym. What do you think the specific value of <span class="math notranslate nohighlight">\(\beta_1\)</span> would be in a model such as <code class="docutils literal notranslate"><span class="pre">heart</span> <span class="pre">rate</span> <span class="pre">~</span> <span class="pre">gym</span> <span class="pre">hours</span></code>?</p>
<p>If you’re having a hard time thinking of where to even start with that question, you wouldn’t be alone. A specific hypothesis is really tough to create and requires a level of knowledge about the data generation process that is unpractical for most topics we care about. It’s much easier to hypothesize that there is just <em>some</em> sort of relationship between gym hours and heart rate, such that the population parameter is something that is NOT 0.</p>
<p>This is the crux of how Frequentists test hypotheses, called <strong>Null Hypothesis Significance Testing</strong> (NHST). In this process, your hypothetical thinking skills will be exercised. First, you pose a formal hypothesis that <span class="math notranslate nohighlight">\(\beta = 0\)</span>. This is the <strong>null hypothesis</strong>, that there is a null effect for <span class="math notranslate nohighlight">\(\beta\)</span>. The null hypothesis name is often abbreviated as <span class="math notranslate nohighlight">\(H_0\)</span>. Then, we collect data, make an estimate, and judge how likely a population with <span class="math notranslate nohighlight">\(\beta = 0\)</span> is to produce these data. If it’s very unlikely, we decide that the data were probably generated by some other population: the <strong>alternative hypothesis</strong>. We <em>reject</em> the null hypothesis.</p>
<p>It’s important to remember that an estimate of a coefficient having a non-zero value is not enough to reject the null hypothesis. As we saw last chapter, a population with a specific parameter can generate samples with a range of estimates. So if we get a sample with <span class="math notranslate nohighlight">\(b = 0.5\)</span>, is that different enough from 0 to reject the null hypothesis? Or could that estimate have been reasonably generated by a population with <span class="math notranslate nohighlight">\(\beta = 0\)</span>?</p>
<p>To answer this question, we turn to a concept called <strong>statistical significance</strong>.</p>
</section>
<section id="sampling-distribution-of-an-estimate">
<h2>16.2 Sampling distribution of an estimate<a class="headerlink" href="#sampling-distribution-of-an-estimate" title="Permalink to this heading">#</a></h2>
<p>Let’s refresh on the concept of a sampling distribution in a concrete example before we learn how to test the statistical significance of a sample estimate. Going back to the <code class="docutils literal notranslate"><span class="pre">studentdata</span></code> dataset, we previously explored how someone’s sex would explain some of the variation in thumb lengths. In other words, we thought there was a non-zero difference between the mean of female thumbs and the mean of male thumbs. We didn’t have a more specific hypothesis than that, but that “non-zero” part is enough to define a null hypothesis and test whether or not these data likely came from a population that <em>does</em> have a zero difference between female and male thumbs.</p>
<p>In our equation of the statistical model of this question:</p>
<div class="math notranslate nohighlight">
\[Y_i = b_0 + b_1X_i + e_i \]</div>
<p><span class="math notranslate nohighlight">\(X_i\)</span> represents whether or not someone is male or female. The parameter estimate <span class="math notranslate nohighlight">\(b_1\)</span> is the effect of Sex (i.e., difference in thumb lengths between males and females), and is thus the specific one we are interested in. This is our best estimate of <span class="math notranslate nohighlight">\(\beta_1\)</span>, the true effect of sex in the population of thumb lengths.</p>
<p>Before we fit this model again and get an estimate, let’s imagine what we would expect to see if a particular version of the data generation process were true. If there is a real difference between male and female thumb lengths such that male thumbs were generally longer (i.e., if <span class="math notranslate nohighlight">\(\beta_1\)</span> is a positive value), we might expect that samples from such a population would have positive <span class="math notranslate nohighlight">\(b_1\)</span> values on average in the sampling distribution.</p>
<p>Although we couldn’t predict any single <span class="math notranslate nohighlight">\(b_1\)</span> that will be drawn from the population, we can make predictions about the average <span class="math notranslate nohighlight">\(b_1\)</span> that would be generated from multiple random samples. On average, the <span class="math notranslate nohighlight">\(b_1\)</span> estimates would cluster around the parent <span class="math notranslate nohighlight">\(\beta_1\)</span> from which they come. So a negative <span class="math notranslate nohighlight">\(\beta_1\)</span> would tend to produce negative <span class="math notranslate nohighlight">\(b_1\)</span>, a positive <span class="math notranslate nohighlight">\(\beta_1\)</span> would tend to produce positive <span class="math notranslate nohighlight">\(b_1\)</span>.</p>
<p>The null hypothesis is a special case in which <span class="math notranslate nohighlight">\(\beta_1 = 0\)</span>. If the null hypothesis is true it means that someone’s sex has no effect on their thumb length. The <span class="math notranslate nohighlight">\(b_1\)</span> values generated by multiple random samples from a population in which <span class="math notranslate nohighlight">\(\beta_1 = 0\)</span> would tend to cluster around 0, but they wouldn’t necessarily be exactly 0. We can construct a sampling distribution to find out if a single <span class="math notranslate nohighlight">\(b_1\)</span> for our sample could have been generated by the null hypothesis. This is called the <strong>null sampling distribution</strong>.</p>
<p>We can make a null sampling distribution from our own data with a type of simulation called <strong>permutation testing</strong>. In this, we <em>permute</em> or shuffle around the datapoints in the predictor variable, thus breaking any relationship between a predictor and the outcome. We can use <code class="docutils literal notranslate"><span class="pre">sample()</span></code> to do this, drawing all the datapoints in a variable randomly without replacement and then making a new variable out of that randomized vector.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">#first ten values before shuffling</span>
<span class="n">studentdata</span><span class="o">$</span><span class="n">Sex</span><span class="p">[</span><span class="m">1</span><span class="o">:</span><span class="m">10</span><span class="p">]</span>

<span class="n">studentdata</span><span class="o">$</span><span class="n">Shuffled_Sex</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sample</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">studentdata</span><span class="o">$</span><span class="n">Sex</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="o">=</span><span class="nf">length</span><span class="p">(</span><span class="n">studentdata</span><span class="o">$</span><span class="n">Sex</span><span class="p">),</span><span class="w"> </span><span class="n">replace</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">)</span>

<span class="c1">#first ten values after shuffling</span>
<span class="n">studentdata</span><span class="o">$</span><span class="n">Shuffled_Sex</span><span class="p">[</span><span class="m">1</span><span class="o">:</span><span class="m">10</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Breaking any association between <code class="docutils literal notranslate"><span class="pre">Sex</span></code> and <code class="docutils literal notranslate"><span class="pre">Thumb</span></code> by shuffling the values of <code class="docutils literal notranslate"><span class="pre">Sex</span></code> around makes it so that any still existing relationship between those variables is only due to randomness - the true association, or true <span class="math notranslate nohighlight">\(\beta_1\)</span>, is 0. Doing this many times and collecting the sample estimates of models fit on these shuffled data will give us a sampling distribution of <span class="math notranslate nohighlight">\(b_1\)</span> when the null hypothesis is true.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">#creating an empty vector of 1000 spots</span>
<span class="n">null_b1s</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">vector</span><span class="p">(</span><span class="n">length</span><span class="o">=</span><span class="m">1000</span><span class="p">)</span>

<span class="c1">#generate 1000 unique shuffled samples, saving each b1</span>
<span class="nf">set.seed</span><span class="p">(</span><span class="m">47</span><span class="p">)</span>
<span class="nf">for </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="m">1000</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">studentdata</span><span class="o">$</span><span class="n">Shuffled_Sex</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sample</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">studentdata</span><span class="o">$</span><span class="n">Sex</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="o">=</span><span class="nf">length</span><span class="p">(</span><span class="n">studentdata</span><span class="o">$</span><span class="n">Sex</span><span class="p">),</span><span class="w"> </span><span class="n">replace</span><span class="o">=</span><span class="kc">FALSE</span><span class="p">)</span>
<span class="w">    </span><span class="n">model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">lm</span><span class="p">(</span><span class="n">Thumb</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">Shuffled_Sex</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="o">=</span><span class="n">studentdata</span><span class="p">)</span>
<span class="w">    </span><span class="n">null_b1s</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">model</span><span class="o">$</span><span class="n">coefficients</span><span class="p">[[</span><span class="m">2</span><span class="p">]]</span>
<span class="p">}</span>

<span class="n">b1s_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">data.frame</span><span class="p">(</span><span class="n">null_b1s</span><span class="p">)</span>
<span class="nf">gf_histogram</span><span class="p">(</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">null_b1s</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="o">=</span><span class="n">b1s_df</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>In this sampling distribution, we can see that <span class="math notranslate nohighlight">\(b_1\)</span> varies each time we shuffle the data and calculate a new model fit. Also, the center seems to be around 0. We know from last chapter that the mean of a sampling distribution converges on the population parameter. Because the sampling distribution is based on the null hypothesis, where <span class="math notranslate nohighlight">\(\beta_1 = 0\)</span>, we expect the parameter estimates to be clustered around 0. But we also expect each individual one to vary because of sampling variation. We can see here that it’s possible to get a <span class="math notranslate nohighlight">\(b_1\)</span> estimate as high as +/-5 just by chance, even when there is no true difference between female and male thumbs!</p>
<p>However, these values are really rare in the sampling distribution. It’s much more common to generate small mean differences (e.g., 0.6).</p>
<p>Just eyeballing the histogram can give us a rough idea of the probability of getting a particular sample <span class="math notranslate nohighlight">\(b_1\)</span> from this population where we know <span class="math notranslate nohighlight">\(\beta_1\)</span> is equal to 0. When we use these frequencies to estimate probability, we are using this distribution of shuffled <span class="math notranslate nohighlight">\(b_1\)</span> values as a probability distribution.</p>
</section>
<section id="significance-testing">
<h2>16.3 Significance testing<a class="headerlink" href="#significance-testing" title="Permalink to this heading">#</a></h2>
<p>We used R to simulate a world where the null hypothesis is true in order to construct a sampling distribution. Now let’s return to our original goal, to decide whether the null hypothesis is a good explanation for our data, or if it should be rejected.</p>
<p>The basic idea is this: using the sampling distribution of possible sample <span class="math notranslate nohighlight">\(b_1\)</span> values that could have resulted from a population in which the null hypothesis is true (i.e., in which <span class="math notranslate nohighlight">\(\beta_1 = 0\)</span>), we can look at a specific sample <span class="math notranslate nohighlight">\(b_1\)</span> that we estimated with data and gauge how likely such a <span class="math notranslate nohighlight">\(b_1\)</span> would be if the null hypothesis is, in fact, true.</p>
<p>If we judge the <span class="math notranslate nohighlight">\(b_1\)</span> we observed to be unlikely to have come from the null hypothesis, we then reject <span class="math notranslate nohighlight">\(H_0\)</span> as our specific idea about the data generation process. If, on the other hand, we judge our observed <span class="math notranslate nohighlight">\(b_1\)</span> to be likely (or at least not all that <em>unlikely</em>), then the null hypothesis could still be a good explanation for these data. We fail to reject the null hypothesis.</p>
<p>Let’s see how this works in the context of the <code class="docutils literal notranslate"><span class="pre">studentdata</span></code> dataset, where <span class="math notranslate nohighlight">\(b_1\)</span> represents the difference in average thumb lengths between males and females.</p>
<p>Samples that are extreme in either a positive (average male thumbs much longer than females) or negative direction (average male thumbs much shorter than females) are unlikely to be generated if the true <span class="math notranslate nohighlight">\(\beta_1 = 0\)</span>. If we saw a sample <span class="math notranslate nohighlight">\(b_1\)</span> like this, we would doubt that the null hypothesis produced it.</p>
<p>Put another way: if we had a sample that fell in either the extreme upper tail or extreme lower tail of the null sampling distribution (see figure below), we might reject the null hypothesis as the true picture of the data generation process.</p>
<img src="images/ch16-samplingtails.png" width="500">
<p>In statistics, this is commonly referred to as a two-tailed significance test because whether our actual sample falls in the extreme upper tail or extreme lower tail of this sampling distribution, we would have reason to reject the null hypothesis as the true version of the population. By rejecting the null hypothesis, we decide instead that some alternative hypothesis where <span class="math notranslate nohighlight">\(\beta_1 \neq 0\)</span> must be true. We wouldn’t know exactly what the true <span class="math notranslate nohighlight">\(\beta_1\)</span> is, only that it is probably not 0. In more traditional statistical terms, we would have found a statistically significant difference between male thumb lengths and female thumb lengths. <span class="math notranslate nohighlight">\(b_1\)</span> is significantly different from 0.</p>
<p>All of this, however, begs the question of how extreme a sample <span class="math notranslate nohighlight">\(b_1\)</span> would need to be in order for us to reject the null hypothesis. What is unlikely to one person might not seem so unlikely to another person. It would help to have some sort of agreed upon standard of what counts as unlikely before we actually bring in our real sample statistic. The definition of “unlikely” depends on what you are trying to do with your statistical model and what your community of practice agrees on.</p>
<p>The common standard used in the social sciences is that a sample counts as unlikely if there is less than a 5% chance of generating a sample estimate at least as extreme as this one (either negative or positive) in the null sampling distribution. We notate this numerical definition of “unlikely” with the Greek letter <span class="math notranslate nohighlight">\(\alpha\)</span> (pronounced “alpha”). A scientist might describe this criterion by writing that they “set <span class="math notranslate nohighlight">\(\alpha = .05\)</span>”. If they wanted to use a stricter definition of unlikely, they might say “<span class="math notranslate nohighlight">\(\alpha = .001\)</span>,” indicating that a sample would have to be really unlikely for us to reject the null hypothesis.</p>
<p>Let’s identify an α-level of .05 in the null sampling distribution of <span class="math notranslate nohighlight">\(b_1\)</span> we generated from random shuffles of thumb lengths. If you take the 1000 <span class="math notranslate nohighlight">\(b_1\)</span> values and line them up in order, the 2.5% lowest quartile and the 2.5% highest quartile would be the most extreme 5% of values and therefore the most unlikely values to be randomly generated. So let’s sort our null distribution and then find the value corresponding to the 2.5%ile and 97.5%ile. Since there are 1,000 data points in our simulated null sampling distribution, this would be the 25th and 975th values of the sorted vector.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">#cut-off values for extremeness</span>
<span class="n">sorted_nullb1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sort</span><span class="p">(</span><span class="n">null_b1s</span><span class="p">)</span>
<span class="n">high_cutoff</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sorted_nullb1</span><span class="p">[</span><span class="m">975</span><span class="p">]</span>
<span class="n">low_cutoff</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sorted_nullb1</span><span class="p">[</span><span class="m">25</span><span class="p">]</span>
<span class="n">high_cutoff</span>
<span class="n">low_cutoff</span>

<span class="c1">#marking something as extreme if it is greater than 97.5%ile or less than 2.5%ile</span>
<span class="n">b1s_df</span><span class="o">$</span><span class="n">extreme</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">b1s_df</span><span class="o">$</span><span class="n">null_b1s</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="n">high_cutoff</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">b1s_df</span><span class="o">$</span><span class="n">null_b1s</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">low_cutoff</span>

<span class="nf">gf_histogram</span><span class="p">(</span><span class="o">~</span><span class="w"> </span><span class="n">null_b1s</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">b1s_df</span><span class="p">,</span><span class="w"> </span><span class="n">fill</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">~</span><span class="n">extreme</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>A neat trick to know is, if we convert a sampling distribution to z-scores, we automatically know which samples are extreme or not by looking at which z-scores are above 1.96 or below -1.96, since 1 in z-scored units corresponds to 1 standard deviation and the 2.5%ile and 97%ile values are 1.96 SDs away from the mean.</p>
</section>
<section id="the-p-value">
<h2>16.4 The p-value<a class="headerlink" href="#the-p-value" title="Permalink to this heading">#</a></h2>
<p>We have now spent some time looking at the sampling distribution of <span class="math notranslate nohighlight">\(b_1\)</span> assuming the null hypothesis is true (i.e., <span class="math notranslate nohighlight">\(\beta_1 = 0\)</span>). We have developed the idea that simulated samples, generated by random shuffles of the thumb length data, are typically clustered around 0. Samples that end up in the tails of the distribution – the upper and lower 2.5% of values – are considered unlikely.</p>
<p>Let’s place our sample <span class="math notranslate nohighlight">\(b_1\)</span> onto our histogram of the sampling distribution and see where it falls. Does it fall in the tails of the distribution, or in the middle 95%?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">sex_model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">lm</span><span class="p">(</span><span class="n">Thumb</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">Sex</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="o">=</span><span class="n">studentdata</span><span class="p">)</span>
<span class="n">b1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sex_model</span><span class="o">$</span><span class="n">coefficients</span><span class="p">[[</span><span class="m">2</span><span class="p">]]</span>

<span class="nf">gf_histogram</span><span class="p">(</span><span class="o">~</span><span class="w"> </span><span class="n">null_b1s</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">b1s_df</span><span class="p">,</span><span class="w"> </span><span class="n">fill</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">~</span><span class="n">extreme</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">    </span><span class="nf">gf_point</span><span class="p">(</span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">b1</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">3</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">    </span><span class="nf">gf_refine</span><span class="p">(</span><span class="nf">coord_cartesian</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">-7</span><span class="p">,</span><span class="m">7</span><span class="p">)))</span><span class="w"> </span>
</pre></div>
</div>
</div>
</div>
<p>We can see that our sample is in the unlikely zone. It is beyond the point where the histogram of the null sampling distribution switches to blue extreme values. Thus we would reject that it comes from a population where male and female thumb lengths are the same, and say that our estimate of <span class="math notranslate nohighlight">\(\beta_1\)</span> is significantly different than 0.</p>
<p>But we can say it more specifically than this, and give its location in the tail a quantitative score. Instead of only asking whether the sample <span class="math notranslate nohighlight">\(b_1\)</span> is in the unlikely area or not (yes or no), we could instead ask, what is the <em>probability</em> of getting a <span class="math notranslate nohighlight">\(b_1\)</span> as extreme or more extreme as the one observed in the actual experiment, if the null hypothesis were true? The answer to this question is called the <strong>p-value</strong>.</p>
<p>We know what our <span class="math notranslate nohighlight">\(\alpha\)</span> is before we even do a study – it’s just a statement of our criterion for deciding what we will count as unlikely. Whatever the values in the null sampling distribution end up being, we define the “extreme” ones as the top and bottom 2.5% of the distribution. The p-value is calculated after we do a study, based on actual sample data. The p-value is calculated based on both the value of the sample estimate we want to assign a p-value and the standard error (which in turn depends on the sample size).</p>
<p>We calculate the p-value of any potential <span class="math notranslate nohighlight">\(b_1\)</span> value as the cumulative probability that we would produce a <span class="math notranslate nohighlight">\(b_1\)</span> as extreme or more extreme than this one when the null hypothesis is true. Since our <span class="math notranslate nohighlight">\(\alpha = 0.05\)</span>, any p-value &lt; 0.05 would be considered significant.</p>
<p>We can use the <code class="docutils literal notranslate"><span class="pre">summary()</span></code> function we learned about earlier to find the exact p-value for a sample estimate:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">summary</span><span class="p">(</span><span class="n">sex_model</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Look at the last column in the Coefficients table called “Pr(&gt;|t|)”. This is the probability of getting a value more extreme than our <span class="math notranslate nohighlight">\(b_1\)</span> estimate under the null hypothesis - aka the p-value. You can see it is a very small number - clearly smaller than our 0.05 <span class="math notranslate nohighlight">\(\alpha\)</span> criterion. We would thus decide that a <span class="math notranslate nohighlight">\(b_1\)</span> estimate of 6.056 is so unusual in the sampling distribution of <span class="math notranslate nohighlight">\(\beta_1 = 0\)</span> that that probably is not the correct population parameter that created this estimate. We reject the null hypothesis.</p>
<p>You’ll see the intercept has a p-value too, also a tiny number. This is saying that we can reject the null hypothesis that <span class="math notranslate nohighlight">\(\beta_0\)</span> is 0. All fitted parameters in a linear model get a p-value, but you might not care to evaluate all of these parameters if your research question is about a particular one.</p>
<p>Something else to notice as well is the confidence interval for <span class="math notranslate nohighlight">\(b_1\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">confint</span><span class="p">(</span><span class="n">sex_model</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;SexMale&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">level</span><span class="o">=</span><span class="m">0.95</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>This suggests that we are 95% confident that the true difference between male and female thumb lengths is somewhere between 3.57mm and 9.32mm. This interval does not include 0, so we are 95% confident that the null hypothesis is not the parameter that made these data.</p>
<p>A confidence interval tells you the same thing as a p-value, just in a different way. If the interval does not include 0, we decide that this estimate is significantly different than 0. If it does include 0, then 0 might reasonably be the parameter that created these data and we fail to reject the null hypothesis.</p>
</section>
<section id="the-t-distribution">
<h2>16.5 The t-distribution<a class="headerlink" href="#the-t-distribution" title="Permalink to this heading">#</a></h2>
<p>The early statisticians who developed the ideas behind sampling distributions and p-values didn’t have computers. They couldn’t even imagine what it might be like to shuffle their data a thousand times in just a couple seconds. What we have been able to do with R would seem like a miracle to them! So instead of using computational techniques to create sampling distributions, the early statisticians had to develop mathematical formulas of what the sampling distributions should look like, and then calculate probabilities based on these mathematical distributions. These calculations are what the <code class="docutils literal notranslate"><span class="pre">summary()</span></code> function uses to give us exact p-values, so we’ll learn a little about that now to understand where these numbers come from.</p>
<p>The mathematical concept that <code class="docutils literal notranslate"><span class="pre">summary()</span></code> uses to understand the sampling distribution of coefficient estimates is known as the <strong>t-distribution</strong>. The t-distribution looks very similar to the normal distribution.</p>
<p>In the figure below we have overlaid the t-distribution (depicted as a red line) on top of a sampling distribution constructed with permutation testing. You can see that it looks very much like the normal distribution you learned about previously.</p>
<img src="images/ch16-nulldist.png" width="600">
<p>While the sampling distribution we created using permutation testing looks jagged (because it was made up of just 1000 separate estimates), the t-distribution is a smooth continuous mathematical function. It is the theoretically idealized shape of the null sampling distribution. If you want to see the equation that describes this shape, you can see it <a class="reference external" href="https://mathworld.wolfram.com/Studentst-Distribution.html">here</a>.</p>
<p>Whereas the shape of the normal distribution is completely determined by its mean and standard deviation, the t-distribution changes shape slightly depending on how many degrees of freedom are in the samples that make up the sampling distribution. You can see how degrees of freedom affect the shape of the t-distribution in the figure below. Once the degrees of freedom reach about 30, however, the t-distribution looks very similar to the normal distribution.</p>
<img src="images/ch16-tdist.png" width="600">
<p>To calculate a p-value, we find the cumulative probabilities in the upper and lower tails of the t-distribution. I.e., the area under the curve that is more extreme than + or - our sample <span class="math notranslate nohighlight">\(b_1\)</span>. Fortunately, you don’t have to do this math; R will do it for you in <code class="docutils literal notranslate"><span class="pre">summary()</span></code>. But you <em>could</em> do it with probability sampling functions like <a class="reference external" href="https://stat.ethz.ch/R-manual/R-devel/library/stats/html/TDist.html">these</a>, which work the same way as <code class="docutils literal notranslate"><span class="pre">rnorm()</span></code> or <code class="docutils literal notranslate"><span class="pre">pnorm()</span></code> that we learned in chapter 8.</p>
<p>These functions give you a <strong>t-value</strong>, telling you where on the t-distribution your sample <span class="math notranslate nohighlight">\(b_1\)</span> falls if the null hypothesis is true. This specific t-value can be found in the third column of the Coefficients table in the <code class="docutils literal notranslate"><span class="pre">summary()</span></code> output.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">#Write some code to again output the bigger results table for sex_model, not just the coefficients</span>
</pre></div>
</div>
</div>
</div>
<p>Because this is the traditional method of determining p-values and statistical significance, sometimes you’ll read research papers where the authors ran a linear model but also report a t-value. This does not mean that they did a t-test, only that they’re reporting the corresponding t-value of their beta estimate that is found in the linear model output.</p>
</section>
<section id="things-that-affect-the-p-value">
<h2>16.5 Things that affect the p-value<a class="headerlink" href="#things-that-affect-the-p-value" title="Permalink to this heading">#</a></h2>
<p>In our one-predictor model, the sample <span class="math notranslate nohighlight">\(b_1\)</span> for the difference between male and female thumb lengths was 6.056. Based on the the output of <code class="docutils literal notranslate"><span class="pre">summary()</span></code>, the probability of getting a sample with a <span class="math notranslate nohighlight">\(b_1\)</span> as extreme or more extreme than 6.056 when the null hypothesis is true is approximately 0.000113. Based on our <span class="math notranslate nohighlight">\(\alpha\)</span> criterion of .05, we decided that <span class="math notranslate nohighlight">\(b_1 = 6.056\)</span> is significantly different from 0, and we reject the null hypothesis. Some alternative population likely make these data instead.</p>
<p>Compare that to a <span class="math notranslate nohighlight">\(b_1 = 3\)</span>. This isn’t our real sample estimate, but we can use permutation testing to find what the p-value of this estimate <em>would</em> be in our simulated null sampling distribution. To do so, we simply find how many values in the <code class="docutils literal notranslate"><span class="pre">null_b1s</span></code> vector are above 3 or below 3. This number divided by the size of the full simulated sampling distribution gives us the proportion of values more extreme than 3 - aka the p-value.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">num_above</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sum</span><span class="p">(</span><span class="n">null_b1s</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="m">3</span><span class="p">)</span>
<span class="n">num_below</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sum</span><span class="p">(</span><span class="n">null_b1s</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="m">-3</span><span class="p">)</span>

<span class="n">pvalue</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="p">(</span><span class="n">num_above</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">num_below</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="nf">length</span><span class="p">(</span><span class="n">null_b1s</span><span class="p">)</span>

<span class="n">pvalue</span>
</pre></div>
</div>
</div>
</div>
<p>According to this simulation, a <span class="math notranslate nohighlight">\(b_1\)</span> of 3 would have a p-value ~ 0.072. This is larger than the <span class="math notranslate nohighlight">\(\alpha\)</span> value of 0.05, so we fail to reject the null hypothesis. This kind of group difference is not so unlikely that we think the null hypothesis didn’t generate it.</p>
<p>As evidenced here, the p-value is affected by how far the observed <span class="math notranslate nohighlight">\(b_1\)</span> is from 0. Since 6.056 is further away from 0 than 3 is from 0, <span class="math notranslate nohighlight">\(b_1 = 6.056\)</span> has a smaller p-value. The further away <span class="math notranslate nohighlight">\(b_1\)</span> is from 0, the lower the p-value, meaning the less likely the observed <span class="math notranslate nohighlight">\(b_1\)</span> estimate is to have been produced by the null hypothesis.</p>
<p>But the distance between <span class="math notranslate nohighlight">\(b_1\)</span> and 0 (or the hypothesized <span class="math notranslate nohighlight">\(\beta_1\)</span>) is not the only thing that affects a p-value. The other important factor is the width of the sampling distribution, also known as the standard error.</p>
<p>Take a look at the two simulated sampling distributions in the figure below. The one on the left is something like what we created in our permutation test earlier, where <span class="math notranslate nohighlight">\(b_1 = 3\)</span> is not significant. The one on the right is similar, but narrower. Both have a roughly normal shape, both consist of 1000 sample estimates, and both distributions are centered at 0. But the standard error is smaller for the distribution on the right. By being narrower, the sampling distribution on the right brings the extreme zone farther in, and makes a <span class="math notranslate nohighlight">\(b_1\)</span> value of 3 now significant.</p>
<img src="images/ch16-stderror.png" width="650">
<p>The standard error can make a big difference in our ability to reject the null hypothesis. If it is smaller, we will have an easier time rejecting the null. This is because whatever estimate we get for <span class="math notranslate nohighlight">\(b_1\)</span>, it will be more likely to be in the upper or lower .025 of the null sampling distribution.</p>
<p>We learned last chapter that the size of the standard error is tied to the size of the samples within it. Samples with fewer datapoints will have more varied parameter estimates, and thus the sampling distribution will have a wider standard error. In the above figure, the left sampling distribution was simulated with sample sizes of N = 100. The right sampling distribution was simulated with samples sizes of N = 300.</p>
<p>Our <code class="docutils literal notranslate"><span class="pre">studentdata</span></code> dataset has 157 data points in it, which is a fine sample size. But let’s see what would happen if it were a much smaller study, with only 20 people. We’ll use <code class="docutils literal notranslate"><span class="pre">slice_sample()</span></code> to draw only 20 random people from this dataset and then fit a model on those values.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">set.seed</span><span class="p">(</span><span class="m">47</span><span class="p">)</span>
<span class="n">smaller_studentdata</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">slice_sample</span><span class="p">(</span><span class="n">studentdata</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">20</span><span class="p">,</span><span class="w"> </span><span class="n">replace</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">FALSE</span><span class="p">)</span>

<span class="n">smaller_sex_model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">lm</span><span class="p">(</span><span class="n">Thumb</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">Sex</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">smaller_studentdata</span><span class="p">)</span>

<span class="nf">summary</span><span class="p">(</span><span class="n">smaller_sex_model</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>This standard error is much larger than we saw before. This means that extreme values of the null sampling distribution go further, and a wider range of <span class="math notranslate nohighlight">\(\beta_1\)</span> values may have created this <span class="math notranslate nohighlight">\(b_1\)</span> estimate. Our estimate <span class="math notranslate nohighlight">\(b_1\)</span> of the effect of Sex is no longer significant.</p>
<p>Sample size is thus directly tied to our ability to reject the null hypothesis, independent of what our <span class="math notranslate nohighlight">\(b_1\)</span> estimate actually is. Running a bigger study makes the null sampling distribution narrower, and thus it is easier to reject the null hypothesis. This is relevant for when we think the true value of a coefficient is something small or close to 0. If we want to get a good estimate of it (narrow confidence interval) and be confident it is not 0 (p &lt; 0.05), we need to collect a lot of data.</p>
<p>But this doesn’t mean that if we get a small p-value with a small study, that we’re in the clear. Coefficient estimates in general are less stable when sample sizes are small, meaning the difference between whether something is significant or not could be the inclusion/exclusion of one extreme data point, or some other small modeling choice. Fishing around for the perfect configuration of data that makes your results significant is called p-hacking, and increases the chance that you incorrectly reject the null hypothesis when it is actually true (more on this in chapter 22).</p>
<p>We can also take the relationship between p-values and sample size to absurd limits, to where it may be possible to collect <em>too much</em> data. Now let’s sample giant datasets, of one hundred thousand data points. We’ll use <code class="docutils literal notranslate"><span class="pre">replace=TRUE</span></code> in the <code class="docutils literal notranslate"><span class="pre">slice_sample()</span></code> function or else we would run out of datapoints to use for this simulation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">set.seed</span><span class="p">(</span><span class="m">47</span><span class="p">)</span>
<span class="n">bigger_studentdata</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">slice_sample</span><span class="p">(</span><span class="n">studentdata</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">100000</span><span class="p">,</span><span class="w"> </span><span class="n">replace</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span>

<span class="n">bigger_sex_model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">lm</span><span class="p">(</span><span class="n">Thumb</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">Sex</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bigger_studentdata</span><span class="p">)</span>

<span class="nf">summary</span><span class="p">(</span><span class="n">bigger_sex_model</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The standard error in this case is much smaller. Even a mean difference in thumb lengths as small as 0.15mm would be considered significantly different than 0 with this sample size.</p>
<p>In very large datasets, nearly every model coefficient you estimate will be statistically significant. Significance then becomes a less useful concept. Sure 0.15mm is significantly different from 0 when it’s estimated from 100k datapoints, but how much does a difference that tiny matter to us for <em>using</em> the model? Do we care that male and female thumb lengths would be different by only 0.15mm? Or is that so small that it’s no longer <em>practically</em> different than 0?</p>
</section>
<section id="the-limits-of-significance-testing">
<h2>16.6 The limits of significance testing<a class="headerlink" href="#the-limits-of-significance-testing" title="Permalink to this heading">#</a></h2>
<p>Significance testing is a major component of modern psychology research. Hypotheses and theories are made or broken on the back of p &lt; 0.05. But the strong reliance on p-values for determining whether effects exist or not is not always a good thing. There are several limits to what conclusions we can make using significance testing, and sometimes people push past these limits.</p>
<section id="type-i-and-type-ii-error">
<h3>Type I and Type II error<a class="headerlink" href="#type-i-and-type-ii-error" title="Permalink to this heading">#</a></h3>
<p>First, we typically set our <span class="math notranslate nohighlight">\(\alpha\)</span> criterion to be 0.05, meaning that any <span class="math notranslate nohighlight">\(b_1\)</span> value that is as extreme or more extreme than the 5% most extreme values in the null sampling distribution will be treated as unlikely. For unlikely estimates, we decide that they are probably not from the null hypothesis at all. This could be the right decision…</p>
<p>But it might be the wrong decision. If the null hypothesis <em>is</em> true, 5% of the <span class="math notranslate nohighlight">\(b_1\)</span> values that could appear would be extreme enough to lead us to reject the null hypothesis. We would be incorrectly deciding that these data came from a different distribution than they actually did. If we rejected the null hypothesis when it is in fact true, we would be making what’s called a <strong>Type I error</strong> or <strong>false positive</strong>.</p>
<p>Our chance of making this type of inference error is directly tied to the <span class="math notranslate nohighlight">\(\alpha\)</span> level we chose. By setting <span class="math notranslate nohighlight">\(\alpha = .05\)</span>, we are saying that <span class="math notranslate nohighlight">\(b_1\)</span> values above or below 1.96 SDs of the null sampling distribution are so unlikely that we think they’re from a different population. But by definition, 5% of the <span class="math notranslate nohighlight">\(b_1\)</span> values in the <span class="math notranslate nohighlight">\(\beta_1 = 0\)</span> sampling distribution are this level of unlikely. Thus when the null hypothesis is true, 5% of samples drawn from the population will lead us to the wrong conclusion.</p>
<p>We can reduce this error rate by making our <span class="math notranslate nohighlight">\(\alpha\)</span> value smaller. Maybe instead of 5% of the null sampling distribution being surprising, we set <span class="math notranslate nohighlight">\(\alpha = 0.001\)</span> such that only 0.1% of the null sampling distribution would be considered unlikely. This would make it harder for us to erroneously reject the null hypothesis.</p>
<p>However, that causes us other problems. Now, it is harder to detect when the null hypothesis <em>should</em> be rejected. We need much stronger evidence to do so when <span class="math notranslate nohighlight">\(\alpha = 0.001\)</span> than when <span class="math notranslate nohighlight">\(\alpha = 0.05\)</span>. If we fail to reject the null hypothesis when it should be rejected, this is called a <strong>Type II error</strong> or a <strong>false negative</strong>. Type I and Type II errors are always a tradeoff with each other - where we reduce the chance of one, we increase the chance of the other. Also, it is never possible to completely eliminate the risk of either. This is an inherent limitation of Null Hypothesis Significance Testing.</p>
</section>
<section id="can-only-reject-the-null-not-support-it">
<h3>Can only reject the null, not support it<a class="headerlink" href="#can-only-reject-the-null-not-support-it" title="Permalink to this heading">#</a></h3>
<p>A second limitation of NHST is that, while it is possible to declare a sample estimate as too unlikely for us to think it comes from the null hypothesis, it is not possible to be sure an estimate definitely <em>does</em> come from the null hypothesis. To be concrete, what if a <span class="math notranslate nohighlight">\(b_1\)</span> estimate doesn’t fall in the tails of the null sampling distribution but instead falls in the middle part? Should we say that we’re confident that the <span class="math notranslate nohighlight">\(\beta_1 = 0\)</span> is the truth? In this framework, we can never <em>confirm</em> a specific null hypothesis, only reject it or fail to reject it. This is because there are infinite other population parameters slightly above or below 0 that could likely generate this estimate as well. You can’t prove that any estimate came from <span class="math notranslate nohighlight">\(\beta_1 = 0\)</span> specifically and not <span class="math notranslate nohighlight">\(\beta_1 = 0.1\)</span>. You can only say there is a very high likelihood that something did <em>not</em> come from a particular population like <span class="math notranslate nohighlight">\(\beta_1 = 0\)</span>. Null hypothesis testing is about rejecting some reality, not confirming one. If we fail to reject the null hypothesis, the true population parameter <em>could</em> be 0, but it could also be something else sort of close to 0. We don’t know.</p>
</section>
<section id="significance-is-a-binary-decision">
<h3>Significance is a binary decision<a class="headerlink" href="#significance-is-a-binary-decision" title="Permalink to this heading">#</a></h3>
<p>Thirdly, another limitation of NHST is that there are not different levels of statistical significance. We can compute different p-values for different <span class="math notranslate nohighlight">\(b_1\)</span> estimates and find that one has a higher probability to appear under the null hypothesis than another. But the <em>decision</em> of something being significantly different from 0 is a binary decision - it either is significant, or it is not. Something that is in the top 2% of the null sampling distribution is just as significant as something that is in the top 0.02% of the distribution, because both are beyond the alpha cutoff. Null hypothesis testing doesn’t give us a way to test whether different p-values are significantly different from <em>each other</em>. Thus a sample estimate is either significant, or it’s not. We decide it either didn’t come from the null hypothesis, or there’s not enough evidence to decide. You can’t have one estimate that is <em>more</em> or <em>less</em> significant than another, and you can’t have something that is <em>almost</em> significant.</p>
</section>
<section id="probability-of-data-not-population">
<h3>Probability of data, not population<a class="headerlink" href="#probability-of-data-not-population" title="Permalink to this heading">#</a></h3>
<p>Fourthly, the specific meaning of a p-value can be tricky to get right. It is the probability that, given the null hypothesis being true, we would find a sample value as extreme or more extreme than the one we did. Thinking in terms of conditional probability again, it is <span class="math notranslate nohighlight">\(P(b|\beta=0)\)</span>. A p-value does NOT mean the probability that the <em>null hypothesis</em> is true. In Frequentist statistics, a population parameter cannot have a probability - it exists as a single entity that can’t be repeated multiple times like a sample. The null hypothesis thus does not have a probability. <em>These data</em> have a probability, given the null hypothesis being true.</p>
</section>
<section id="p-values-and-sample-size">
<h3>P-values and sample size<a class="headerlink" href="#p-values-and-sample-size" title="Permalink to this heading">#</a></h3>
<p>Lastly, as we explored previously, the p-value we get is tied to our sample size. In small amounts of data, we would need to estimate a really large effect in order to call it significant, because the confidence interval is so wide that it covers many values including 0. Thus when we only have a small amount of data available to us, we might not be able to declare anything as significant. But in large amounts of data, almost everything is significant, even effects so tiny that they’re not useful in the real world. Would it matter to you to hear that a predictor in a model was significant, but the model only explained 0.01% of the variation in the outcome data? P-values can thus be “gamed” with large sample sizes. They tell us something about the confidence of this true parameter being 0 or not, but they don’t help us with determining whether this effect size is useful for real-world purposes.</p>
<p>In summary, there are five main hang-ups that cause a lot of consternation for people trying to use null hypothesis testing:</p>
<ul class="simple">
<li><ol class="arabic simple">
<li><p>The risk of Type I and Type II error is everpresent</p></li>
</ol>
</li>
<li><ol class="arabic simple" start="2">
<li><p>You can only reject the null hypothesis, or fail to reject the null hypothesis (not confirm the null hypothesis)</p></li>
</ol>
</li>
<li><ol class="arabic simple" start="3">
<li><p>You can only decide if a sample estimate is significantly different from 0 or not; there are no levels of significance</p></li>
</ol>
</li>
<li><ol class="arabic simple" start="4">
<li><p>A p-value is the probability of these data given the null hypothesis, NOT the probability of the null hypothesis given these data.</p></li>
</ol>
</li>
<li><ol class="arabic simple" start="5">
<li><p>P-values are tied to sample sizes, so it is possible to miss true effects in small datasets and to get a significant p-value with an impractically-tiny effect in large datasets.</p></li>
</ol>
</li>
</ul>
<p>This is a long section on the difficulties of null hypothesis testing because <a class="reference external" href="https://statisticsbyjim.com/hypothesis-testing/p-values-misinterpreted/">a lot of people use it incorrectly</a>, even professional scientists. NHST is a powerful approach but it’s tricky to interpret correctly, and it often gets used for the wrong questions that it is just not able to answer.</p>
<p>Thus if you default to only ever looking at p-values in your research and not the wider context of what your model looks like and what goals you have for it, you run the risk of misinterpreting your statistics. Just because a p-value is significant does not mean your model can be used in the way you want to, and just because a p-value is insignificant doesn’t mean your model is useless. We’ll cover the idea of <em>practical</em> vs. statistical significance in a later chapter but for now, remember that p-values aren’t the whole story! We also want to judge what sort of predictions our models are making, do we like that level of accuracy or not, and is there any way we can think of to make our model better.</p>
</section>
</section>
<section id="significant-effects-in-practice">
<h2>16.7 Significant effects in practice<a class="headerlink" href="#significant-effects-in-practice" title="Permalink to this heading">#</a></h2>
<section id="effects-in-the-simple-model">
<h3>Effects in the simple model<a class="headerlink" href="#effects-in-the-simple-model" title="Permalink to this heading">#</a></h3>
<p>Let’s practice using and interpreting p-values with some concrete research situations. One hypothesis we have used a lot is whether someone’s sex explains some of the variation in their thumb length. A research question like this implies that we are interested in the predictor “sex” specifically, and its unique contribution to explaining variation in thumb length. We recognize that there are likely many factors that lead to how long someone’s thumb is, and we may even know some more of them and be able to model them as well. But our interest right now is on the effect of specifically someone’s sex in the data generation process. Is it a meaningful contributor to explaining variation in thumb length on its own? Or is there not really an effect?</p>
<p>Using the framework of Null Hypothesis Statistical Testing, we can answer this sort of question. We just have to go through five steps in order to frame this question correctly for the NHST.</p>
<p>First, we need to pick the variables we’re using to test this question. We’ll use the outcome variable <code class="docutils literal notranslate"><span class="pre">Thumb</span></code> from the <code class="docutils literal notranslate"><span class="pre">studentdata</span></code> dataset, as we have been, and we’ll use <code class="docutils literal notranslate"><span class="pre">Sex</span></code> to predict it.</p>
<p>Second, we need to formulate the null hypothesis. We think that sex is interesting if it predicts some variation in thumb length - that there is a relationship between the two. That implies that there would be a non-zero coefficient for <code class="docutils literal notranslate"><span class="pre">Sex</span></code> in a linear model. It would not be an interesting variable if there was no change in predicted thumb length when sex varied - i.e., if the <span class="math notranslate nohighlight">\(b_1\)</span> coefficient were 0. Thus, we define the null hypothesis to be:</p>
<div class="math notranslate nohighlight">
\[H_0: \beta_1 = 0\]</div>
<p>In a world where the true population parameter <span class="math notranslate nohighlight">\(\beta_1\)</span> is 0, where there is no real effect of sex, we wouldn’t be so interested in it as a predictor. So we want to test if it’s likely that <span class="math notranslate nohighlight">\(b_1\)</span> in our sample came from a world where <span class="math notranslate nohighlight">\(\beta_1\)</span> is 0, or if we want to reject that explanation.</p>
<p>In step 3, we now need to fit a model in the data. We need to specify the equation of that model so we know what we’re testing:</p>
<div class="math notranslate nohighlight">
\[\hat{Y} = b_0 + b_1X_i\]</div>
<p>Where <span class="math notranslate nohighlight">\(X_i\)</span> is <code class="docutils literal notranslate"><span class="pre">Sex</span></code>. Knowing that equation, we automatically fit it with <code class="docutils literal notranslate"><span class="pre">lm()</span></code> as we have done many times before:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">model_obj</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">lm</span><span class="p">(</span><span class="n">Thumb</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">Sex</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">studentdata</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>From this model, we can extract our estimate of <span class="math notranslate nohighlight">\(b_1\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">model_obj</span><span class="o">$</span><span class="n">coefficients</span><span class="p">[</span><span class="m">2</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Fourthly, we determine the probability of getting a <span class="math notranslate nohighlight">\(b_1\)</span> like this, were the null hypothesis true - if <span class="math notranslate nohighlight">\(\beta_1\)</span> truly equals 0. Use <code class="docutils literal notranslate"><span class="pre">summary()</span></code> to find this p-value for the <span class="math notranslate nohighlight">\(b_1\)</span> coefficient:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">#Use summary() to see the p-value of the effect of Sex</span>
</pre></div>
</div>
</div>
</div>
<p>And as our final step, we look at that p-value and decide whether or not it is significant - whether or not we should reject the hypothetical world of <span class="math notranslate nohighlight">\(\beta_1 = 0\)</span>. As psychologists we are likely using an <span class="math notranslate nohighlight">\(\alpha\)</span> criterion of 0.05, so we will check if the p-value for <span class="math notranslate nohighlight">\(b_1\)</span> is &lt; 0.05.</p>
<p>According to this output, it is. Thus, we reject the null hypothesis that there is no effect of sex. Further, if we find the 95% confidence interval for <span class="math notranslate nohighlight">\(b_1\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">#Use confint() to find the 95% CI of the SexMale estimate</span>
</pre></div>
</div>
</div>
</div>
<p>We see that the interval includes only positive values. Thus we reject the null hypothesis that there is no effect of sex, AND we say we are 95% confident the true <span class="math notranslate nohighlight">\(\beta_1\)</span> is a positive number. As someone’s sex label switches from 0 to 1 (difference from female to male), the change in thumb length is likely to increase as well.</p>
<p>Congrats, you have tested a hypothesis and come up with a conclusion! You can now write up your study and send it for peer review! In APA style, we would write this result as:</p>
<p>“There is a significant effect of sex (<em>b</em> = 6.056, <em>p</em> &lt;0.001, 95% CI [3.037, 9.075]).”</p>
<p>By following these five steps, you were able to fit and evaluate a model of the data generation process for thumb length. Just remember that the conclusions we can make about this research question are limited, when we use null hypothesis testing. In the context of this analysis:</p>
<ul class="simple">
<li><p>We can’t be <em>sure</em> that <span class="math notranslate nohighlight">\(\beta_1\)</span> doesn’t equal 0 - we’re only very confident.</p></li>
<li><p>If our p-value had been &gt; 0.05, we couldn’t say that we’re sure <span class="math notranslate nohighlight">\(\beta_1\)</span> equals 0, because we can’t confirm the null hypothesis - only reject it or fail to reject it.</p></li>
<li><p>If our p-value had been 0.051, that would still be an insignificant effect. We set our <span class="math notranslate nohighlight">\(\alpha\)</span> decision criterion to be 0.05, and we need to stick to that.</p></li>
<li><p>Our p-value is the probability that we would get this <span class="math notranslate nohighlight">\(b_1\)</span> estimate if <span class="math notranslate nohighlight">\(\beta_1 = 0\)</span>; it is NOT the probability that <span class="math notranslate nohighlight">\(\beta_1 = 0\)</span> given this <span class="math notranslate nohighlight">\(b_1\)</span> estimate. Those statements are different conditional probabilities and thus different numbers (and a Frequentist would say you can’t get a probability of <span class="math notranslate nohighlight">\(\beta_1\)</span> anyways).</p></li>
<li><p>Our p-value is tied to our sample size N=157. If we had more or less data, we might have made a different decision.</p></li>
</ul>
</section>
<section id="effects-in-the-multivariable-model">
<h3>Effects in the multivariable model<a class="headerlink" href="#effects-in-the-multivariable-model" title="Permalink to this heading">#</a></h3>
<p>As we saw in chapter 13, predictors in a multivariable model can be correlated. Thus if we’re interested specifically in the effect of sex, we may want to <em>control for</em> what might be a better explanatory variable, like height. This would enable us to investigate if sex is a significant, unique effect in its own right or if it’s only related to thumb length by virtue of being related to height.</p>
<p>To do this, first we pick our variables. We’ll use <code class="docutils literal notranslate"><span class="pre">Thumb</span></code> and <code class="docutils literal notranslate"><span class="pre">Sex</span></code> as before, as well as <code class="docutils literal notranslate"><span class="pre">Height</span></code> as another predictor.</p>
<p>Second, specify the null hypothesis. Here we’re still interested in the effect of sex in particular. We’re just using height as a control. So the null hypothesis is still:</p>
<div class="math notranslate nohighlight">
\[H_0: \beta_1 = 0\]</div>
<p>Third, specify and fit the model:</p>
<div class="math notranslate nohighlight">
\[\hat{Y} = b_0 + b_1X_{1i} + b_2X_{2i}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">model_obj</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">lm</span><span class="p">(</span><span class="n">Thumb</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">Sex</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">Height</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">studentdata</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Fourth, find the p-value of <span class="math notranslate nohighlight">\(b_1\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">summary</span><span class="p">(</span><span class="n">model_obj</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>And finally, make a decision about whether or not sex is significant.</p>
<p>Because sex and height are related, there is shared variation between them - both of them explain some of the same variation in thumb length. We need to put them both in a model together in order to disentangle what amount of <em>unique</em> variation they each explain. By doing this, we see that the effect of sex got smaller, after taking height into account - the coefficient fell from 6.056 to 2.998. This effect is so small, that it is no longer significant - the p-value is 0.114, which is above our <span class="math notranslate nohighlight">\(\alpha\)</span> cutoff of 0.05. We’d interpret this to mean that in a world where there is <em>no</em> unique effect of sex (<span class="math notranslate nohighlight">\(\beta_1 = 0\)</span>), a <span class="math notranslate nohighlight">\(b_1\)</span> estimate of 2.998 would be a likely outcome. We can further see this by checking the 95% confidence interval:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">confint</span><span class="p">(</span><span class="n">model_obj</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;SexMale&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">level</span><span class="o">=</span><span class="m">0.95</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>This range includes 0 as one of the <span class="math notranslate nohighlight">\(\beta_1\)</span> values that are likely to produce <span class="math notranslate nohighlight">\(b_1 = 2.998\)</span>. Thus, with the data we have, we fail to reject the null hypothesis that there is no unique effect of sex. We’d report this result as:</p>
<p>“The effect of sex was insignificant when controlling for height (<em>b</em> = 2.998, <em>p</em> = 0.114, 95% CI [-0.728, 6.725]).”</p>
<p>Note again that this does not mean <span class="math notranslate nohighlight">\(\beta_1\)</span> is definitely 0. There are still many non-zero values in the confidence interval that could be the truth. It could be that we only failed to find a significant effect because <code class="docutils literal notranslate"><span class="pre">studentdata</span></code> only had a sample size of N=157. If we ran this model in a much larger dataset, we could create a narrower CI that might not overlap with 0. It’s easier to distinguish small effects from <span class="math notranslate nohighlight">\(\beta_1 = 0\)</span> with more data. But in that case, it’s always worthwhile to ask yourself if an effect that is significant, but with a very small coefficient, is worth anything to you in practical terms. Statistically significant doesn’t mean <em>important</em>, only unlikely to be 0.</p>
</section>
<section id="effects-in-interaction-models">
<h3>Effects in interaction models<a class="headerlink" href="#effects-in-interaction-models" title="Permalink to this heading">#</a></h3>
<p>When we build interaction models, we are often most interested in the interaction term. We want to know if the effect of one variable depends on values of the other (a non-zero interaction coefficient), or if the effects of each variable operate on the outcome variable independently of each other (a zero interaction coefficient). Thus, when we have hypotheses about interactions, we usually assess the significance of the interaction term.</p>
<p>Let’s test the hypothesis that there is an interaction between sex and height on thumb length. Our variables will be the same as in the multivariable case, <code class="docutils literal notranslate"><span class="pre">Thumb</span></code>, <code class="docutils literal notranslate"><span class="pre">Sex</span></code>, and <code class="docutils literal notranslate"><span class="pre">Height</span></code>.</p>
<p>Our null hypothesis is now for the interaction effect, and not the main effect of sex:</p>
<div class="math notranslate nohighlight">
\[H_0: \beta_3 = 0\]</div>
<p>Specifying and fitting our model, we get:</p>
<div class="math notranslate nohighlight">
\[\hat{Y} = b_0 + b_1X_{1i} + b_2X_{2i} + b_3X_{1i}X_{2i}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">model_obj</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">lm</span><span class="p">(</span><span class="n">Thumb</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">Sex</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">Height</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">Sex</span><span class="o">*</span><span class="n">Height</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">studentdata</span><span class="p">)</span>

<span class="nf">summary</span><span class="p">(</span><span class="n">model_obj</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Our hypothesis is about the interaction effect <span class="math notranslate nohighlight">\(b_3\)</span>, so investigate the estimate and p-value on the line for <code class="docutils literal notranslate"><span class="pre">SexMale:Height</span></code>. We see that the p-value of the interaction is p = 0.6145, which is not below the <span class="math notranslate nohighlight">\(\alpha\)</span> criterion of 0.05. Thus we fail to reject the null hypothesis that there is no interaction. We can’t be <em>sure</em> that there is no interaction truly in the population, but we are pretty confident that the true interaction effect is something close to zero:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">confint</span><span class="p">(</span><span class="n">model_obj</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;SexMale:Height&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">level</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.95</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Without a significant interaction, we would interpret the relationship between sex, height, and thumb length just as main effects.</p>
<p>Further, by including more predictors in the model, we’ve soaked up more degrees of freedom. That means the standard error for the effect of Height (which is calculated based on degrees of freedom) is now larger (and in fact not significant). It doesn’t matter that we previously fit a model where Height was significant - this was the model we chose to test, and this one says Height does not have a significant main effect. We can’t go fishing around for whichever model makes our hypothesis look better.</p>
<p>We’d report this as:</p>
<p>“There was no significant interaction between the effects of sex and height on thumb length (<em>b</em> = 0.258, <em>p</em> = 0.615, 95% CI [-0.753, 1.270]). Additionally, there was no main effect of Height (<em>b</em> = 0.556, <em>p</em> = 0.0598, 95% CI [-0.023, 1.13]) or Sex (<em>b</em> = -14.526, <em>p</em> = 0.677, 95% CI [-83.231, 54.179].”</p>
</section>
</section>
<section id="frequentist-vs-bayesian-hypothesis-testing">
<h2>16.8 Frequentist vs. Bayesian hypothesis testing<a class="headerlink" href="#frequentist-vs-bayesian-hypothesis-testing" title="Permalink to this heading">#</a></h2>
<p>The ideas about hypothesis testing that we covered in this and last chapter come from the perspective of what’s called Frequentist statistics. Almost every textbook for undergraduate psychology students starts with this framework, as the Frequentist view of statistics dominated the academic field of statistics for most of the 20th century and is a popular collection of tools among applied scientists. It was and is the most common practice among psychologists to use Frequentist methods. Because Frequentist methods are ubiquitous in scientific papers, every student of statistics needs to understand those methods.</p>
<p>However, Frequentist statistics can be frustrating to interpret. The major tenet of Frequentism is that there is only one state of the world. A hypothesis you are testing is either true, or it is not. We can describe the probability of generating certain datasets given the true state of the world, but if we don’t know that state, we can’t describe the probability of what that state is given a particular dataset we have collected. Unfortunately, that’s usually the case we find ourselves in - we <em>don’t</em> know the true state of the world, and our only hope to figure it out is by looking at some data. Frequentism thus isn’t an ideal method for answering that question, and we have to do some brain bending things to make it work for us anyways:</p>
<ul class="simple">
<li><p>We can’t figure out the probability of the true population parameter, so we have to think up some hypothetical parameters and see how likely those are to produce our data (p-values)</p></li>
<li><p>We then have to reduce these probabilities to a binary yes/no decision about whether we want to accept this hypothetical parameter as being the truth (null hypothesis testing)</p></li>
<li><p>We can only ever reject a null hypothesis, but we can never build support for it</p></li>
</ul>
<p>If these topics have felt abstract and difficult to master, you wouldn’t be alone. The convoluted nature of statistical inference with Frequentist methods partially explains errors that can be found in published research.</p>
<p>This is why some people prefer a different approach to inferential statistics, called <strong>Bayesian statistics</strong>. These ideas as a collection are named after Thomas Bayes, the mathematician who contributed much to our understanding of probabilities. While we won’t go in depth into this approach, it’s good to know that there are other ways of thinking about statistical evidence.</p>
<p>Bayesian statistics have a fundamentally different view of probability than Frequentist statistics do. In Frequentism, probability means the long-run proportion across many samples. If the true population parameter <span class="math notranslate nohighlight">\(\beta\)</span> is 0, we can’t measure it directly, but we can estimate it many times across many samples. This builds a sampling distribution, composed of many b estimates.</p>
<img src="images/ch15-betadist0.png" width="400">
<p>Because we sample these b estimates over and over and get different values each time, the b estimates can have probability. There is some proportion of these estimates, across many samples, that will equal a particular value.</p>
<p>To a Frequentist, the population parameter <span class="math notranslate nohighlight">\(\beta\)</span> <em>cannot</em> have a probability. It is inherently just one value, and it isn’t generated by some underlying process. It just is. A population parameter is either equal to a particular value, or it is not. There’s no way to get many values of it for measuring a proportion.</p>
<p>In contrast, in the Bayesian perspective, probability means something different. To a Bayesian, probability is your strength of belief about the population parameter. There is still one population parameter that we don’t know, but until we know it there are many possible values is <em>could</em> be.</p>
<img src="images/ch21-beliefdist.png" width="600">
<p>We can believe more strongly in some possible values than others - one value is more likely to be the truth than another value. In this sense, a population parameter can have a probability. Do we think a <span class="math notranslate nohighlight">\(\beta\)</span> value of 0 is more likely to be the truth than a <span class="math notranslate nohighlight">\(\beta\)</span> value of 10? What value do we believe in most strongly?</p>
<p>While Bayesian statistics is not practiced as widely as Frequentist statistics, it is gaining ground among psychologists and can be used for many of the same use cases as Frequentist statistics. With this alternate approach to defining probability comes a whole suite of methods for analyzing data. If you’re interested in learning about such an approach, you can read through a secret optional chapter of this textbook <a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/adv-bayesian.ipynb">here</a>.</p>
</section>
<section id="chapter-summary">
<h2>Chapter summary<a class="headerlink" href="#chapter-summary" title="Permalink to this heading">#</a></h2>
<p>After reading this chapter, you should be able to:</p>
<ul class="simple">
<li><p>Define the null hypothesis</p></li>
<li><p>Explain what it means to reject the null hypothesis</p></li>
<li><p>Shuffle data to create null sampling distributions</p></li>
<li><p>Explain the alpha criterion</p></li>
<li><p>Define the meaning of a p-value</p></li>
<li><p>Find the p-value of a model estimate</p></li>
<li><p>Explain the limits of null hypothesis testing</p></li>
</ul>
</section>
<section id="new-concepts">
<h2>New concepts<a class="headerlink" href="#new-concepts" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Null Hypothesis Significance Testing (NHST)</strong> - A statistical method that tests whether a sample of data is likely under a specific hypothesis of the population parameter being 0.</p></li>
<li><p><strong>null hypothesis</strong> - The specific hypothesis being tested in NHST. Often, it is that the true population parameter = 0.</p></li>
<li><p><strong>alternative hypothesis</strong> - In NHST, the hypothesis that would be true if the null hypothesis is false. Often, it is that the true population parameter is something other than 0.</p></li>
<li><p><strong>statistical significance</strong> - A judgment that a sample estimate is unlikely to have happened if the null hypothesis was true.</p></li>
<li><p><strong>null sampling distribution</strong> - The distribution of sample estimates that could be created from a population where the null hypothesis is true.</p></li>
<li><p><strong>permutation testing</strong> - A type of simulation that randomly shuffles values in a variable in order to remove any association with another variable. Repeating this process approximates the null sampling distribution of an estimate.</p></li>
<li><p><strong>p-value</strong> - The probability that a particular estimate or something more extreme would happen if the null hypothesis were true.</p></li>
<li><p><strong>t-distribution</strong> - The theoretical shape of the null sampling distribution of a model coefficient estimate.</p></li>
<li><p><strong>Type I error</strong> - One commits a Type I error or false positive decision if they reject the null hypothesis when it is in fact the truth. Defined by the significance decision criterion <span class="math notranslate nohighlight">\(\alpha\)</span>.</p></li>
<li><p><strong>Type II error</strong> - One commits a Type II error or false negative decision if they fail to reject the null hypothesis when it is not the truth.</p></li>
</ul>
<p><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-17.ipynb">Next: Chapter 17 - Significance Testing Whole Models</a></p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "smburns47/Psyc158",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "r"
        },
        kernelOptions: {
            name: "ir",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'ir'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-null-hypothesis">16.1 The null hypothesis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sampling-distribution-of-an-estimate">16.2 Sampling distribution of an estimate</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#significance-testing">16.3 Significance testing</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-p-value">16.4 The p-value</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-t-distribution">16.5 The t-distribution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#things-that-affect-the-p-value">16.5 Things that affect the p-value</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-limits-of-significance-testing">16.6 The limits of significance testing</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#type-i-and-type-ii-error">Type I and Type II error</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#can-only-reject-the-null-not-support-it">Can only reject the null, not support it</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#significance-is-a-binary-decision">Significance is a binary decision</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-of-data-not-population">Probability of data, not population</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#p-values-and-sample-size">P-values and sample size</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#significant-effects-in-practice">16.7 Significant effects in practice</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#effects-in-the-simple-model">Effects in the simple model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#effects-in-the-multivariable-model">Effects in the multivariable model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#effects-in-interaction-models">Effects in interaction models</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#frequentist-vs-bayesian-hypothesis-testing">16.8 Frequentist vs. Bayesian hypothesis testing</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#chapter-summary">Chapter summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#new-concepts">New concepts</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Shannon Burns
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>