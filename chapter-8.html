
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Chapter 6 - Sampling data &#8212; Pomona Psych 158 Online Textbook</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Pomona Psych 158 Online Textbook</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Pomona College Psych 158 Online Textbook
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Unit 1 Describing Data
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="chapter-1.html">
   Chapter 1 - Intro to Doing Statistics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter-2.html">
   Chapter 2 - Statistical Reasoning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter-3.html">
   Chapter 3 - What are Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter-4.html">
   Chapter 4 - Organizing Data
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter-5.html">
   Chapter 5 - Describing Data
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Unit 2 - Modeling Data
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="chapter-6.html">
   Chapter 6 - Variation in Multiple Variables
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Unit 3 - Evaluating Models
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="chapter-7.html">
   Chapter 7 - Principles of Data Visualization
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/smburns47/Psyc158/main?urlpath=tree/chapter-8.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
      <li>
        
<button onclick="initThebeSBT()"
  class="headerbtn headerbtn-launch-thebe"
  data-toggle="tooltip"
data-placement="left"
title="Launch Thebe"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="headerbtn__text-container">Live Code</span>
</button>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/smburns47/Psyc158"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/smburns47/Psyc158/issues/new?title=Issue%20on%20page%20%2Fchapter-8.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/chapter-8.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#from-populations-to-samples">
   6.1 From populations to samples
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-sampling-process">
   6.2 The sampling process
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sampling-error">
   6.3 Sampling error
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sampling-distributions">
   6.4 Sampling distributions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-importance-of-sample-size">
   6.5 The importance of sample size
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-central-limit-theorem">
   6.6 The Central Limit Theorem
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#estimating-population-parameters">
   6.7 Estimating population parameters
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#estimating-the-population-mean">
     Estimating the population mean
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#estimating-the-population-standard-deviation">
     Estimating the population standard deviation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#confidence-intervals">
   6.8 Confidence intervals
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pitfall-of-interpreting-confidence-intervals">
     Pitfall of interpreting confidence intervals
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#chapter-summary">
   Chapter summary
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#chapter-quiz">
   Chapter quiz
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Chapter 6 - Sampling data</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#from-populations-to-samples">
   6.1 From populations to samples
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-sampling-process">
   6.2 The sampling process
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sampling-error">
   6.3 Sampling error
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sampling-distributions">
   6.4 Sampling distributions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-importance-of-sample-size">
   6.5 The importance of sample size
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-central-limit-theorem">
   6.6 The Central Limit Theorem
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#estimating-population-parameters">
   6.7 Estimating population parameters
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#estimating-the-population-mean">
     Estimating the population mean
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#estimating-the-population-standard-deviation">
     Estimating the population standard deviation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#confidence-intervals">
   6.8 Confidence intervals
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pitfall-of-interpreting-confidence-intervals">
     Pitfall of interpreting confidence intervals
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#chapter-summary">
   Chapter summary
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#chapter-quiz">
   Chapter quiz
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <ul class="simple">
<li><p>the data generation process</p></li>
<li><p>equation representation of a model (4.6)</p></li>
<li><p>sources of variation (4.7)</p></li>
<li><p>randomness (4.8)</p></li>
<li><p>probability vs. statistics</p></li>
<li><p>making a probability model - knowing the population (law of large numbers)</p></li>
<li><p>basic rules of probability</p></li>
<li><p>probability distributions</p></li>
<li><p>conditional probability</p></li>
<li><p>when you dont know the population, how to try and estimate the DGP: sampling. this is what leads to statistics</p></li>
</ul>
<section class="tex2jax_ignore mathjax_ignore" id="chapter-6-sampling-data">
<h1>Chapter 6 - Sampling data<a class="headerlink" href="#chapter-6-sampling-data" title="Permalink to this headline">#</a></h1>
<section id="from-populations-to-samples">
<h2>6.1 From populations to samples<a class="headerlink" href="#from-populations-to-samples" title="Permalink to this headline">#</a></h2>
<p>In the previous chapter we discussed how knowing a probability model allows us to predict outcomes of future events. Even in cases where we don’t know the data generation process, we can still create an accurate probability model if we know the entire <strong>population</strong> of outcomes. For example, if we wanted to know the probability that someone in the US had COVID-19 right now, and if we were able to test every individual in the country right this moment, we’d be able to count how many people were covid-positive out of the entire group and calulate probability that way. The population is the entirety of the group we want to study, so if we have data from that entire group, we can calculate summaries and probabilities really easily.</p>
<p>However, it is almost always impractical, or straight up impossible, to collect data from the entire population you care about. If a psychologist cares about a general question like “the way humans think,” there’s no way they can test every human on the planet in order to generate an accurate distribution of the population data.</p>
<p>Instead, one of the foundational ideas in statistics is that we can make guesses about the features of an entire population based on a smaller <strong>sample</strong> of individuals from that population. In this chapter we will introduce the concept of statistical sampling and discuss why it works.</p>
</section>
<section id="the-sampling-process">
<h2>6.2 The sampling process<a class="headerlink" href="#the-sampling-process" title="Permalink to this headline">#</a></h2>
<p>Anyone living in the United States will be familiar with the concept of sampling from the political polls that have become a central part of our electoral process. In some cases, these polls can be incredibly accurate at predicting the outcomes of elections. The best known example comes from the 2008 and 2012 US Presidential elections, when the pollster Nate Silver correctly predicted electoral outcomes for 49/50 states in 2008 and for all 50 states in 2012. Silver did this by combining data from 21 different polls, each of which included data from about 1000 likely voters. If we consider the population of interest during an election to be all the people who cast a vote, then in this case the intentions of that 1000-person sample very closely matched the overall voting behavior of the entire electorate. Yet, the 2016 US Presidential election was noteworthy for how <em>badly</em> polls predicted the outcome (Clinton was strongly favored to win by many polling samples, but the electorate behaved differently). This illustrates the extreme importance of picking a sample that is <strong>representative</strong> of your population of interest.</p>
<p>Since you don’t know the population distribution (which is the whole reason we’re sampling data in the first place), it can be hard to know how representative a sample is. Over time, statisticians have figured out that the best way to pick a sample that most closely resembles the shape, center, and variability of the target population is to draw a <strong>random sample,</strong> where every member of the population has an equal chance of getting picked for the sample. This is also usually impractical, though. For a psychologist, it’s not as easy to contact, recruit, or even know about some people in the population compared to others (indeed, the data in psychology results are often based on an over-representation of psychology students, since universities often require their psych majors to participate in studies!). For an ecologist, it is much easier to access animals that live in one site nearby, versus the same species that lives in a remote canyon. You can probably think of examples in other fields of study that make it hard to sample every member of the population at equal likelihood.</p>
<p>Instead, there are a variety of more practical sampling methods that are easier to do, and which one <em>hopes</em> is mostly similar to what a random sample looks like. The most common type is that described above, where the sample is comprised of datapoints that were the easiest to obtain. This is called <strong>convenience sampling.</strong> The samples are chosen in a way that is convenient to the researcher. In real life, most studies are convenience samples of one form or another. This sometimes does a good job of creating a random sample, but can instead create a <strong>biased sample</strong> if there’s some reason for why the sample is easier to collect that also affects the variables being studied. E.g., can you think of any way psychology students might differ from students of other majors? There are other types too that are less common that you may learn about in other classes (.e.g, stratified sampling, snowball sampling, etc.). Whatever sampling method you use, the most important thing to keep in mind is how likely it is to create a representative sample, and what factors to watch out for that might contribute to bias.</p>
<p>In addition, it is important to know the difference between <strong>sampling with replacement</strong> and <strong>sampling without replacement.</strong> With replacement means that, after picking a data point out of the entire population for inclusion in the sample, you record its value and then put it back “into the bag” of the population, so that it has a chance of being selected again. Without replacement is the opposite - once a data point is recorded, it can’t be selected again. In real world data collection, you almost always use sampling without replacement because you want to collect as much unique information in your data as possible. I.e., once someone has participated in your study, you don’t let them participate again. However, for some theoretical procedures that we’ll talk about in a moment, sampling with replacement is an underlying assumption.</p>
</section>
<section id="sampling-error">
<h2>6.3 Sampling error<a class="headerlink" href="#sampling-error" title="Permalink to this headline">#</a></h2>
<p>A population can be described by summaries of its distribution: center, shape, spread. These are called <strong>population parameters</strong>. If we take a sample of data from this population, it will also create a distribution which we can calculate summaries of as well. These are called the <strong>sample statistics</strong>.</p>
<p>Even with a random sample, the sample statistics won’t always match the parameters of the population from which it came. In a survey of student height on campus, you can imagine the possibility (albeit small) of a random sample that ended up being comprised of the entire basketball team. In addition, if you were to take several different samples, the sample statistics for each one won’t always be the same. For example, let’s say we know the population mean of height for all Claremont College students is 168.35cm, and the standard deviation is 10.16cm. Then, a researcher comes along and picks 5 different samples of 50 students each, creating the following sample statistics:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:center head"><p>Sample number</p></th>
<th class="text-align:center head"><p>Height mean (cm)</p></th>
<th class="text-align:center head"><p>Height SD (cm)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:center"><p>1</p></td>
<td class="text-align:center"><p>167</p></td>
<td class="text-align:center"><p>9.1</p></td>
</tr>
<tr class="row-odd"><td class="text-align:center"><p>2</p></td>
<td class="text-align:center"><p>171</p></td>
<td class="text-align:center"><p>8.3</p></td>
</tr>
<tr class="row-even"><td class="text-align:center"><p>3</p></td>
<td class="text-align:center"><p>170</p></td>
<td class="text-align:center"><p>10.6</p></td>
</tr>
<tr class="row-odd"><td class="text-align:center"><p>4</p></td>
<td class="text-align:center"><p>168</p></td>
<td class="text-align:center"><p>9.5</p></td>
</tr>
<tr class="row-even"><td class="text-align:center"><p>5</p></td>
<td class="text-align:center"><p>166</p></td>
<td class="text-align:center"><p>9.5</p></td>
</tr>
</tbody>
</table>
<p>The sample mean and standard deviation are similar but not exactly equal to the population values. This is called <strong>sampling error.</strong></p>
</section>
<section id="sampling-distributions">
<h2>6.4 Sampling distributions<a class="headerlink" href="#sampling-distributions" title="Permalink to this headline">#</a></h2>
<p>Now let’s take a large number of samples (say, 5,000) of 50 individuals and compute the mean for each of them. We will sample with replacement, so that we don’t run out of datapoints to do this with. The result is a set of 5,000 sample statistics - kinda sounds like a dataset, doesn’t it? In fact, we can make a distribution of sample <em>statistics</em>, rather than only raw observations. In this case, the statistic (e.g. the mean of each separate sample) is each unique entry in the distribution. This is a <strong>sampling distribution</strong> - a distribution of sampling statistics. Specifically, this is a sampling distribution of the mean (since that’s the statistics we’re using).</p>
<img src="ch7-sampdist.png"  width="750">
<p>The gray histogram above shows each raw observation for height in the dataset, and the blue histogram shows the mean values for each sample. The sample means vary somewhat, but something to notice is that overall they are centered around the population mean. The average of the 5,000 sample means (168.3463) is very close to the true population mean (168.3497).</p>
<p>Technically you can make a sampling distribution for any statistic you can think of - the mode, the maximum, even statistics we’ll learn about later like a t-stat or F-value. The mean is the most common to talk about and care about, but it’s not the only basis for a sampling distribution.</p>
</section>
<section id="the-importance-of-sample-size">
<h2>6.5 The importance of sample size<a class="headerlink" href="#the-importance-of-sample-size" title="Permalink to this headline">#</a></h2>
<p>You probably have some intuition that statistical inferences are better the bigger your sample size. In the popular conscious, it seems like “sample size” ranks right under “correlation is not causation” in terms of critiques about psychology research. Sample size is indeed an important concern, and in this section you’ll learn why.</p>
<p>In the example above, each of our samples was 50 people. For another, let’s see what happens when our sample size is as small as possible. Let’s say among a population of people with IQ scores, we will pick a sample that contains only one person’s data. Calculating the mean of this sample is just the raw value of the individual score we picked out. Doing this for several samples thus reveals a sampling distribution that approaches the same shape as the population distribution (since each “sample” is just one data point from the population distribution; panel A below). However, when we raise the sample size to 2 (panel B), the mean of any one sample tends to be closer to the population mean than a single person’s IQ score, and so the histogram (i.e., the sampling distribution) is a bit narrower than the population distribution. When we raise the sample size to 10 (panel C), we can see that the distribution of sample means tend to be fairly tightly clustered around the true population mean.</p>
<img src="ch7-sampsize.png"  width="750">
<p>Based on this demonstration, we can see that if you only have a few observations in a sample, any one sample mean is likely to be quite inaccurate: if you replicate a small experiment and recalculate the mean you’ll get a very different answer. In other words, the sampling distribution is quite wide. This is why statistical answers from studies with small samples can be quite misleading.</p>
<p>In contrast, if you replicate a large experiment and recalculate the sample mean, you’ll probably get nearly the same answer you got last time, so the sampling distribution will be very narrow. We can quantify this variation in the sampling distribution by calculating the standard deviation of the sampling distribution, which is a special version of the standard deviation referred to as the <strong>standard error.</strong> The standard error of a statistic is often denoted SE, and since we’re usually interested in the standard error of the sample mean, we often use the acronym SEM. As you can see just by looking at the above picture, as the sample size N increases, the SEM decreases.</p>
</section>
<section id="the-central-limit-theorem">
<h2>6.6 The Central Limit Theorem<a class="headerlink" href="#the-central-limit-theorem" title="Permalink to this headline">#</a></h2>
<p>Despite how poorly a single small sample can do on telling you about a population parameter, it’s important to remember that several samples together, even if very small, will stack up into a distribution that is centered on the true population parameter.</p>
<p>Another interesting observation is that the sampling distribution in the above picture was always normally distributed. Is this because IQ is the kind of data that has a normal distribution in the population? Actually, a remarkable thing is that no matter what shape your underlying population distribution is, as you grab more samples and plot their means together, the sampling distribution of the mean starts to look more like a normal distribution. To give you a sense of this, check out the figure below.</p>
<img src="ch7-clt.png"  width="650">
<p>In panel A is a weird distribution where the maximum value is the most numerous. Comparing it to the black line, which shows what a perfectly normal shape would be, you can see the population distribution is far from normal. Panels B-D show the sampling distributions of samples taken from the population distribution in A with various sample sizes (2, 4, and 8 respectively. As you can see, even though the original population distribution is non-normal, the sampling distribution of the mean becomes pretty close to normal by the time you have a sample of even 4 observations.</p>
<p>On the basis of these figures, it seems like we have evidence for all of the following claims about the
sampling distribution of the mean:</p>
<ul class="simple">
<li><p>The mean of the sampling distribution is the same as the mean of the population</p></li>
<li><p>The standard deviation of the sampling distribution (i.e., the standard error) gets smaller as the sample size increases</p></li>
<li><p>The shape of the sampling distribution becomes normal as the sample size increases</p></li>
</ul>
<p>As it happens, not only are all of these statements true, there is a very famous theorem in statistics that proves all three of them, known as the <strong>Central Limit Theorem.</strong> Among other things, the Central Limit Theorem tells us that if the population distribution has mean µ and standard deviation σ, then the sampling distribution of the mean also has mean µ, and the standard error of the mean is</p>
<div class="math notranslate nohighlight">
\[SEM = \frac{σ}{\sqrt{N}}\]</div>
<p>where N is the size of a sample. This says that, when sampling from a population with standard deviation σ, the means of all samples of size N will vary around the true mean by the SEM amount. Further, because we divide the population standard devation σ by the square root of the sample size N, the SEM gets smaller as the sample size increases. It also tells us that the shape of the sampling distribution becomes normal.</p>
<p>This result is useful for all sorts of things. It tells us why large experiments are more reliable than small ones, and because it gives us an explicit formula for the standard error it tells us how much more reliable a large experiment is. It tells us why the normal distribution is, well, normal. In real experiments, many of the things that we want to measure are actually averages of lots of different quantities (e.g., arguably, “general” intelligence as measured by IQ is an average of a large number of “specific” skills and abilities), and when that happens, the averaged quantity should follow a normal distribution. Because of this mathematical law, the normal distribution pops up over and over again in real data.</p>
</section>
<section id="estimating-population-parameters">
<h2>6.7 Estimating population parameters<a class="headerlink" href="#estimating-population-parameters" title="Permalink to this headline">#</a></h2>
<p>In all the examples in the previous sections, we knew the population parameters ahead of time. This is helpful for learning about statistics, but of course the most interesting things to do research on are the things we don’t already know about; that which we don’t know the population distribution for.</p>
<p>For instance, suppose you wanted to measure the effect of low level lead poisoning on cognitive functioning. Perhaps you decide that you want to compare typical IQ scores, with a mean of 100 and SD of 15, to people in Oxnard living near the <a class="reference external" href="https://archive.epa.gov/region9/socal/web/html/index-7.html#halaco">Halaco Superfund site</a>, where metallic waste from a smelting operation leaked into the groundwater between 1965 and 2004. If you believe that lead poisoning affects cognitive functioning, then it would not be reasonable to assume that the population distribution of IQ in Oxnard will be the same as the population distribution of IQ in a city without lead pollution. We’re going to have to estimate the population parameters from a sample of data. So how do we do this?</p>
<section id="estimating-the-population-mean">
<h3>Estimating the population mean<a class="headerlink" href="#estimating-the-population-mean" title="Permalink to this headline">#</a></h3>
<p>Suppose we go to Oxnard and 100 of the locals are kind enough to sit through an IQ test. The average IQ score among these people turns out to be X¯=98.5. So what is the true mean IQ for the entire population of Oxnard? Obviously, we don’t know the answer to that question. It could be 97.2, but if could also be 103.5. We only have one sample, so we cannot give a definitive answer. Nevertheless, right now our “best guess” is 98.5. That’s the essence of statistical
estimation: giving a best guess.</p>
<p>In this example, estimating the unknown poulation parameter is straightforward. We calculate the sample mean, and we use that as an estimate of the population mean. However, it’s very important to remember that the <em>sample statistic</em> and the <em>estimate of the population parameter</em> are conceptually different things. A sample statistic is a description of your data, whereas the estimate is a guess about the population. With that in mind, statisticians often different notation to refer to them. For instance, if true population mean is denoted µ (pronouned “mew”), then we would use ˆµ  (“mew hat”) to refer to our estimate of the population mean. In contrast, the sample mean is denoted X¯ (“X bar”) or sometimes m. To help keep the notation clear, here’s a handy table:</p>
<p><strong>Notation for sample mean, population mean, and estimate of the population mean</strong></p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:center head"><p>Symbol</p></th>
<th class="text-align:center head"><p>What is it?</p></th>
<th class="text-align:center head"><p>Do we know it?</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:center"><p>X¯</p></td>
<td class="text-align:center"><p>Sample mean</p></td>
<td class="text-align:center"><p>Yes, calculated from the raw data</p></td>
</tr>
<tr class="row-odd"><td class="text-align:center"><p>µ</p></td>
<td class="text-align:center"><p>True population mean</p></td>
<td class="text-align:center"><p>Almost never known for sure</p></td>
</tr>
<tr class="row-even"><td class="text-align:center"><p>µˆ</p></td>
<td class="text-align:center"><p>Estimate of the population mean</p></td>
<td class="text-align:center"><p>Yes, identical to the sample mean</p></td>
</tr>
</tbody>
</table>
</section>
<section id="estimating-the-population-standard-deviation">
<h3>Estimating the population standard deviation<a class="headerlink" href="#estimating-the-population-standard-deviation" title="Permalink to this headline">#</a></h3>
<p>So far, estimation seems pretty simple, and you might be wondering why we forced you to read through all that stuff about sampling theory. In the case of the mean, our estimate of the population parameter (i.e. ˆµ) turned out to identical to the corresponding sample statistic (i.e. X¯). However, that’s not always true. To see this, let’s think about how to construct an estimate of the population standard deviation, which we’ll denote ˆσ. What shall we use as our estimate in this case?</p>
<p><strong>Notation for sample standard deviation, population standard deviation, and estimate of the population standard deviation</strong></p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:center head"><p>Symbol</p></th>
<th class="text-align:center head"><p>What is it?</p></th>
<th class="text-align:center head"><p>Do we know it?</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:center"><p>s</p></td>
<td class="text-align:center"><p>Sample sd</p></td>
<td class="text-align:center"><p>Yes, calculated from the raw data</p></td>
</tr>
<tr class="row-odd"><td class="text-align:center"><p>σ</p></td>
<td class="text-align:center"><p>True population sd</p></td>
<td class="text-align:center"><p>Almost never known for sure</p></td>
</tr>
<tr class="row-even"><td class="text-align:center"><p>σˆ</p></td>
<td class="text-align:center"><p>Estimate of the population sd</p></td>
<td class="text-align:center"><p>Yes, identical to the sample mean</p></td>
</tr>
</tbody>
</table>
<p>Your first thought might be that we could do the same thing we did when estimating the mean, and just use the sample
statistic as our estimate. That’s almost the right thing to do, but not quite. Here’s why. Suppose we have a sample that contains a single IQ observation of 98. This is a perfectly legitimate sample, even if it does have a sample size of N=1. It has a sample mean of 98, and because every observation in this sample is equal to the sample mean (obviously!) it has a sample standard deviation of 0: the sample contains a single observation and therefore there is no variation observed within the sample. But as an estimate of the population standard deviation, this may feel completely wrong. Knowing that data implies variability, the only reason that we don’t see any variability in the sample is that the sample is too small to display any variation, not because everyone has an IQ of 98. So, if you have a sample size of N=1, it feels like the right answer about the population standard deviation is just to say “no idea at all”.</p>
<p>Suppose we now make a second observation. The dataset now has N=2 observations, and the complete sample now contains the observations 98 and 100. This time around, our sample is just large enough for us to be able to observe some variability: two observations is the bare minimum number needed for any variability to be observed. For our new dataset, the sample mean is X¯=99, and the sample standard deviation is s=1. What intuitions do we have about the population? Again, as far as the population mean goes, the best guess we can possibly make is the sample mean: if forced to guess, we’d probably guess that the population mean cromulence is 21. What about the standard deviation? This is a little more complicated. The sample standard deviation is only based on two observations, so you may feel that we haven’t given the population “enough of a chance” to reveal its true variability to us. It’s not just that we suspect that the estimate is wrong: after all, with only two observations we expect it to be wrong to some degree. The worry is that the error is <em>systematic</em>, such that <em>every</em> sample of 2 observations would be wrong in the same direction. Specifically, we suspect that the sample standard deviation is likely to be smaller than the population standard deviation.</p>
<p>We can use R to simulate the results of many samples to demonstrate this. Given the true population mean of IQ is 100 and the standard deviation is 15, we can use the <code class="docutils literal notranslate"><span class="pre">rnorm()</span></code> function to generate the the results of an experiment in which we measure N=2 IQ scores, and calculate the sample standard deviation. If we do this over and over again, and plot a histogram of these sample standard deviations, we get the sampling distribution of the standard deviation (see figure below).</p>
<img src="ch7-sdestimate.png"  width="350">
<p>Even though the true population standard deviation is 15, the average of the sample standard deviations is only 8.5. Notice that this is a very different result to what we found when we plotted the sampling distribution of the mean. In that sampling distribution, the population mean is 100, and the average of the sample means is also 100.</p>
<p>Now let’s extend the simulation. Instead of restricting ourselves to the situation where we have a
sample size of N=2, let’s repeat the exercise again for sample sizes from 1 to 10. If we plot the average sample mean and average sample standard deviation as a function of sample size, you get the results shown in this next figure.</p>
<img src="ch7-estimatesim.png"  width="650">
<p>On the left hand side (panel A) is the average sample mean for each sample size, and on the right hand side (panel B) is the average standard deviation for each sample size. The two plots are quite different: no matter the sample size, the average sample mean from a sample distribution of the mean is equal to the population mean. It is an <strong>unbiased estimator,</strong> which is the reason why your best estimate for the population mean is the sample mean. The plot on the right shows that the average sample standard deviation from a sample distribution of the standard deviation is smaller than the population standard deviation σ for small sample sizes. It is a <strong>biased estimator.</strong> In other words, if we want to make a “best guess” σˆ about the value of the population standard deviation σ, we should make sure our guess is a little bit larger than the sample standard deviation s.</p>
<p>The fix to this systematic bias turns out to be very simple. Here’s how it works. Before tackling the standard deviation, let’s look at the variance. If you recall from Chapter 5, the sample variance is defined to be the average of the squared deviations from the sample mean, with one change. That is:</p>
<div class="math notranslate nohighlight">
\[s^2 = \frac{1}{N-1}\sum_{i=1}^{N}|X_i-\bar{X}|^2\]</div>
<p>where the sum of the squared deviations are divided by N-1 instead of N, like in a normal average. As it turns out, this change is all we need to do to make variance an unbiased estimator. This is also true in the equation for standard deviation:</p>
<div class="math notranslate nohighlight">
\[s = \sqrt{\frac{1}{N-1}\sum_{i=1}^{N}|X_i-\bar{X}|^2}\]</div>
<p>This is why it’s important to keep the idea of a <em>sample statistic</em> and an <em>estimate of the population parameter</em> separate in your head. For mean, they are calculated the same way, but for standard deviation they are calculated differently. Notice also that, because the R functions <code class="docutils literal notranslate"><span class="pre">var()</span></code> and <code class="docutils literal notranslate"><span class="pre">sd()</span></code> use N-1, they are technically calculating a population parameter estimate and not a sample statistic. This is because, in almost every real life application, what we actually care about is the estimate of the population parameter (i.e., extrapolating from the current sample to saying something about the larger world), and so people usually report ˆσ rather than s.</p>
</section>
</section>
<section id="confidence-intervals">
<h2>6.8 Confidence intervals<a class="headerlink" href="#confidence-intervals" title="Permalink to this headline">#</a></h2>
<p>Up to this point in this chapter, we’ve outlined the basics of sampling theory which statisticians rely on to make guesses about population parameters on the basis of a sample of data. As this discussion illustrates, one of the reasons we need all this sampling theory is that every data set leaves us with a some of uncertainty, so our estimates are never going to be perfectly accurate. What has been missing from this discussion is an attempt to <em>quantify</em> the amount of uncertainty that attaches to our estimate. It’s not enough to be able guess that the mean IQ of undergraduate psychology students is 110 - we also want to be able to say something that expresses the degree of certainty that we have in our guess. For example, it would be nice to be able to say that there is a 95% chance that the true mean lies between 104 and 116.</p>
<p>The name for this is a <strong>confidence interval</strong> for the mean. Armed with an understanding of sampling distributions, constructing a confidence interval for the mean is actually pretty easy. Suppose the true population mean is µ and the
standard deviation is σ. We’ve just finished running a study that has N participants, and the mean IQ among those participants is X¯. We know from our discussion of the Central Limit Theorem that the sampling distribution of the mean is approximately normal. We also know from our discussion of standard deviation in Chapter 5 that 95% of a normally distribution will fall within 2 standard deviations of the true mean (it’s more like 1.96 standard deviations, actually). To be more precise, we can use the <code class="docutils literal notranslate"><span class="pre">qnorm()</span></code> function (like <code class="docutils literal notranslate"><span class="pre">quantile()</span></code> but for sampling from the normal distribution rather than a dataset) to compute the 2.5th and 97.5th percentiles of the normal distribution:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">qnorm</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">0.025</span><span class="p">,</span><span class="m">0.975</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>
.list-inline {list-style: none; margin:0; padding: 0}
.list-inline>li {display: inline-block}
.list-inline>li:not(:last-child)::after {content: "\00b7"; padding: 0 .5ex}
</style>
<ol class=list-inline><li>-1.95996398454005</li><li>1.95996398454005</li></ol>
</div></div>
</div>
<p>Next, recall that the standard deviation of the sampling distribution of the mean is referred to as the standard error (not to be confused with the mean of the sampling distriution of the standard deviation - be careful with your naming!) Because several sample means compile into a normal distribution (according to the Central Limit Theorem), there is a 95% chance than one sample mean is within 1.96 standard error of the mean of the sampling distribution.
Written mathematically, there is a 95% chance that:</p>
<div class="math notranslate nohighlight">
\[µ - (1.96 * SEM) \leq \bar{X} \leq µ + (1.96 * SEM)\]</div>
<p>where the SEM is equal to σ/sqrt(N). However, that’s not answering the question that we’re actually interested in. The equation above tells us what we should expect about the sample mean, given that we know what the population parameters are. What we want is to have this work the other way around: we want to know what we should believe about the population parameters, given that we have observed a particular sample. Using a little algebra, we can rewrite our equation as:</p>
<div class="math notranslate nohighlight">
\[\bar{X} - (1.96 * SEM) \leq µ \leq \bar{X} + (1.96 * SEM)\]</div>
<p>What this is telling is is that the range of values has a 95% probability of containing the population mean µ. We refer to this range as a <strong>95% confidence interval</strong>, denoted CI<sub>95</sub>. In short, as long as N is sufficiently large – large enough for us to believe that the sampling distribution of the mean is normal – then we can write this as our formula for the 95% confidence interval:</p>
<div class="math notranslate nohighlight">
\[CI_95 = \bar{X} \pm (1.96*\frac{\hat{σ}}{\sqrt{N}})\]</div>
<p>Note that we’re specifying σ^ here because we usually don’t know the true population σ. Also, there’s nothing special about the number 1.96 here: it just happens to be the multiplier you need to use if you want a 95% confidence interval. If I’d wanted a 70% confidence interval, I could have used the qnorm() function to calculate the 15th and 85th quantiles:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">qnorm</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">0.15</span><span class="p">,</span><span class="m">0.85</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><style>
.list-inline {list-style: none; margin:0; padding: 0}
.list-inline>li {display: inline-block}
.list-inline>li:not(:last-child)::after {content: "\00b7"; padding: 0 .5ex}
</style>
<ol class=list-inline><li>-1.03643338949379</li><li>1.03643338949379</li></ol>
</div></div>
</div>
<p>and so the formula for CI<sub>70</sub> would be the same as the formula for CI<sub>95</sub> except that we’d use 1.04 as our magic number rather than 1.96.</p>
<p>Additionally, because the SEM is included in the equation, we can conclude that smaller sample sizes (and thus larger SEM) lead to a wider confidence interval.</p>
<section id="pitfall-of-interpreting-confidence-intervals">
<h3>Pitfall of interpreting confidence intervals<a class="headerlink" href="#pitfall-of-interpreting-confidence-intervals" title="Permalink to this headline">#</a></h3>
<p>The hardest thing about confidence intervals is remembering specifically what they mean. Whenever people first encounter confidence intervals, often the first instinct is to treat an interval as <em>the</em> interval in which future sample means can be found. This is incorrect, and to understand it we have to remember the concept of repeated sampling. Imagine taking repeated samples of the same size from the same population. For each sample calculate a 95% confidence interval. Since the samples are different, so are the confidence intervals. We know that 95% of these intervals will include the population parameter. However, without any additional information, we cannot say which ones! Thus with only one sample, and no other information about the population parameter, we can say there is a 95% chance that this interval includes the population parameter. But this does <em>not</em> mean that 95% of other sample means will be in this interval. The confidence interval itself is an estimate, not a fixed parameter.</p>
<img src="ch7-cis.png"  width="650">
<p><em>95% confidence intervals. The top (panel A) shows 50 simulated replications of an experiment in which we measure the IQs of 10 people. The dot marks the location of the sample mean, and the line shows the 95% confidence interval. In total 47 of the 50 confidence intervals do contain the true mean (i.e., 100), but the three intervals marked with asterisks do not. The lower graph (panel B) shows a similar simulation, but this time we simulate replications of an experiment that measures the IQs of 25 people. The sample means are generally closer to the population mean, so the CIs can be narrower in order for ~95% of them to contain the true mean and 5% of them to not.</em></p>
</section>
</section>
<section id="chapter-summary">
<h2>Chapter summary<a class="headerlink" href="#chapter-summary" title="Permalink to this headline">#</a></h2>
<p>After reading this chapter, you should be able to:</p>
<ul class="simple">
<li><p>Recount the three statements proved by the Central Limit Theorem</p></li>
</ul>
</section>
<section id="chapter-quiz">
<h2>Chapter quiz<a class="headerlink" href="#chapter-quiz" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>single sample statistics versus sampling distribution statistics</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "smburns47/Psyc158",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "r"
        },
        kernelOptions: {
            kernelName: "ir",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'ir'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Shannon Burns<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>