{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50a5336e",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](https://www.shannonmburns.com/Psyc158/intro.html)\n",
    "\n",
    "[Previous: Chapter 18 - Evaluating Effect Sizes](https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-18.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28ba96a",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Run this first so it's ready by the time you need it\n",
    "install.packages(\"dplyr\")\n",
    "install.packages(\"ggformula\")\n",
    "install.packages(\"performance\")\n",
    "install.packages(\"see\")\n",
    "install.packages(\"patchwork\")\n",
    "library(dplyr)\n",
    "library(ggformula)\n",
    "library(performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110d1b35",
   "metadata": {},
   "source": [
    "# Chapter 19 - Model Bias\n",
    "\n",
    "## 19.1 Error vs. bias\n",
    "\n",
    "We use statistics to try and explain complex things in the world, and arrive at some general conclusions about what we can expect. More specifically in the context of the general linear model, we try to model the data generation process of data we care about and see what information helps us make the best predictions about those data.\n",
    "\n",
    "We know from our discussion of model error that it is hard and maybe impossible to make perfectly correct predictions. In any data generation process, we might be able to figure out that using information from some predictor variables helps us explain some variation in an outcome variable and make *better* predictions, but there is almost always some error left unexplained.  \n",
    "\n",
    "<img src=\"images/ch10-var1.png\" width=\"500\">\n",
    "\n",
    "For any particular prediction we make about the outcome value of one data point, that prediction is likely to be off by a bit. This amount that we typically miss by is the error of a model. We can quantify it by looking at the distribution of the residuals a model produces when making predictions. For example, let's simulate a sample of data with a partially-known data generation process, and make predictions using the part of the model that we know: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2557313",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "x <- rnorm(100,0,1)  #defining a random variable\n",
    "e <- rnorm(100,0,1)  #some unexplained error\n",
    "y <- 2 + 0.5*x + e   #the data generation process \n",
    "sim_df <- data.frame(x,y)\n",
    "\n",
    "sim_df$predicted_y <- 2 + 0.5*sim_df$x\n",
    "sim_df$residuals <- sim_df$y - sim_df$predicted_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fbdedf",
   "metadata": {},
   "source": [
    "If we plot the residuals of the model in this sample, making a histogram of the error distribution, we see that most of them are non-zero. We are missing our predictions by a bit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120aee60",
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "gf_histogram(~ sim_df$residuals)%>% gf_vline(., xintercept = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1daeb32",
   "metadata": {},
   "source": [
    "The spread of the error distribution tells us by how much we typically miss our predictions. A wide error distribution means a model has a lot of error. The narrower we can make the error distribution, and the less error there is, the better our model. \n",
    "\n",
    "However, model error isn't the only thing for us to be aware of when relying on the predictions a model makes. Let's imagine a situation where the true data generation process only has a very small amount of unaccountable variation, so that we are able to explain almost all the error. We can still make very inaccurate predictions if we use the wrong coefficients in a model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750ac9ff",
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "x <- rnorm(100,0,1)     #defining a random variable\n",
    "e <- rnorm(100,0,0.1)   #tiny unexplained error\n",
    "y <- 2 + 0.5*x + e      #data generation process\n",
    "sim_df <- data.frame(x,y)\n",
    "\n",
    "sim_df$biased_y <- 4 + 0.5*sim_df$x\n",
    "sim_df$residuals <- sim_df$y - sim_df$biased_y\n",
    "\n",
    "gf_histogram(~ sim_df$residuals)%>% gf_vline(., xintercept = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f380c8b",
   "metadata": {},
   "source": [
    "Take a look at the center of both these error distributions. In the first error distribution we made, few residual values were exactly equal to 0 (few predictions were perfect), but across all the residuals they clustered *around* 0. That means even if any one prediction is unlikely to be perfect, the predictions as a whole aren't missing in any systematic way. \n",
    "\n",
    "In contrast, the spread of the second error distribution is narrow, but the central tendency is way off 0. This means that every prediction we are making is wrong, and they're all wrong in the same way. \n",
    "\n",
    "This is known as **bias**. A model has error if its predictions are sometimes wrong. A model has bias if the predictions are wrong in a systematic way. \n",
    "\n",
    "We can use a bulls-eye metaphor to understand predictions in terms of error and bias. If the bulls-eye is the actual value of a datapoint on an outcome variable and each dot is a prediction, model error refers to the spread of those predictions while model bias refers to where those predictions are centered on. \n",
    "\n",
    "<img src=\"images/ch20-errorbias.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275afaa1",
   "metadata": {},
   "source": [
    "What will make a model biased? Take a look at the model coefficients that were used to make predictions for both error distributions above. In the unbiased model, we used the values \"2\" and \"0.5\" as coefficients for the ```intercept``` and ```x```, respectively, in order to make predictions about ```y```. These are the exact same values we used in the data generation process for y, so we know the only reason our predictions were off is because we didn't know the value of ```e``` to include in the prediction equation. \n",
    "\n",
    "In the biased model, we used \"4\" instead of \"2\" for the coefficient of the intercept. This made our predictions systematically overshoot the actual values of ```y```. \n",
    "\n",
    "While error speaks to the spread of the error distribution, bias is about the central tendency. If the central tendency of the the error distribution is not zero, a model is biased."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0365c42f",
   "metadata": {},
   "source": [
    "## 19.2 Out-of-sample predictions\n",
    "\n",
    "In practice, if you fit models in R, the model will never be biased *for this data sample*. This is because R automatically computes the best-fitting coefficients to reduce error and eliminate bias (this is what using the mean as a model does inherently). However, usually we care about more than just these data. A bigger concern is thus whether the model will be biased *for new data*.\n",
    "\n",
    "Recall from our discussion of sampling distributions that any one estimate derived from a data sample is unlikely to exactly match the population parameter. For example, let's simulate an entire population of data with the same data generation process above, and fit a model in just a sample of it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75cfc20",
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "set.seed(30)\n",
    "x <- rnorm(10000,0,1)  #defining a random variable\n",
    "e <- rnorm(10000,0,1)  #some unexplained error\n",
    "y <- 2 + 0.5*x + e     #data generation process \n",
    "sim_df <- data.frame(x,y)\n",
    "\n",
    "sim_sample <- slice_sample(sim_df, n=100, replace=TRUE)\n",
    "\n",
    "sim_model <- lm(y ~ x, data = sim_sample)\n",
    "sim_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81c8b81",
   "metadata": {},
   "source": [
    "Whereas the true intercept and effect of x are 2 and 0.5 respectively, our estimates for those coefficients are 1.95 and 0.57. If we were to use these values to make predictions within a separate dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a81e7a",
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "set.seed(40)\n",
    "sim_sample2 <- slice_sample(sim_df, n=100, replace=TRUE)\n",
    "\n",
    "sim_sample2$predicted_y <- predict(sim_model, sim_sample2)\n",
    "sim_sample2$residuals <- sim_sample2$y - sim_sample2$predicted_y\n",
    "mean(sim_sample2$residuals) #center of the error distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb6a5ce",
   "metadata": {},
   "source": [
    "The residuals are not centered on 0. This is because the model coefficients we used for predictions are just estimates of the population parameter and in this case didn't exactly match. Making predictions about new data will thus be wrong in a systemic way. \n",
    "\n",
    "There are a few things we can do to minimize bias when making out-of-sample predictions. One of them is to collect larger samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6113d26",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "set.seed(50)\n",
    "sim_sample <- slice_sample(sim_df, n=1000, replace=TRUE) #bigger sample\n",
    "sim_model <- lm(y ~ x, data = sim_sample)\n",
    "\n",
    "\n",
    "sim_sample2 <- slice_sample(sim_df, n=1000, replace=TRUE) #bigger sample\n",
    "\n",
    "sim_sample2$predicted_y <- predict(sim_model, newdata = sim_sample2)\n",
    "sim_sample2$residuals <- sim_sample2$y - sim_sample2$predicted_y\n",
    "mean(sim_sample2$residuals) #center of the error distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9646d96",
   "metadata": {},
   "source": [
    "The center of the error distribution when we make predictions in ```sim_sample2``` using the model fitted on ```sim_sample1``` is still not 0, but it's much closer to 0. This is because of the Central Limit Theorem - with bigger samples, there is less variance in coefficient estimates sample to sample. \n",
    "\n",
    "Another option is to collect several small samples and then average together their effect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a60b88",
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#drawing many samples and saving their coefficients\n",
    "b0s <- vector(length=100)\n",
    "b1s <- vector(length=100)\n",
    "for (i in 1:100) {\n",
    "    sim_sample <- slice_sample(sim_df, n=100, replace=TRUE) \n",
    "    sim_model <- lm(y ~ x, data = sim_sample)\n",
    "    b0s[i] <- sim_model$coefficients[[1]]\n",
    "    b1s[i] <- sim_model$coefficients[[2]]\n",
    "}\n",
    "\n",
    "mean_b0 <- mean(b0s)\n",
    "mean_b1 <- mean(b1s)\n",
    "\n",
    "#predicting new data in the population based on average estimated effect\n",
    "sim_df$predicted_y <- mean_b0 + mean_b1*sim_df$x\n",
    "sim_df$residuals <- sim_df$y - sim_df$predicted_y\n",
    "mean(sim_df$residuals) #center of the error distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a24ffb",
   "metadata": {},
   "source": [
    "This is what people are doing when they perform a meta-analysis. They are compiling the estimates across many different studies to arrive at one average effect that is hopefully closer to the true population parameter, and thus produces less bias in new predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4febe68",
   "metadata": {},
   "source": [
    "## 19.3 Biased and unbiased estimates\n",
    "\n",
    "In the example above, we saw that even if coefficient estimates derived in one sample are different from the true population parameter and produced biased out-of-sample predictions by themselves, *on average* across many studies these estimates converged on the population parameter. This means that if we repeat our sampling and analysis process many times (or if we get a really big sample size), there is no systematic bias in our estimation of the population parameter. This is the difference between bias in predictions and bias in estimates. Over many data points there may be error in residuals but no bias in predictions. Comparably, over many studies there may be error in effect estimations but no bias in the estimated population parameter.\n",
    "\n",
    "It is possible to get bias in coefficient estimations, however. This is the worst situation for bias because if your effect estimate is always off, your predictions will always be off too in a systematic way and no amount of sample size or meta-analyzing will help you. \n",
    "\n",
    "Biased estimators come from the process of building and fitting a model. Typically, the GLM framework produces unbiased estimates. But this only holds if your data meets certain assumptions that the GLM has about it. The GLM framework is really powerful, but it's not infallible. There are situations where it is not the appropriate statistical method to use. For the rest of the chapter we will learn about the assumptions of the GLM and how to check for them. If your data violate these assumptions, the GLM will produce biased estimates that will never give you good predictions in out-of-sample data. We will also learn what to do about this situation if you find that it is the case with your data. \n",
    "\n",
    "There are 5 assumptions about how a model fits our data that we want to meet if we are to avoid bias. Violations of these threaten the validity of your statistical conclusions.\n",
    "\n",
    "1) Linearity\n",
    "2) Homogeneity of variance\n",
    "3) Low leverage \n",
    "4) No multicollinearity\n",
    "5) Normality of residuals\n",
    "\n",
    "These assumptions can be evaluated within your data sample, enabling you to take a different modeling approach if this evaluation is less than satisfactory. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440bc2ba",
   "metadata": {},
   "source": [
    "## 19.4 Assumption 1: Linearity\n",
    "\n",
    "Throughout this course we've been learning how to use the General *Linear* Model for prediction and inference. A \"linear\" model means that the best fitting regression line through the scatterplot of X/Y association is straight. This is specified during the fitting process by combining predictors with addition. Another way of looking at it is that the slope of the effect of X is the same regardless of the value of X.\n",
    "\n",
    "<img src=\"images/ch14-linear.png\" width=\"250\">\n",
    "\n",
    "However another data situation is when the effect of X *changes* depending on the value of X. In this case, the best-fitting regression line through the data is not straight. \n",
    "\n",
    "<img src=\"images/ch14-nonlinear.png\" width=\"250\">\n",
    "\n",
    "The GLM form that we've been using assumes that the linear type of relationship is what best fits the data. Much of the time it is safe to assume that the relationship between a predictor and an outcome is linear. However if you happen to be dealing with a nonlinear situation and you misspecify the model as linear by using the basic form of the GLM, this can result in bias. \n",
    "\n",
    "We can use simulation to see how this bias would play out. First, we will simulate a relationship between x and y where the true $\\beta_1$ is 0.5, but in a log relationship such that as x gets larger, its effect on y gets smaller. You can see such a relationship in the scatterplot below, where the red line is the curved log relationship between x and y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad79240",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "set.seed(10)\n",
    "x <- runif(10000,0,5)    #defining a random variable\n",
    "e <- rnorm(10000,0,1)    #some unexplained error\n",
    "x_log <- log(x)\n",
    "y <- 2 + 0.5*x_log + e  #0.5 is the true b1 value in a log model \n",
    "sim_df <- data.frame(x, x_log, y)\n",
    "\n",
    "sim_sample <- sample_n(sim_df, size=100, replace=TRUE)\n",
    "\n",
    "#scatterplot of true data generation process with incorrect linear predictions\n",
    "gf_point(y ~ x, data = sim_sample) %>% gf_smooth(color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409480b0",
   "metadata": {},
   "source": [
    "Now let's draw many such samples from this data generation process to see the sampling distribution of the $b_1$ estimates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ac580f",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#sampling many estimates of unbiased b1\n",
    "nobias_b1s <- vector(length=1000)\n",
    "for (i in 1:1000) {\n",
    "    sim_sample <- sample_n(sim_df, size=100, replace=TRUE) \n",
    "    sim_model <- lm(y ~ x_log, data = sim_sample)  \n",
    "    nobias_b1s[i] <- sim_model$coefficients[[2]]\n",
    "}\n",
    "\n",
    "#red = unbiased sampling distribution\n",
    "nobias_b1s_df <- data.frame(nobias_b1s)\n",
    "gf_histogram(~ nobias_b1s, data = nobias_b1s_df, fill=\"red\", alpha=0.5) %>% \n",
    "    gf_vline(., xintercept = 0.5) #true b1 is 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc3877d",
   "metadata": {},
   "source": [
    "As we can see from these results, any one sample may find a $b_1$ estimate that is larger or smaller than the true effect 0.5, but in general they will cluster around 0.5.\n",
    "\n",
    "Now let's try a simulation of what happens when the true data generation process is nonlinear like this, but we try to make predictions with a linear model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbc0738",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#sampling many estimates of biased b1\n",
    "bias_b1s <- vector(length=1000)\n",
    "for (i in 1:1000) {\n",
    "    sim_sample <- sample_n(sim_df, size=100, replace=TRUE) \n",
    "    sim_model <- lm(y ~ x, data = sim_sample)  #fitting a linear model instead of nonlinear\n",
    "    bias_b1s[i] <- sim_model$coefficients[[2]]\n",
    "}\n",
    "\n",
    "#red = unbiased sampling distribution\n",
    "#blue = biased sampling distribution\n",
    "nobias_b1s_df <- data.frame(nobias_b1s)\n",
    "bias_b1s_df <- data.frame(bias_b1s)\n",
    "gf_histogram(~ nobias_b1s, data = nobias_b1s_df, fill=\"red\", alpha=0.5) %>% \n",
    "    gf_histogram(~ bias_b1s, data = bias_b1s_df, fill=\"blue\", alpha=0.5) %>%\n",
    "    gf_vline(., xintercept = 0.5) #true b1 is 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b230a8",
   "metadata": {},
   "source": [
    "This model misspecification causes the sampling distribution of $b_1$ estimates to center on ~0.3 rather than 0.5 as in the true data generation process. By misspecifying the model, we are systematically wrong on the estimation of $\\beta_1$ and thus making consistently wrong guesses about the effect of x on y. If we were to take a model estimate at face value without investigating whether the data should actually be fit with a linear model, we'd make incorrect conclusions about the true effect size. This is an example of model bias.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23164e34",
   "metadata": {},
   "source": [
    "## 19.5 Assumption 2: Homogeneity of variance\n",
    "\n",
    "The second assumption of the GLM is that there is constant residual variance at each level of model prediction. The best way to understand this is visually. We will make a plot where, as a predictor x increases, the error in the model increases as well: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ebae29",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "x <- runif(10000,0,5)             #defining a random variable\n",
    "e_het <- x*rnorm(10000,0,1)    #error gets wider as a function of predictor\n",
    "e_hom <- rnorm(10000,0,1)\n",
    "y_het <- 0.5*x + e_het\n",
    "y_hom <- 0.5*x + e_hom\n",
    "sim_df <- data.frame(x, y_het, y_hom)\n",
    "\n",
    "subsample <- sample_n(sim_df, size=100, replace=TRUE)\n",
    "sub_model <- lm(y_het ~ x, data = subsample)\n",
    "subsample$resid <- subsample$y_het - predict(sub_model, subsample)\n",
    "gf_point(resid ~ x, data=subsample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7132866",
   "metadata": {},
   "source": [
    "In this plot we are showing model residuals as they vary with a predictor x. When the assumption of homogeneity of variance is met, this plot should look like a fairly consistent cloud where the range of residuals is about the same for every level of x. If there is instead a clear cone shape like in this plot, this means the residuals are more variable at certain levels of x than others and the assumption is violated. Other words you might hear that refer to this assumption is **homoscedasticity** (when residual variance is homogenous) and **heteroscedasticity** (when residual variance changes as a function of a predictor).\n",
    "\n",
    "This situation will occur if measurement error is correlated with values of the predictor. Let's run another simulation and see what happens when this assumption is violated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76236a7f",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#sampling distribution of b1 with homoscedastic errors\n",
    "nobias_b1s <- vector(length=1000)\n",
    "for (i in 1:1000) {\n",
    "  sim_sample <- sample_n(sim_df, size=100, replace=TRUE) \n",
    "  sim_model <- lm(y_hom ~ x, data = sim_sample)  #fitting a model with homoscedastic errors\n",
    "  nobias_b1s[i] <- sim_model$coefficients[[2]]\n",
    "}\n",
    "\n",
    "#sampling distribution of b1 with heteroscedastic errors\n",
    "bias_b1s <- vector(length=1000)\n",
    "for (i in 1:1000) {\n",
    "  sim_sample <- sample_n(sim_df, size=100, replace=TRUE) \n",
    "  sim_model <- lm(y_het ~ x, data = sim_sample)  #fitting a model with heteroscedastic errors\n",
    "  bias_b1s[i] <- sim_model$coefficients[[2]]\n",
    "}\n",
    "\n",
    "#red = unbiased sampling distribution\n",
    "#blue = biased sampling distribution\n",
    "nobias_b1s_df <- data.frame(nobias_b1s)\n",
    "bias_b1s_df <- data.frame(bias_b1s)\n",
    "gf_histogram(~ nobias_b1s, data = nobias_b1s_df, fill=\"red\", alpha=0.5) %>% \n",
    "    gf_histogram(~ bias_b1s, data = bias_b1s_df, fill=\"blue\", alpha=0.5) %>%\n",
    "    gf_vline(., xintercept = 0.5) #true b1 is 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c221f25c",
   "metadata": {},
   "source": [
    "In the data generation process we specified, $\\beta_1$ is 0.5. In samples with homoscedastic errors, the sampling distribution (red) is correctly centered on this value. In samples with heteroscedastic errors, the sampling distribution (blue) is also centered on this value. So what's the problem? The difference here is that the sampling distribution is *wider*, despite having the same sample size. It isn't the model coefficient that is biased in this case, but the *standard error* is biased to be too big. On average we will detect the right effect, but it will be harder to get an accurate picture of it. Our p-values will be wrong. \n",
    "\n",
    "<img src=\"images/ch19-heteroscedasticity.gif\" width=\"800\">\n",
    "\n",
    "*[gif source](https://sites.google.com/view/robertostling/home/teaching)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f63c20b4",
   "metadata": {},
   "source": [
    "## 19.6 Assumption 3: Low leverage\n",
    "\n",
    "We've discussed outliers before. In review, these are data points that are far outside the other values of a distribution. In a simple histogram, one outlier can change our estimate of the mean (median and mode are more robust to outliers, as you recall). In a general linear model, a value can be an outlier on a predictor variable, the outcome, or both. \n",
    "\n",
    "Outliers have the potential to bias statistical estimates in GLMs like they do the mean in a histogram, but this isn't guaranteed. Consider the below simple regression plots, showing a 0.5 linear correlation between an x and y variable with and without an outlier included: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332714d1",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "set.seed(7)\n",
    "x <- rnorm(100,mean=0,sd=1)    #defining a random predictor\n",
    "ex <- rnorm(100,mean=0,sd=1)   #some unexplained error\n",
    "y <- 0.5*x + ex             \n",
    "sim_df <- data.frame(x, y)\n",
    "\n",
    "#without outlier\n",
    "summary(lm(y~x, data = sim_df))\n",
    "gf_point(y ~ x, data = sim_df) %>% gf_lm(interval = \"confidence\")\n",
    "\n",
    "#with an outlier added\n",
    "sim_df_outlier <- add_row(sim_df, x=7, y=4)\n",
    "summary(lm(y~x, data = sim_df_outlier))\n",
    "gf_point(y ~ x, data = sim_df_outlier, color='darkred') %>% gf_lm(interval = \"confidence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28563eca",
   "metadata": {},
   "source": [
    "In the plot with red dots, the outlier at x=7 is far outside the other x values. But notice that the $b_1$ estimate in this case (0.52) is not that much different than the initial estimate without an outlier (0.51). \n",
    "\n",
    "Now consider this plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b98dffd",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#another outlier\n",
    "sim_df_outlier <- add_row(sim_df, x=7, y=-2)\n",
    "summary(lm(y~x, data = sim_df_outlier))\n",
    "gf_point(y ~ x, data = sim_df_outlier, color='darkgreen') %>% gf_lm(interval = \"confidence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c28583",
   "metadata": {},
   "source": [
    "We have another example with an outlier that is extreme on x, but this time the $b_1$ estimate is 0.23. That's less than half the size of the correlation estimate without this outlier. What happened?\n",
    "\n",
    "The answer is something called **leverage**. An outlier has leverage if its presence meaningfully changes the effect estimate in a sample. This would happen if the relationship between the x and y values for that data point are very different than expected; i.e. it has a huge residual. You can see this in the green plot above, where the outlier is far below the regression line. Since the regression line is trying to balance the amount of error above and below, a huge residual for one data point means the line is going to tip towards that outlier to compensate. This is why the outlier in the red plot didn't have much leverage - its values were extreme on x and y, but it still fell close to the initial regression line so it didn't change the slope estimate by much.\n",
    "\n",
    "Here is a simulation of the effect of adding one outlier to a sample of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fdd7d3",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#sampling many estimates of unbiased b1\n",
    "nobias_b1s <- vector(length=1000)\n",
    "for (i in 1:1000) {\n",
    "    sim_sample <- slice_sample(sim_df, n=100, replace=TRUE) \n",
    "    sim_model <- lm(y ~ x, data = sim_sample)  \n",
    "    nobias_b1s[i] <- sim_model$coefficients[[2]]\n",
    "}\n",
    "\n",
    "#sampling many estimates of biased b1\n",
    "bias_b1s <- vector(length=1000)\n",
    "for (i in 1:1000) {\n",
    "    sim_sample <- slice_sample(sim_df, n=100, replace=TRUE)\n",
    "    outlier_sample <- add_row(sim_sample, x=rnorm(1,0,5), y=rnorm(1,0,5)) \n",
    "    sim_model <- lm(y ~ x, data = outlier_sample)  #fitting a linear model instead of nonlinear\n",
    "    bias_b1s[i] <- sim_model$coefficients[[2]]\n",
    "}\n",
    "\n",
    "#red = unbiased sampling distribution\n",
    "#blue = biased sampling distribution\n",
    "nobias_b1s_df <- data.frame(nobias_b1s)\n",
    "bias_b1s_df <- data.frame(bias_b1s)\n",
    "gf_histogram(~ nobias_b1s, data = nobias_b1s_df, fill=\"red\", alpha=0.5) %>% \n",
    "    gf_histogram(~ bias_b1s, data = bias_b1s_df, fill=\"blue\", alpha=0.5) %>%\n",
    "    gf_vline(., xintercept = 0.5) #true b1 is 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e023c4",
   "metadata": {},
   "source": [
    "There's a bit of bias to this blue distribution (it's not perfectly normal), because more often \"noise\" like an outlier added to a variable will make statistical associations weaker instead of stronger. The biased sampling distribution is also wider, so we have a problem with the standard error estimate again. In situations with leverage, our $b_1$ estimates are likely to be biased downwards *and* our p-values will be wrong. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3979f52",
   "metadata": {},
   "source": [
    "## 19.7 Assumption 4: Normality of residuals\n",
    "\n",
    "A related situation is when there isn't one particular data point with a lot of leverage, but the overall error distribution is highly skewed. Here's an example of what non-normal residuals might look like:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88bb7c6",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "x <- runif(10000,0,5)           #defining a random variable\n",
    "e_norm <- runif(10000,0,1)\n",
    "e_skew <- log(runif(10000,0,1)) #error is skewed, not normal\n",
    "e_skew <- e_skew - mean(e_skew) #mean centering the error term\n",
    "gf_histogram(~ e_skew)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b25a08c",
   "metadata": {},
   "source": [
    "This distribution means most of our residuals are small, but the ones that aren't are all in the same direction.\n",
    "\n",
    "Now we simulate what such an error distribution does to our model estimations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502193fe",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "y_norm <- 0.5*x + e_norm\n",
    "y_skew <- 0.5*x + e_skew\n",
    "sim_df <- data.frame(x, y_skew, y_norm)\n",
    "\n",
    "#sampling distribution of b1 with normal errors\n",
    "nobias_b1s <- vector(length=1000)\n",
    "for (i in 1:1000) {\n",
    "  sim_sample <- sample_n(sim_df, size=100, replace=TRUE) \n",
    "  sim_model <- lm(y_norm ~ x, data = sim_sample)  \n",
    "  nobias_b1s[i] <- sim_model$coefficients[[2]]\n",
    "}\n",
    "\n",
    "#sampling distribution of b1 with skewed errors\n",
    "bias_b1s <- vector(length=1000)\n",
    "for (i in 1:1000) {\n",
    "  sim_sample <- sample_n(sim_df, size=100, replace=TRUE) \n",
    "  sim_model <- lm(y_skew ~ x, data = sim_sample)  \n",
    "  bias_b1s[i] <- sim_model$coefficients[[2]]\n",
    "}\n",
    "\n",
    "#red = unbiased sampling distribution\n",
    "#blue = biased sampling distribution\n",
    "nobias_b1s_df <- data.frame(nobias_b1s)\n",
    "bias_b1s_df <- data.frame(bias_b1s)\n",
    "gf_histogram(~ nobias_b1s, data = nobias_b1s_df, fill=\"red\", alpha=0.5) %>% \n",
    "    gf_histogram(~ bias_b1s, data = bias_b1s_df, fill=\"blue\", alpha=0.5) %>%\n",
    "    gf_vline(., xintercept = 0.5) #true b1 is 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4b38a4",
   "metadata": {},
   "source": [
    "In this case, the sampling distribution is not really biased but the standard error will be wrong. Our p-values will be off if we don't stop to consider and fix this situation.\n",
    "\n",
    "Note that this assumption is about the normality of the distribution of *residuals* in a model, not the distribution of the *raw variables*. That is a common misconception. A highly non-normal predictor or outcome can still produce normally-distributed errors in the model in many situations. To assess this assumption you need to check the error distribution specifically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b50ef83",
   "metadata": {},
   "source": [
    "## 19.8 Assumption 5: No multicollinearity\n",
    "\n",
    "The last GLM assumption that you can check for in the data has to do with how closely related predictors are in a model if you have more than one. As we learned in chapter 13, if both x1 and x2 are both possible explanatory variables of y, you should include them both in a general linear model. However, it can be problematic to make conclusions about the model estimates if the predictors are highly correlated with each other. If the variation in y explained by the predictors is almost entirely overlapping, that means there is almost no unique variation attributable to either one. In that case, minute differences in the values of each variable can result in huge swings and instability in the model estimates from sample to sample. This situation is called **multicollinearity**.\n",
    "\n",
    "Here's an example of what multicollinearity can do to model estimates. We'll first generate predictor variables that are closely related to eachother:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655f9c1c",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "x1 <- runif(10000,0,5)    #defining a random variable\n",
    "ex <- rnorm(10000,0,0.5)  #small amount of unexplained error between predictors\n",
    "x2 <- x1 + ex             #another, highly related predictor variable\n",
    "ey <- rnorm(10000,0,1)    #some unexplained error in the model\n",
    "y <- 0.5*x1 + ey          #true data generation process only involves x1\n",
    "sim_df <- data.frame(x1, x2, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c45ad1",
   "metadata": {},
   "source": [
    "In this data generation process, the true effect of x1 should be 0.5 and x2 is not a part of it. If we repeatedly sample and estimate the effect of x1 in a multivariable model when there is multicollinearity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce96ef4d",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "#sampling many estimates of unbiased b1\n",
    "nobias_b1s <- vector(length=1000)\n",
    "for (i in 1:1000) {\n",
    "  sim_sample <- sample_n(sim_df, size=100, replace=TRUE) \n",
    "  sim_model <- lm(y ~ x1, data = sim_sample) \n",
    "  nobias_b1s[i] <- sim_model$coefficients[[2]]\n",
    "}\n",
    "\n",
    "#sampling many estimates of biased b1\n",
    "bias_b1s <- vector(length=1000)\n",
    "for (i in 1:1000) {\n",
    "  sim_sample <- sample_n(sim_df, size=100, replace=TRUE) \n",
    "  sim_model <- lm(y ~ x1 + x2, data = sim_sample) \n",
    "  bias_b1s[i] <- sim_model$coefficients[[2]]\n",
    "}\n",
    "\n",
    "#red = unbiased sampling distribution\n",
    "#blue = biased sampling distribution\n",
    "nobias_b1s_df <- data.frame(nobias_b1s)\n",
    "bias_b1s_df <- data.frame(bias_b1s)\n",
    "gf_histogram(~ nobias_b1s, data = nobias_b1s_df, fill=\"red\", alpha=0.5) %>% \n",
    "    gf_histogram(~ bias_b1s, data = bias_b1s_df, fill=\"blue\", alpha=0.5) %>%\n",
    "    gf_vline(., xintercept = 0.5) #true b1 is 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9e2efa",
   "metadata": {},
   "source": [
    "We don't get much bias for the $b_1$ estimate, but the standard error is wrong (width of sampling distribution). This has an unfortunate effect on the p-values of *both* x1 and x2 in the model. There's an inflated Type I error risk for x2 (the non-causal predictor) and Type II error risk for x1 (the real predictor). A model with multicollinearity is both more likely to find the wrong predictor as significant and more likely to miss which predictor is actually important. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3eef0dc",
   "metadata": {},
   "source": [
    "## 19.9 Checking for violations of model assumptions \n",
    "\n",
    "These assumptions we've discussed so far (linearity, homogeneity of variance, low leverage, normality of residuals, and no multicollinearity) are problems with the error distribution that comes out of a model. Thus, by plotting and examining the error distribution in certain ways, we can investigate if the assumptions are violated or if we can trust our general linear modeling procedure. \n",
    "\n",
    "The ```performance``` package has a really easy method of checking all these assumptions at once, in the ```check_model()``` function. For example, let's check how a multiple regression model example from the GSS dataset performs on these assumption checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb1c184",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "GSS <- read.csv(\"https://raw.githubusercontent.com/smburns47/Psyc158/main/GSS.csv\")\n",
    "set.seed(10)\n",
    "GSS_subset <- slice_sample(GSS, n=100) %>% \n",
    "    select(., highest_year_of_school_completed, born_in_us, highest_year_school_completed_father) %>%\n",
    "    na.omit(.)\n",
    "\n",
    "m <- lm(highest_year_of_school_completed ~ highest_year_school_completed_father + born_in_us, \n",
    "        data = GSS_subset)\n",
    "check_model(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1ee77e",
   "metadata": {},
   "source": [
    "Walking through these panels one by one, let's first consider the top left. This shows a bunch of predicted outcome variable distributions (blue lines) our model form would make in a simulated resampling procedure. The thick green line is a smoothed version of the actual data distribution. While this panel doesn't check any model assumptions directly, it can let you see if the predicted distributions of the outcome variable generally resemble the actual distribution. In this example, it looks like our predicted distributions on average aren't too far off. But there could be subtle assumption violations still, so let's move on to the next panel.\n",
    "\n",
    "The top right panel checks the linearity assumption. If the relationship between predictors and outcome is linear, then residuals should be randomly distributed at all values of predictions. However if the relationship is nonlinear, a linear model will systematically underpredict some data points at one end of the outcome range and overpredict some data points at the other end of the outcome range. Thus in this scatterplot between a predicted outcome value and accompanying residual, you want the best fitting line to be approximately flat. If it's not, then that indicates the linearity assumption is violated. In this example, it looks like the best-fitting line is mostly flat, except for one point at the left side of the plot. \n",
    "\n",
    "Moving to the middle left plot, this investigates homogeneity of variance. This plots the variance of the residuals this time, as a function of predicted values. Remember that this assumption says there is constant variance at all levels of the variable - thus, we want this plot to have a mostly flat horizontal line. If it's notably lifted on either side, we have a situation of heteroscedasticity. In this example plot, looks again like there's one data point that's driving some heteroscedasticity on the left side of the plot. \n",
    "\n",
    "The middle right plot investigates leverage. This plots amount of outlierness h (expressed as metric called [Cook's Distance](https://en.wikipedia.org/wiki/Cook%27s_distance)) against the size of that data point's residual. The more a datapoint is an outlier, the smaller its residual needs to be to have high leverage on the model estimate. For this reason there is a cone in the plot to designate zones of leverage. Inside the cone are data points with low leverage, outside the cone are data points with high leverage. If a point has high leverage, it will also be colored red in this plot. In this way, we can see there is a datapoint that has been identified as high leverage - observation 48 in the dataset on which the model was fit. This data point is also probably the reason for the non-flat lines in the linearity and homogeneity of variance plots.\n",
    "\n",
    "The bottom left panel assesses multicollinearity using a metric called [Variance Inflation Factor (VIF)](https://en.wikipedia.org/wiki/Variance_inflation_factor). We want a low VIF, meaning that the predictors in the model aren't highly related to each other and thus there is low multicollinearity. This plot defines a cutoff VIF of 5 as being the point at which parameter estimates become uncertain to a problematic degree. The VIF for each of the predictors in our example is far below the cutoff, so we don't violate the assumption of multicollinearity in this example. \n",
    "\n",
    "Finally, the bottom right panel assesses normality of residuals. This assesses the proportion of the error distribution that should fall in each quantile (if it follows a standard normal distribution), vs. the proportion of the error distribution that is actually in each quantile in this sample. Dots should fall mostly along the flat line at 0. If many of these dots vear far off the 0 line, the assumption of normal residuals is violated. \n",
    "\n",
    "The key to interpreting this plot is to know that assumption violation is a spectrum, not a yes/no quality. While we can come up with some har cutoffs (like VIF = 5 or the leverage zone), some datasets will violate assumptions much more strongly than others. Thus you need to use your judgment to decide if a little bit of violation, and thus a little bit of bias, is ok or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbfdd24",
   "metadata": {},
   "source": [
    "## 19.10 What to do about model assumption violations\n",
    "\n",
    "If the ```check_model()``` plot tells you no assumptions are seriously violated, then you are good to use the GLM to test your hypotheses about your data. However, if any of these assumptions are violated, then you shouldn't trust the effect estimates or statistical significance given by the GLM. Instead, depending on what these assumption violations are, there are some alternative approaches you can use.\n",
    "\n",
    "First, if your assumption checks indicated multicollinearity, you should consider pruning down the set of variables in your model. How you do this depends on the meaning of the variables, and what your hypotheses are. If one variable is more important to you than other, you could consider leaving out the less important one. Alternatively, two highly correlated predictors might be measuring the same underlying construct. In this case, you could consider combining the variables into one measure (by averaging or summing) to use in the model. \n",
    "\n",
    "Next, if your leverage plot identified outliers with high leverage, you could consider removing those values from the dataset. Sometimes this also fixes issues other assumption violations, if those violations were because of the residual associated with high-leverage outliers. For example, we identified small violations of linearity and homogeneity of variance in the model above, but also a high-leverage outlier. let's see what happens if we remove that outlier, which was identified to be observation number 48:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28164f8c",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "GSS_removed <- GSS_subset[-48,]\n",
    "m <- lm(highest_year_of_school_completed ~ highest_year_school_completed_father + born_in_us, \n",
    "        data = GSS_removed)\n",
    "check_model(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4bee3e",
   "metadata": {},
   "source": [
    "Now there are no more high leverage outliers, and the linearity check looks much better. There may still be an issue with homogenity of variance though, so finally the last consideration to make is whether a different type of model besides the GLM would do a better job. These alternative types of models are outside the scope of this intro course, but here are some common choices for each assumption violation if you want to read more yourself: \n",
    "\n",
    "- Non-linear modeling: if the assumption of linearity is violated, this implies that a straight line is not the best relationship between a predictor and outcome. Other forms may better describe this relationship that you can capture by transforming a predictor variable (e.g., by log transforming X or making it into a quadratic term). Alternatively you can use tools that make more general assumptions, such as \"when X goes up some amount Y goes up some amount\" (i.e. [Spearman Rank Correlation](https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient)) or \"uncertainty in Y is reduced by knowing X\" (i.e. [Mutual Information](https://en.wikipedia.org/wiki/Mutual_information)). \n",
    "\n",
    "- Dimensionality reduction: An option of dealing with multicollinearity when there are a lot of predictors to think about. Different tools deal with this problem differently, but generally search for the amount of shared variation between the multicollinear predictors and make new variable(s) just out of that. Such tools include [factor analysis](https://towardsdatascience.com/exploratory-factor-analysis-in-r-e31b0015f224), [principle component analysis](https://www.r-bloggers.com/2021/05/principal-component-analysis-pca-in-r/), and [ridge regression](https://www.statology.org/ridge-regression-in-r/).\n",
    "\n",
    "- Robust regression: The math underlying the GLM uses sum of squares to quantify model residuals and find the best-fitting regression line. This approach is also called ordinary least squares (OLS) regression. Because of this, large residuals have an outsized impact on the eventual model fit (because their magnitude is squared). But this isn't the only way we could quantify a residual. Another approach called [robust regression](https://stats.oarc.ucla.edu/r/dae/robust-regression/) weights the contribution of residuals by their leverage. This means that high leverage points or non-normal residuals have less influence on the final parameter estimates. This preserves more degrees of freedom and thus statistical power than simply removing a bunch of data points.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21b2b14",
   "metadata": {},
   "source": [
    "## 19.11 Other sources of bias\n",
    "\n",
    "The five assumptions above have to do with the way a model form fits a set of data. If these assumptions are violated you want to change your approach to modeling, but luckily you can make that change at the analysis step. \n",
    "\n",
    "However there are other sources of bias in data that occur because of the way data was collected. This is harder to detect post-hoc and might not be able to be fixed after data collection. For this reason, you should consider these sources when you are first designing a study and recruiting subjects so that you don't create serious limitations in your research. \n",
    "\n",
    "### Non-representative samples\n",
    "We've discussed before what it means to have a representative sample - the distribution of data in the sample closely resembles the distribution of data in the population. When your data sample is representative, the statistical estimates you make in that sample have the best chance of resembling the true population parameter. A difference between your estimate and the population parameter can happen just by bad luck during random sampling: we just happen to draw a particularly strange sample by chance. But over many samples, random sampling will insure that on the whole our samples are representative and our estimates unbiased. \n",
    "\n",
    "If there's any reason our sampling process is not representative, those estimates might be biased even across many samples. For example, there is a famous saying that \"money does not buy happiness\". If you were to test this statistically, you might look for a significant relationship between how much money people make and their happiness levels. But if you were to conduct this research only with college graduates (who make on average 80% higher salaries than non-grads), you would only be investigating the relationship between money and happiness in people with more money, not across all income brackets. No matter what sample you drew, if it was not representative of the wider population, you might get a biased estimate (and you might miss the fact that [money is indeed correlated with happiness](http://content.time.com/time/magazine/article/0,9171,2019628,00.html) at lower income levels where financial security is threatened). If your estimate is biased due to nonrepresentative sampling, it might apply to this specific subset of the population, but not **generalize to other populations of people. \n",
    "\n",
    "One way a sample can be unrepresentative is if you don't have values from the full range of population values on your outcome. When your data is missing the full range of the outcome variable, there's less variation present that would be explained by the predictor. Noise now has a bigger proportional presence, and the predictor can't reliably explain noise, so it estimates a weaker effect. \n",
    "\n",
    "<img src=\"images/ch20-selectiony.gif\" width=\"500\">\n",
    "\n",
    "*[gif source](https://sites.google.com/view/robertostling/home/teaching)*\n",
    "\n",
    "Interestingly, the same is not true for truncating the x variable. On average, the sampling distribution will still cluster around the true effect for x. The difference here is that the estimate is more variable, which affects standard errors. \n",
    "\n",
    "<img src=\"images/ch20-selectionx.gif\" width=\"500\">\n",
    "\n",
    "*[gif source](https://sites.google.com/view/robertostling/home/teaching)*\n",
    "\n",
    "A nonrepresentative sample doesn't *guarantee* that your estimates will be biased. Estimates will only be biased if your sample is non-representative on variables that are involved in the data generation process you are investigating. If you are investigating the relationship between money and happiness but only have a sample of right-handed people, there's a pretty good argument to make that being right or left handed doesn't influence how happy money makes you. So the assumption of representativeness only applies to variables that are involved in the particular data generation process you are investigating. \n",
    "\n",
    "### Omitted variable bias\n",
    "In doing research and building theory, we are trying to create a model that closely approximates the data generation process. This means we hope to identify the predictor variables that are important for creating the outcome variable, with the right relationships between them. If we leave out important parts of the data generation process, especially variables that confound other predictors or are involved in interactions with other predictors, this will create bias in the parameter estimates for the predictors we do have. Such bias is called **omitted variable bias**. Omitted variable bias is responsible for erroneous conclusions in psychology, such as the notion that [the marshmellow test predicts self-control ability](https://www.washingtonpost.com/science/2024/08/29/research-bias-cognitive-studies-executive-function-marshmallow-test/) or that [AI can accurately read people's emotions through their facial expressions](https://www.scientificamerican.com/article/darwin-was-wrong-your-facial-expressions-do-not-reveal-your-emotions/). In both these cases, important other variables like SES or cultural background better explain or moderate the initial effect.\n",
    "\n",
    "To avoid omitted variable bias, we need to think hard about what is known of the true data generation process of an outcome variable, not just one predictor we are interested in. This means staying up to date with the latest research literature on your topic of interest and plotting out conceptual models with as much detail as one can. We should then collect these other important variables in our study and incorporate them into our modeling procedure. \n",
    "\n",
    "### Non-interval outcome variable\n",
    "Many of the model assumptions discussed earlier end up violated in cases when there are not a lot of unique values in the outcome variable - e.g. when it is ordinal or categorical. Consider an example where you are predicting what political party someone will join as a function of their SES. With the form of the GLM that we have learned in this course, we could predict something quantitative like position on a liberal/conservative spectrum. But if the outcome variable has instead been recorded as \"Democrat\" and \"Republican\", this is categorical information. There's no \"increase in Y\" we can predict in this case because there aren't quantitative values to this variable.\n",
    "\n",
    "For this reason, the problems with dichotomizing a predictor variable persist when dichotomizing an outcome variable. If at all possible, one should avoid dichotomization/binning at the time of data collection.\n",
    "\n",
    "However, some variables in the world truly are categorical - e.g. whether or not someone dies from an illness, what state they live in, etc. If we try to make *these* variables into quantitative data by labeling them with numbers, we'd be making another error. This would be assuming that there's sortability to these categories, and that values between whole numbers are possible to predict. \n",
    "\n",
    "The solution in this case is to use a more advanced type of modeling procedure. For binary categorical data, use [logistic regression](https://stats.oarc.ucla.edu/r/dae/logit-regression/). For more categories or ordinal outcome data, use [ordinal regression](https://stats.oarc.ucla.edu/r/dae/ordinal-logistic-regression/).\n",
    "\n",
    "\n",
    "### Repeated measures\n",
    "When we collect data, sample size matters in a lot of ways. Things like p-values and error scores are calculating using degrees of freedom. This is because we assume with each new data point, we have more information about the population. Another way of saying this is that we assume data points are *independent* of each other. If you know the data value for one observation, you're not able to predict what the next data point will be. \n",
    "\n",
    "However, data points might not always be independent. Consider a situation where you are interested in a new test anxiety intervention, so you give some students the intervention and others you leave alone as a control group. Throughout the semester, you measure the students' test scores (on 3 separate exams) so that you have 3*N observations in your dataset - one score per test, per student. You want to see if there's a difference between the group averages. \n",
    "\n",
    "If every student took three exams during the semester, do you think those three exam scores are independent of each other? If you knew student A's score on exam 1, would it be easier to guess their score on exam 2 than for an entirely different student? \n",
    "\n",
    "This is an example of what is called **repeated measures**. We took measurements from the same source (one person), and those measurements are probably not the same. But they are likely similar, such that there is more similarity between scores from the same person than there is between scores of different people. We are repeatedly measuring information from the same source, so we don't have completely new information each time. Who these scores come from affects the scores. \n",
    "\n",
    "<img src=\"images/ch19-repeatedmeasures.png\" width=\"800\">\n",
    "\n",
    "If you run a GLM analysis on these data, you are implicitly assuming that there is more information (and more degrees of freedom) in your dataset than is actually available. This will bias standard error estimates downwards, as you think your sample size is bigger than it effectively is. \n",
    "\n",
    "To avoid this issue, we should consider whether there is any shared source that multiple data points in a study are coming from - the same person, same dyad of people, same school, etc. We shouldn't get rid of these repeated measures though by averaging within the source or only taking one data point per person. Oftentimes repeated measures can be a helpful way to get extra statistical power in a study. What we should do instead is use different statistical tools that don't assume independent data. These include the [paired samples t-test](https://www.r-bloggers.com/2021/10/paired-sample-t-test-using-r/) when there is a pre and post score per person; [repeated measures ANOVA](https://www.datanovia.com/en/lessons/repeated-measures-anova-in-r/) when doing a within-subjects design with subjects exposed to multiple conditions; or [mixed-effects modeling](https://m-clark.github.io/mixed-models-with-R/random_intercepts.html) as a more general approach to dealing with repeated measures. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8d5537",
   "metadata": {},
   "source": [
    "## Chapter Summary\n",
    "\n",
    "After reading this chapter, you should be able to:\n",
    "\n",
    "- Explain the difference between error and bias\n",
    "- Explain the difference between biased and unbiased estimators\n",
    "- Describe the assumptions of the general linear model where violations lead to model bias\n",
    "- Check for these assumptions in a dataset\n",
    "- Consider other sources of bias introduced at the time of data collection\n",
    "\n",
    "## New concepts\n",
    "- **bias**: In contrast to error (when predictions or estimates don't match the true values), bias occurs when these predictions/estimates are wrong in a systematic way.\n",
    "- **homoscedasticity**: When the variance in model residuals is consisent for the whole range of model prediction values. \n",
    "- **heteroscedasticity**: When the variance in model residuals varies as a function of the model predicted value. Results in unstable effect estimates in the model. \n",
    "- **leverage**: A data point has high leverage on a model estimate when it's inclusion/exclusion heavily influences what the model estimate is. \n",
    "- **multicollinearity**: A situation when two or more predictors in a model are highly related to each other, and thus not much unique variation can be explained by either. Results in unstable effect estimates for these predictors. \n",
    "- **omitted variable bias**: When bad statistical conclusions are made because some important component of the data generation process was left out of modeling. \n",
    "- **repeated measures**: Multiple datapoints that are collected from the same \"source\" and thus are not wholly independent from each other. \n",
    "\n",
    "## New R functionality\n",
    "- [performance::check_model()](https://easystats.github.io/performance/reference/check_model.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4af6db",
   "metadata": {},
   "source": [
    "[Next: Chapter 20 - Alternate Approaches - Traditional Statistical Tools](https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-20.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
