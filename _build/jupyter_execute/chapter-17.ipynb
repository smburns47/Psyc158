{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82638bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this first so it's ready by the time you need it\n",
    "install.packages(\"readr\")\n",
    "install.packages(\"ggformula\")\n",
    "library(readr)\n",
    "library(ggformula)\n",
    "fingers <- read_csv(\"https://raw.githubusercontent.com/smburns47/Psyc158/main/fingers.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c553a28c",
   "metadata": {},
   "source": [
    "# Chapter 17 - Significance Testing\n",
    "\n",
    "## 17.1 The null hypothesis\n",
    "\n",
    "In the course so far we have discussed, is somewhat vague terms, how we use statistics to test pre-existing hypotheses about data. So far, our hypotheses have only been as specific as \"a predictor explains some of the variance in an outcome.\" But we can define a more formal hypothesis than that. The strictest definition of a hypothesis is a pre-defined statement about what value a model coefficient will have, once we fit the model. Once we have a formal hypothesis about what the population parameter is, we can judge whether or not we think a sample of data likely came from that population or not. \n",
    "\n",
    "For example, rather than hypothesizing generally that someone's height is related to their thumb length, a formal hypothesis would be \"the effect of height for predicting thumb length is 0.9.\" In Frequentist statistics, we assume there is one population parameter from which random samples are derived. So we can collect some data, fit a model, build a confidence interval, and check if the estimated interval includes our hypothesized 0.9 parameter. If it does, then we would say the data is consistent with the proposed population - some data like this is likely to be generated by a population with a coefficient of 0.9. However if 0.9 is not in the confidence interval, we would be very surprised to hear that this data was indeed generated by this parameter. Only a very few rare samples would look like ours, if 0.9 were the true coefficient. We would think it more likely that a different sort of population generated the data.  \n",
    "\n",
    "Try thinking of some formal hypotheses for other research questions. E.g., predicting someone's resting heart rate based on how many hours during the week they spend in the gym. What do you think the specific value of &beta;<sub>1</sub> would be in a model such as heart rate ~ gym hours? \n",
    "\n",
    "If you're having a hard time thinking of where to even start with that question, you wouldn't be alone. A specific hypothesis is really tough to create and requires a level of knowledge about the data generation process that is unpractical for most topics we care about. It's much easier to hypothesize that there is some sort of relationship between gym hours and heart rate, such that the population parameter is something that is NOT 0. \n",
    "\n",
    "This is the crux of how Frequentists test hypotheses, called **Null Hypothesis Significance Testing** (NHST). In this process, your hypothetical thinking skills will be tested: you make an assumption of a formal hypothesis that &beta; = 0. This is the **null hypothesis**, that there is a null effect for &beta;. The null hypothesis name is often abbreviated as H<sub>0</sub>. Then, we collect data, make an estimate, and judge how likely a population with &beta; = 0 is to produce these data. If it's very unlikely, we make a guess that the data were probably generated by some other population: the **alternative hypothesis**. We *reject* the null hypothesis. \n",
    "\n",
    "It's important to remember that an estimate of a coefficient having a non-zero value is not enough to reject the null hypothesis. As we saw last chapter, a population with a specific parameter can generate samples with a range of estimates. So if we get a b = 0.5, Is that different enough from 0 to reject the null hypothesis? Or could that estimate have been reasonably generated by a population with &beta; = 0?\n",
    "\n",
    "To answer this question, we turn to a concept called **statistical significance**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55df30e2",
   "metadata": {},
   "source": [
    "## 17.2 Sampling distribution of an estimate\n",
    "\n",
    "Let's refresh on the concept of a sampling distribution in a concrete example before we learn how test the statistical significance of a sample estimate. Going back to the ```fingers``` dataset, we previously explored how someone's sex would explain some of the variation in thumb lengths. In other words, we thought there was a non-zero difference between the mean of female thumbs and the mean of male thumbs. We didn't have a more specific hypothesis than that, but that \"non-zero\" part is enough to define a null hypothesis and test whether or not these data likely came from a population that *does* have a zero difference between female and male thumbs. \n",
    "\n",
    "In our equation of the statistical model of this question:\n",
    "\n",
    "$$Y_i = b_0 + b_1X_i + e_i $$\n",
    "\n",
    "X<sub>i</sub> represents whether or not someone is male or female. The parameter estimate b<sub>1</sub> is the difference in thumb lengths between males and females, and is thus the specific one we are interested in. This is our best estimate of &beta;<sub>1</sub>, the true effect of sex in the population of thumb lengths.\n",
    "\n",
    "Before we fit this model again and get an estimate, let’s imagine what we would expect to see if a particular version of the data generation process were true. If there is a real difference between male and female thumb lengths such that male thumbs were generally longer (i.e., if &beta;<sub>1</sub> is a positive value), we might expect that samples from such a population would have positive b<sub>1</sub>s on average in the sampling distribution.\n",
    "\n",
    "Although we couldn’t predict any single b<sub>1</sub> that will be drawn from the population, we can make predictions about the average b<sub>1</sub> that would be generated from multiple random samples. On average, the b<sub>1</sub>s would tend to resemble the parent &beta;<sub>1</sub> from which they come. So a negative &beta;<sub>1</sub> would tend to produce negative b<sub>1</sub>s, a positive &beta;<sub>1</sub>, positive b<sub>1</sub>s.\n",
    "\n",
    "The null hypothesis is a special case in which &beta;<sub>1</sub> = 0. If the null hypothesis is true it means that someone's sex has no effect on their thumb length. The b<sub>1</sub>s generated by multiple random samples from a population in which &beta;<sub>1</sub> = 0 would tend to cluster around 0, but they wouldn’t necessarily be exactly 0. We can construct a sampling distribution to find out if our sample b<sub>1</sub> could have been generated by the null hypothesis.\n",
    "\n",
    "We can do this with our own data with a version of bootstrapping called **permutation testing**. In this, we *permute* or shuffle around the datapoints in the predictor variable, thus breaking any relationship between a predictor and the outcome. We can use ```sample()``` to do this, drawing all the datapoints in a variable randomly without replacement and then making a new variable out of that randomized vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f01aa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "head(fingers$Sex)\n",
    "\n",
    "shuffled_Sex <- sample(x=fingers$Sex, size=length(fingers$Sex), replace=FALSE)\n",
    "head(shuffled_Sex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f03a18",
   "metadata": {},
   "source": [
    "Breaking any association between ```Sex``` and ```Thumb``` by shuffling the values of ```Sex``` around makes it so that any still existing relationship between those variables is only due to randomness - the true association, or true &beta;<sub>1</sub>, is 0. Doing this many times and collecting the sample estimates of models fit on these shuffled data will give us a sampling distribution of b<sub>1</sub>s when the null hypothesis is true. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0228d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating an empty vector of 1000 spots\n",
    "null_b1s <- vector(length=1000)\n",
    "\n",
    "#generate 1000 unique samples, saving each b1\n",
    "for (i in 1:1000) {\n",
    "    fingers$shuffled_Sex <- sample(x=fingers$Sex, size=length(fingers$Sex), replace=FALSE)\n",
    "    model <- lm(Thumb ~ shuffled_Sex, data=fingers)\n",
    "    null_b1s[i] <- model$coefficients[[2]]\n",
    "}\n",
    "\n",
    "b1s_df <- data.frame(null_b1s)\n",
    "gf_histogram( ~ null_b1s, data=b1s_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fd99df",
   "metadata": {},
   "source": [
    "In this sampling distribution, we can see that the b<sub>1</sub>s vary each time we shuffle and calculate a new b<sub>1</sub>. Also, the center seems to be around 0. We know from last chapter that the mean of a sampling distribution converges on the population parameter. Because the sampling distribution is based on the null hypothesis, where &beta;<sub>1</sub> = 0, we expect the parameter estimates to be clustered around 0. But we also expect each individual one to vary because of sampling variation. We can see here that it's possible to get a b<sub>1</sub> almost as high as +/-6 just by chance, even when there is no true difference between female and male thumbs!\n",
    "\n",
    "However, these values are really rare in the sampling distribution. It's much more common to generate small mean differences (e.g., 0.6). \n",
    "\n",
    "Just eyeballing the histogram can give us a rough idea of the probability of getting a particular sample b<sub>1</sub> from this population where we know &beta;<sub>1</sub> is equal to 0. When we use these frequencies to estimate probability, we are using this distribution of shuffled b<sub>1</sub>s as a probability distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89f6036",
   "metadata": {},
   "source": [
    "## 17.3 Significance testing\n",
    "\n",
    "We used R to simulate a world where the null hypothesis is true in order to construct a sampling distribution. Now let’s return to our original goal, to decide whether the null hypothesis is a good explanation for our data, or if it should be rejected.\n",
    "\n",
    "The basic idea is this: using the sampling distribution of possible sample b<sub>1</sub>s that could have resulted from a population in which the null hypothesis is true (i.e., in which &beta;<sub>1</sub> = 0), we can look at a specific sample b<sub>1</sub> that we estimated with data and gauge how likely such a b<sub>1</sub>s would be if the null hypothesis is, in fact, true.\n",
    "\n",
    "If we judge the b<sub>1</sub> we observed to be unlikely to have come from the null hypothesis, we then reject H<sub>0</sub> as our specific idea about the data generation process. If, on the other hand, we judge our observed b<sub>1</sub> to be likely (or at least not all that *unlikely*), then the null hypothesis could still be a good explanation for these data.\n",
    "\n",
    "Let’s see how this works in the context of the ```fingers``` dataset, where b<sub>1</sub> represents the average difference in thumb lengths between males and females.\n",
    "\n",
    "Samples that are extreme in either a positive (average male thumbs much longer than females) or negative direction (average male thumbs much shorter than females), are unlikely to be generated if the true &beta;<sub>1</sub> = 0. If we saw a sample b<sub>1</sub> like this, we would doubt that the null hypothesis produced it.\n",
    "\n",
    "Put another way: if we had a sample that fell in either the extreme upper tail or extreme lower tail of the nul sampling distribution (see figure below), we might reject the null hypothesis as the true picture of the data generation process.\n",
    "\n",
    "<img src=\"images/ch17-samplingtails.png\" width=\"600\">\n",
    "\n",
    "In statistics, this is commonly referred to as a **two-tailed significance test** because whether our actual sample falls in the extreme upper tail or extreme lower tail of this sampling distribution, we would have reason to reject the null hypothesis as the true version of the population. By rejecting the null hypothesis, we decide instead that some alternative hypothesis where &beta;<sub>1</sub> != 0 must be true. We wouldn’t know exactly what the true &beta;<sub>1</sub> is, but only that it is probably not 0. In more traditional statistical terms, we would have found a statistically significant difference between male thumb lengths and female thumb lengths. b<sub>1</sub> is significantly different from 0. \n",
    "\n",
    "All of this, however, begs the question of how extreme a sample b<sub>1</sub> would need to be in order for us to reject the null hypothesis. What is unlikely to one person might not seem so unlikely to another person. It would help to have some sort of agreed upon standard of “what counts as unlikely” before we actually bring in our real sample statistic. The definition of “unlikely” depends on what you are trying to do with your statistical model and what your community of practice agrees on.\n",
    "\n",
    "One common standard used in the social sciences is that a sample counts as unlikely if there is less than a .05 chance of generating a sample estimate at least as extreme as this one (either negative or positive) in the null sampling distribution. We notate this numerical definition of “unlikely” with the Greek letter &alpha; (pronounced “alpha”). A scientist might describe this criterion by writing that they “set &alpha; = .05”. If they wanted to use a stricter definition of unlikely, they might say “&alpha; = .001,” indicating that a sample would have to be really unlikely for us to reject the null hypothesis.\n",
    "\n",
    "Let’s identify an &alpha;-level of .05 in the null sampling distribution of b<sub>1</sub>s we generated from random shuffles of thumb lengths. If you take the 1000 b<sub>1</sub>s and line them up in order, the .025 lowest values and the .025 highest values would be the most extreme 5% of values and therefore the most unlikely values to be randomly generated. So let's sort our null distribution and then find the value corresponding to the 2.5%ile and 97.5%ile. Since there are 1,000 data points in our simulated null sampling distribution, this would be the 25th and 975th values of the sorted vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92302184",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#cut-off values for extremeness\n",
    "sorted_nullb1 <- sort(null_b1s)\n",
    "high_cutoff <- sorted_nullb1[975]\n",
    "low_cutoff <- sorted_nullb1[25]\n",
    "high_cutoff\n",
    "low_cutoff\n",
    "\n",
    "#marking something as extreme if it is greater than 97.5%ile or less than 2.5%ile\n",
    "b1s_df$extreme <- b1s_df$null_b1s > high_cutoff | b1s_df$null_b1s < low_cutoff\n",
    "\n",
    "gf_histogram(~ null_b1s, data = b1s_df, fill = ~extreme)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c124483",
   "metadata": {},
   "source": [
    "You might be wondering why some of the bars of the histogram include both red and blue. This is because the data in a histogram is grouped into bins, and some bins might contain values both just inside and just outside the extremeness cutoff. \n",
    "\n",
    "If you would like to see a more sharp delineation, you could try making more unique bins. Doing so would increase the chances of having just one color in each bin.\n",
    "\n",
    "In the code window below, change the bin number to something large, like 100 (the default number of bins is 30).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789a6dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gf_histogram(~ null_b1s, data = b1s_df, fill = ~extreme, bins= #YOUR CODE HERE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5172edb1",
   "metadata": {},
   "source": [
    "Another neat trick to know is, if we convert a sampling distribution to z-scores, we automatically know which samples are extreme or not by looking at which z-scores are above 1.96 or below -1.96, since 1 in z-scored units corresponds to 1 standard deviation and the 2.5%ile and 97%ile values are 1.96 SDs away from the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937079a0",
   "metadata": {},
   "source": [
    "## 17.4 The p-value\n",
    "\n",
    "We have now spent some time looking at the sampling distribution of b<sub>1</sub>s assuming the null hypothesis is true (i.e., &beta;<sub>1</sub> = 0). We have developed the idea that simulated samples, generated by random shuffles of the thumb length data, are typically clustered around 0. Samples that end up in the tails of the distribution – the upper and lower .025 of values – are considered unlikely.\n",
    "\n",
    "Let’s place our sample b<sub>1</sub> right onto our histogram of the sampling distribution and see where it falls. Does it fall in the tails of the distribution, or in the middle .95?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451b0e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_model <- lm(Thumb ~ Sex, data=fingers)\n",
    "b1 <- sex_model$coefficients[[2]]\n",
    "\n",
    "gf_histogram(~ null_b1s, data = b1s_df, fill = ~extreme, bins=100) %>% \n",
    "gf_point(x = b1, y = 0, size = 3) %>% \n",
    "gf_refine(coord_cartesian(x=c(-7,7))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9774bef",
   "metadata": {},
   "source": [
    "We can see that our sample is in the unlikely zone. It is beyond the point where the histogram of the null sampling distribution switches to blue extreme values. Thus we would reject that it comes from a population where male and female thumb lengths are the same, and say that our b<sub>1</sub> estimate is significantly different than 0. \n",
    "\n",
    "But we can say it more specifically than this, and give its location in the tail a quantitative score. Instead of only asking whether the sample b<sub>1</sub> is in the unlikely area or not (yes or no), we could instead ask, what is the probability of getting a b<sub>1</sub> as extreme or more extreme as the one observed in the actual experiment, if the null hypothesis were true? The answer to this question is called the **p-value**.\n",
    "\n",
    "We know what our &alpha; is before we even do a study – it’s just a statement of our criterion for deciding what we will count as unlikely. Whatever the values in the null sampling distribution end up being, we define the \"extreme\" ones as the top and bottom 2.5% of the distribution. The p-value is calculated after we do a study, based on actual sample data. The p-value is calculated based on both the value of the sample estimate we want to assign a p-value, and the shape of the null sampling distribution of the parameter estimate under the null hypothesis. \n",
    "\n",
    "We calculate the p-value of any potential b<sub>1</sub> value as the probability that we would produce a b<sub>1</sub> as extreme or more extreme than this one when the null hypothesis is true. Since our &alpha; = 0.05, any p-value < 0.05 would be considered significant. \n",
    "\n",
    "We can use the null sampling distribution itself for computing the p-value by counting up all the values in the distribution that are more extreme than our estimated b<sub>1</sub>. Let's take, for instance, a value of b<sub>1</sub> = 3. What is its p-value? Well first, let's count how many values in the null sampling distribution are greater than 3 or less than -3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66492913",
   "metadata": {},
   "outputs": [],
   "source": [
    "#boolean where 1 = value is greater than 3\n",
    "more_positive <- null_b1s > 3\n",
    "num_more_positive <- sum(more_positive)\n",
    "\n",
    "#boolean where 1 = value is less than -3\n",
    "more_negative <- null_b1s < -3\n",
    "num_more_negative <- sum(more_negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f09d53",
   "metadata": {},
   "source": [
    "Second, we can find what proportion of the entire null sampling distribution is more extreme than this value of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a446e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "(num_more_positive + num_more_negative) / length(null_b1s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8db48c",
   "metadata": {},
   "source": [
    "The probability that a b<sub>1</sub> value coming from the null sampling distribution is more extreme than +/- 3 is 0.061. Thus, a b<sub>1</sub> of 3 would have a p-value of 0.061. \n",
    "\n",
    "In the code window above, try changing the value of 3 to some other numbers to see what the p-value of those would be. \n",
    "\n",
    "Now let's check our actual b<sub>1</sub> value of 6.447. What is its p-value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df580a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "more_positive <- null_b1s > 6.447\n",
    "num_more_positive <- sum(more_positive)\n",
    "\n",
    "more_negative <- null_b1s < -6.447\n",
    "num_more_negative <- sum(more_negative)\n",
    "\n",
    "(num_more_positive + num_more_negative) / length(null_b1s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7003518",
   "metadata": {},
   "source": [
    "Nothing in the null sampling distribution is more extreme than 6.447. Thus, it looks like its p-value is 0. Clearly significant! \n",
    "\n",
    "In actuality, you can't truly have a p-value of 0. If you drew infinitely many samples to create the null sampling distribution, there would be tiny but existing tails to the distribution that go on forever (it approaches the theoretical shape of the normal distribution). We just aren't able to get tiny enough to calculate the p-value in a null sampling distribution of only 1,000 samples.\n",
    "\n",
    "R can find the exact p-value for us. Simply use the ```summary()``` function again applied to a model object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379d5646",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(sex_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339f802b",
   "metadata": {},
   "source": [
    "Look at the last column in the Coefficients table called \"Pr(>|t|)\". This is the probability of getting a value more extreme than our b<sub>1</sub> estimate under the null hypothesis - aka the p-value. You can see it is not 0, but it is still a very small number. It's so small, R has to represent it with scientific notation (1.79 \\* 10<sup>-5</sup>). Clearly smaller than our 0.05 &alpha; criterion.\n",
    "\n",
    "You'll see the intercept has a p-value too, also a tiny number. This is saying that we can reject the null hypothesis that &beta;<sub>0</sub> is 0. All fitted parameters in a linear model get a p-value, but you might not care to evaluate all of these parameters if your research question is about a particular one. \n",
    "\n",
    "Something else to notice as well is the confidence interval for b<sub>1</sub>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dac427",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "confint(sex_model, \"Sexmale\", level=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54977aee",
   "metadata": {},
   "source": [
    "This suggests that we are 95% confident that the true difference between male and female thumb lengths is somewhere between 3.57mm and 9.32mm. This interval does not include 0, so we are 95% confident that the null hypothesis is not the parameter that made these data. \n",
    "\n",
    "A confidence interval tells you the same thing as a p-value, just in a different way. If the interval does not include 0, we decide that this estimate is significantly different than 0. If it does include 0, then 0 might be the parameter that created these data and we fail to reject the null hypothesis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357414fc",
   "metadata": {},
   "source": [
    "## 17.5 The t-distribution\n",
    "\n",
    "The early statisticians who developed the ideas behind sampling distributions and p-values didn’t have computers. They could only imagine what it might be like to shuffle their data a thousand times in just a couple seconds. What we have been able to do with R would seem like a miracle to them! So instead of using computational techniques to create sampling distributions, the early statisticians had to develop mathematical formulas of what the sampling distributions should look like, and then calculate probabilities based on these mathematical distributions. These are what the ```summary()``` function uses to give us exact p-values, so we'll learn a little about that now to understand where these numbers come from.\n",
    "\n",
    "The mathematical concept that ```summary()``` uses to model the sampling distribution of coefficient estimates is known as the **t-distribution**. The t-distribution is closely related to the normal distribution, and in fact it looks very much like the normal distribution.\n",
    "\n",
    "In the figure below we have overlaid the t-distribution (depicted as a red line) on top of a sampling distribution constructed with permutation testing. You can see that it looks very much like the normal distribution you learned about previously.\n",
    "\n",
    "<img src=\"images/ch17-nulldist.png\" width=\"600\">\n",
    "\n",
    "While the sampling distribution we created using permutation testing looks jagged (because it was made up of just 1000 separate estimates), the t-distribution is a smooth continuous mathematical function. It is the theoretically idealized shape of the null sampling distribution. If you want to see the equation that describes this shape, you can see it [here](https://mathworld.wolfram.com/Studentst-Distribution.html).\n",
    "\n",
    "Whereas the shape of the normal distribution is completely determined by its mean and standard deviation, the t-distribution changes shape slightly depending on how many degrees of freedom are in the samples that make up the sampling distribution. You can see how degrees of freedom affect the shape of the t-distribution in the figure below. Once the degrees of freedom reach about 30, however, the t-distribution looks very similar to the normal distribution.\n",
    "\n",
    "<img src=\"images/ch17-tdist.png\" width=\"600\">\n",
    "\n",
    "In the null sampling distribution you created you were able to just count the number of estimates more extreme than the sample b<sub>1</sub> in order to calculate the p-value. The t-distribution works the same way, except that it finds the cumulative probabilities in the upper and lower tails. I.e., the area under the curve that is more extreme than our sample b<sub>1</sub>. Fortunately, you don’t have to do this math; R will do it for you in ```summary()```. But you *could* do it with probability sampling functions like [these](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/TDist.html), which work the same way as ```rnorm()``` or ```pnorm()``` that we learned in chapter 8. \n",
    "\n",
    "These functions would give you a **t-value**, telling you where on the t-distribution your sample b<sub>1</sub> falls if the null hypothesis is true. This specific t-value can be found in the third column of the Coefficients table in the ```summary()``` output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a1286f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write some code to again output the bigger results table for sex_model, not just the coefficients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d7e1d7",
   "metadata": {},
   "source": [
    "Because this is the traditional method of determining p-values and statistical significance, sometimes you'll read research papers where the authors ran a linear model but also report a t-value. This does not mean that they did a t-test, only that they're reporting the corresponding t-value of their beta estimate that is found in the linear model output. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4014a9bc",
   "metadata": {},
   "source": [
    "## 17.5 Things that affect the p-value\n",
    "\n",
    "Back in the simple model, the sample b<sub>1</sub> for the difference between male and female thumb lengths was 6.447. Based on the the output of ```summary()```, the probability of getting a sample with a b<sub>1</sub> as extreme or more extreme than 6.447 when the null hypothesis is true is approximately 1.79e-05. Based on our &alpha; criterion of .05, we decided that b<sub>1</sub> = 6.447 is significantly different from 0, and we reject the null hypothesis. Some alternative population likely make these data. \n",
    "\n",
    "Compare that to a b<sub>1</sub> = 3. For this, we used permutation testing to find that this value would have a p-value of 0.061. This is larger than the &alpha; value of 0.05, so we fail to reject the null hypothesis. These data are not so unlikely that we think the null hypothesis didn't generate it. \n",
    "\n",
    "As evidenced here, the p-value is affected by how far the observed b<sub>1</sub> is from 0. Since 6.447 is further away from 0 than 3 is from 0, b<sub>1</sub> = 6.447 has a smaller p-value. The further away the b<sub>1</sub> is from 0, the lower the p-value, meaning the less likely the observed b<sub>1</sub> is to have been produced by the null hypothesis.\n",
    "\n",
    "But the distance between b<sub>1</sub> and 0 (or the hypothesized &beta;<sub>1</sub>) is not the only thing that affects a p-value. The other important factor is the width of the sampling distribution, also known as the standard error.\n",
    "\n",
    "Take a look at the two simulated sampling distributions in the figure below. The one on the right is something like what we created in our permutation test earlier. The one on the left is similar, but wider. Both have a roughly normal shape, both consist of 1000 estimates, and both distributions are centered at 0. But the standard error is smaller for the distribution on the right. By being wider, the sampling distribution on the left pushes the extreme zone farther out, and makes our b<sub>1</sub> value of 6.447 no longer significant. \n",
    "\n",
    "<img src=\"images/ch17-stderror.png\" width=\"850\">\n",
    "\n",
    "The standard error can make a big difference in our ability to reject the null hypothesis. If it is smaller, we will have an easier time rejecting the null, because whatever estimate we get for b<sub>1</sub>, it will be more likely to be in the upper or lower .025 of the null sampling distribution.\n",
    "\n",
    "We learned last chapter that the size of the standard error is tied to the size of the samples within it. Samples with fewer data will have more varied parameter estimates, and thus the sampling distribution will have a wider standard error. \n",
    "\n",
    "Our ```fingers``` dataset has 157 data points in it, which is a fine sample size. But let's see what would happen if it were a much smaller study, with only 30 people. First we'll do a permutation test, but only drawing shuffled samples of N=30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7afa40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#creating an empty vector of 1000 spots\n",
    "null_b1s_n30 <- vector(length=1000)\n",
    "\n",
    "#generate 1000 unique samples of N=30, saving each b1\n",
    "for (i in 1:1000) {\n",
    "    shuffled_Thumb <- sample(x=fingers$Thumb, size=30, replace=FALSE)\n",
    "    Sex <- sample(x=fingers$Sex, size=30, replace=FALSE)\n",
    "    small_df <- data.frame(shuffled_Thumb, Sex)\n",
    "    model <- lm(shuffled_Thumb ~ Sex, data=small_df)\n",
    "    null_b1s_n30[i] <- model$coefficients[[2]]\n",
    "}\n",
    "\n",
    "b1s_df$null_b1s_n30 <- null_b1s_n30\n",
    "gf_histogram( ~ null_b1s_n30, data=b1s_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06cc5ba",
   "metadata": {},
   "source": [
    "This null sampling distribution is much wider than we saw before. Now, the tails go on past 10 and -10. The extreme values got more extreme. If we now try to calculate the p-value of b<sub>1</sub> = 6.447 in this sampling distribution: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d247a4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "more_positive <- null_b1s_n30 > 6.447\n",
    "num_more_positive <- sum(more_positive)\n",
    "\n",
    "more_negative <- null_b1s_n30 < -6.447\n",
    "num_more_negative <- sum(more_negative)\n",
    "\n",
    "(num_more_positive + num_more_negative) / length(null_b1s_n30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebafe1ac",
   "metadata": {},
   "source": [
    "This estimate would not be significant.\n",
    "\n",
    "Sample size is thus directly tied to our ability to reject the null hypothesis, independent of what our b<sub>1</sub> estimate actually is. Running a bigger study makes the null sampling distribution narrower, and thus it is easier to reject the null hypothesis. This is relevant for when we think the true value of a coefficient is something small or close to 0. If we want to get a good estimate of it (narrow confidence interval) and be confident it is not 0 (p < 0.05), we need to collect a lot of data. \n",
    "\n",
    "But this doesn't mean that if we get a small p-value with a small study, that we're in the clear. Coefficient estimates in general are less stable when sample sizes are small, meaning the difference between whether something is significant or not could be the inclusion/exclusion of one extreme data point, or some other small modeling choice. Fishing around for the perfect configuration of data that makes your results significant is called **p-hacking**, and increases the chance that you reject the null hypothesis when it is actually true (more on this in chapter 22). \n",
    "\n",
    "We can also take the relationship between p-values and sample size to absurd limits, to where it may be possible to collect *too much* data. Now let's sample giant datasets, of one hundred thousand data points. We'll use ```replace=TRUE``` in the sampling function or else we would run out of datapoints to use for permutation testing. Because the samples are so big, it may take a few moments for this code to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cbb6b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#creating an empty vector of 1000 spots\n",
    "null_b1s_100k <- vector(length=1000)\n",
    "\n",
    "#generate 1000 unique samples of N=100000, saving each b1\n",
    "for (i in 1:1000) {\n",
    "    shuffled_Thumb <- sample(x=fingers$Thumb, size=100000, replace=TRUE)\n",
    "    Sex <- sample(x=fingers$Sex, size=100000, replace=TRUE)\n",
    "    big_df <- data.frame(shuffled_Thumb, Sex)\n",
    "    model <- lm(shuffled_Thumb ~ Sex, data=big_df)\n",
    "    null_b1s_100k[i] <- model$coefficients[[2]]\n",
    "}\n",
    "\n",
    "b1s_df$null_b1s_100k <- null_b1s_100k\n",
    "gf_histogram( ~ null_b1s_100k, data=b1s_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f942b5c1",
   "metadata": {},
   "source": [
    "The null sampling distribution in this case is much narrower. Even a mean difference in thumb lengths as small as 0.2mm would be considered significantly different than 0. \n",
    "\n",
    "In very large datasets, nearly every model coefficient you estimate will be statistically significant. Significance then becomes a less useful concept. Sure 0.2mm is significantly different from 0 when it's estimated from 100k datapoints, but how much does a difference that tiny matter to us for *using* the model? Do we care that male and female thumb lengths would be different by only 0.2mm? Or is that so small that it's no longer *practically* different than 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06724267",
   "metadata": {},
   "source": [
    "## 17.6 The limits of significance testing\n",
    "\n",
    "Significance testing is a major component of modern psychology research. Hypotheses and theories are made or broken on the back of p < 0.05. But the strong reliance on p-values for determining whether effects exist or not is not always a good thing. There are several limits to what conclusions we can make using significance testing, and sometimes people push past these limits.  \n",
    "\n",
    "### Type I and Type II error\n",
    "First, we typically set our &alpha; criterion to be 0.05, meaning that any b<sub>1</sub> value that is as extreme or more extreme than the 5% most extreme values in the null sampling distribution will be treated as unlikely. For unlikely estimates, we decide that they are probably not from the null hypothesis at all. This could be the right decision…\n",
    "\n",
    "But it might be the wrong decision. If the null hypothesis *is* true, 5% of the b<sub>1</sub>s that could appear would be extreme enough to lead us to reject the null hypothesis. We would be incorrectly deciding that these data came from a different distribution than they actually did. If we rejected the null hypothesis when it is in fact true, we would be making what's called a **Type I error** or **false positive**. \n",
    "\n",
    "Our chance of making this type of inference error is directly tied to the &alpha; level we chose. By setting &alpha; = .05, we are saying that b<sub>1</sub> values above or below 1.96 SDs of the null sampling distribution are so unlikely that we think they're from a different population. But by definition, 5% of the b<sub>1</sub> values in the &beta;<sub>1</sub> = 0 sampling distribution are this level of unlikely. Thus when the null hypothesis is true, 5% of samples drawn from the population will lead us to the wrong conclusion.\n",
    "\n",
    "We can reduce this error rate by making our &alpha; value smaller. Maybe instead of 5% of the null sampling distribution being surprising, we set &alpha; = 0.001 such that only 0.1% of the null sampling distribution would be considered unlikely. This would make it harder for us to erroneously reject the null hypothesis. \n",
    "\n",
    "However, that causes us other problems. Now, it is harder to detect when the null hypothesis *should* be rejected. We need much stronger evidence to do so when &alpha; = 0.001 than when &alpha; = 0.05. If we fail to reject the null hypothesis when it should be rejected, this is called a **Type II error** or a **false negative**. Type I and Type II errors are always a tradeoff with each other - where we reduce the chance of one, we increase the chance of the other. Also, it is never possible to completely eliminate the risk of either. This is an inherent limitation of Null Hypothesis Significance Testing. \n",
    "\n",
    "### Can only reject the null, not prove it\n",
    "A second limitation of NHST is that, while it is possible to declare a sample estimate as too unlikely for us to think it comes from the null hypothesis, it is not possible to be sure an estimate definitely *does* come from the null hypothesis. To be concrete, what if a b<sub>1</sub> estimate doesn’t fall in the tails of the null sampling distribution but instead falls in the middle part? Should we then call it “likely”? In this case, we can say that the sample is *not unlikely*. But you can't say it is likely. In this framework, we can never *confirm* the null hypothesis, only reject it or fail to reject it. This is because there are infinite other population parameters slightly above or below 0 that could generate this data as well. You can't prove that any estimate came from &beta;<sub>1</sub> = 0 specifically and not &beta;<sub>1</sub> = 0.1. You can only say there is a very high likelihood that something did *not* come from a particular population like &beta;<sub>1</sub> = 0. Null hypothesis testing is about rejecting some reality, not confirming one. If we fail to reject the null hypothesis, the true population parameter *could* be 0, but it could also be something else sort of close to 0. \n",
    "\n",
    "### Significance is a binary decision\n",
    "Thirdly, another limitation of NHST is that there is no way to compare amongst different levels of extremeness in terms of their statistical significance. We can compute different p-values for different b<sub>1</sub> estimates and find that one has a higher probability to appear under the null hypothesis than another. But the *decision* of something being significantly different from 0 is a binary decision - it either is significant, or it is not. Something that is in the top 2% of the null sampling distribution is just as significant as something that is in the top 0.02% of the distribution, because both are beyond the alpha cutoff. Null hypothesis testing doesn't give us a way to test whether different p-values are significantly different from *each other*. Thus a sample estimate is either significant, or it's not. We decide it either didn't come from the null hypothesis, or there's not enough evidence to decide. You can't have one estimate that is *more* or *less* significant than another, and you can't have something that is *almost* significant.\n",
    "\n",
    "### Probability of data, not population\n",
    "Fourthly, the specific meaning of a p-value can be tricky to get right. It is the probability that, given the null hypothesis being true, we would find a sample value as extreme or more extreme than the one we did. A p-value does NOT mean the probability that the null hypothesis *is* true. Remember that in Frequentist statistics, a population parameter cannot have a probability - it exists as a single entity that can't be repeated multiple times like a sample. The null hypothesis thus does not have a probability. *These data* have a probability, given the null hypothesis being true.\n",
    "\n",
    "### P-values and sample size\n",
    "Lastly, as we explored previously, the p-value we get is tied to our sample size. In small amounts of data, we would need to estimate a really large effect in order to call it significant, because the confidence interval is so wide that it covers many values including 0. Thus when we only have a small amount of data available to us, we might not be able to declare anything as significant. But in large amounts of data, almost everything is significant, even effects so tiny that they're not useful in the real world. Would it matter to you to hear that a predictor in a model was significant, but the model only explained 0.01% of the variation in the outcome data? P-values can thus be \"gamed\" a bit with large sample sizes. They tell us something about the certainty of this true estimate being 0 or not, but they don't help us with determining whether this effect size is useful for real-world purposes. \n",
    "\n",
    "In summary, there are five main hang-ups that cause a lot of consternation for people trying to use null hypothesis testing:\n",
    "\n",
    "- 1) The risk of Type I and Type II error is everpresent\n",
    "- 2) You can only reject the null hypothesis, or fail to reject the null hypothesis (not confirm the null hypothesis)\n",
    "- 3) You can only decide if a sample estimate is significantly different from 0 or not; there are no levels of significance\n",
    "- 4) A p-value is the probability of these data given the null hypothesis, NOT the probability of the null hypothesis given these data. \n",
    "- 5) P-values are tied to sample sizes, so it is possible to get a significant p-value with an impractically-tiny effect. \n",
    "\n",
    "This is a long section on the difficulties of null hypothesis testing because [a lot of people use it incorrectly](https://statisticsbyjim.com/hypothesis-testing/p-values-misinterpreted/), even long-practicing scientists. NHST is a powerful tool but it's tricky to interpret correctly, and it often gets used for the wrong questions that it is just not able to answer. \n",
    "\n",
    "Thus if you default to only ever looking at p-values in your research and not the wider context of what your model looks like and what goals you have for it, you run the risk of misinterpreting your statistics. Just because a p-value is significant does not mean your model can be used in the way you want to, and just because a p-value is insignificant doesn't mean your model is useless. We'll cover the idea of *practical* vs. statistical significance in a later chapter but for now, remember that p-values aren't the whole story! We also want to judge what sort of predictions are our models making, do we like that level of accuracy or not, and is there any way we can think of to make our model better.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267ebc4b",
   "metadata": {},
   "source": [
    "## 17.7 Significant effects in practice \n",
    "\n",
    "\n",
    "### Effects in the simple model\n",
    "Let's practice using and interpreting p-values with some concrete research situations. One hypothesis we have used a lot is whether someone's sex explains some of the variation in their thumb length. A research question like this implies that we are interested in the predictor \"sex\" specifically, and its unique contribution to predictions about thumb length. We recognize that there are likely many factors that lead to how long someone's thumb is, and we may even know some more of them and be able to model them as well. But our interests focus on how important someone's sex is specifically in the data generation process. Is it a meaningful contributor to explaining variation in thumb length? Or is there not really an effect? \n",
    "\n",
    "Using the framework of Null Hypothesis Statistical Testing, we can answer this sort of question. We just have to go through five steps in order to frame this question correctly for the NHST. \n",
    "\n",
    "First, we need to pick the variables we're using to test this question. We'll use the outcome variable ```Thumb``` from the ```fingers``` dataset, as we have been, and we'll use ```Sex``` to predict it. \n",
    "\n",
    "Second, we need to formulate the null hypothesis. We think that sex is interesting if it predicts some variation in thumb length - that there is a relationship between the two. That implies that there would be a non-zero coefficient for ```Sex``` in a linear model. It would *not* be an interesting variable if there was no change in predicted thumb length when sex varied - i.e., if the b<sub>1</sub> coefficient were 0. Thus, we define the null hypothesis to be: \n",
    "\n",
    "$$H_0: b_1 = 0$$\n",
    "\n",
    "In a world where the true population parameter &beta;<sub>1</sub> is 0, where there is no real effect of sex, we wouldn't be so interested in it as a predictor. So we want to test if it's likely that b<sub>1</sub> in our sample came from a world where &beta;<sub>1</sub> is 0, or if we want to reject that explanation. \n",
    "\n",
    "In step 3, we now need to fit a model in the data. We need to specify the equation of that model so we know what we're testing: \n",
    "\n",
    "$$\\hat{Y} = b_0 + b_1X_i$$\n",
    "\n",
    "Where X<sub>i</sub> is ```Sex```. Knowing that equation, we automatically fit it with ```lm()``` as we have done many times before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5d0651",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_obj <- lm(Thumb ~ Sex, data = fingers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bc5811",
   "metadata": {},
   "source": [
    "From this model, we can extract our estimate of b<sub>1</sub>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087b4f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_obj$coefficients[[2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70328665",
   "metadata": {},
   "source": [
    "Fourthly, we determine the probability of getting a b<sub>1</sub> like this, were the null hypothesis true - if &beta;<sub>1</sub> truly equals 0. Use ```summary()``` to find this p-value for the b<sub>1</sub> coefficient: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08b0e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use summary() to see the p-value of the effect of Sex\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5959bc15",
   "metadata": {},
   "source": [
    "And as our final step, we look at that p-value and decide whether or not it is significant - whether or not we should reject the hypothetical world of &beta;<sub>1</sub> = 0. As psychologists we are likely using an &alpha; criterion of 0.05, so we will check if the p-value for b<sub>1</sub> is < 0.05.\n",
    "\n",
    "We can look in the output of ```summary()```, or we can directly output it with the ```$``` operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39bd8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model_obj)$coefficients[2,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af41926",
   "metadata": {},
   "source": [
    "The code above uses the ```summary()``` output as an object itself, digs into the Coefficients table, and ouputs the number on the second row, fourth column. As we saw in the ```summary()``` output, that would be the number for ```Pr(>|t|)``` on the row for ```Sexmale```. \n",
    "\n",
    "Is this value < 0.05?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c6ec89",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model_obj)$coefficients[2,4] < 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1474380c",
   "metadata": {},
   "source": [
    "It is. Thus, we reject the null hypothesis that there is no effect of sex. \n",
    "\n",
    "Further, if we find the 95% confidence interval for b<sub>1</sub>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca6354c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use confint() to find 95% CI of Sexmale estimate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3df1d4",
   "metadata": {},
   "source": [
    "We see that the interval includes only positive values. Thus we reject the null hypothesis that there is no effect of sex, AND we say the likely range of true &beta;<sub>1</sub>s is a positive number. As someone's sex label switches from 0 to 1 (difference from female to male), the change in thumb length is likely to increase as well. \n",
    "\n",
    "Congrats, you have tested a hypothesis and come up with a conclusion! You can now write up your study and send it for peer review! In APA style, we would write this result as:\n",
    "\n",
    "\"There is a significant effect of sex (*b* = 6.447, *p* <0.001, 95% CI [3.571, 9.323]).\"\n",
    "\n",
    "By following these five steps, you were able to fit an evaluate a model of the data generation process for thumb length. Just remember that the conclusions we can make about this research question are limited, when we use null hypothesis testing. In the context of this analysis: \n",
    "\n",
    "- We can't be *sure* that &beta;<sub>1</sub> doesn't equal 0 - we're only very confident.\n",
    "- If our p-value had been > 0.05, we couldn't say that we're sure &beta;<sub>1</sub> equals 0, because we can't confirm the null hypothesis - only reject it or fail to reject it. \n",
    "- If our p-value had been 0.06, that would still be an insignificant effect. We set our &alpha; decision criterion to be 0.05, and we need to stick to that. \n",
    "- Our p-value is the probability that we would get this b<sub>1</sub> estimate if &beta;<sub>1</sub> = 0; it is NOT the probability that &beta;<sub>1</sub> = 0 if we get this b<sub>1</sub> estimate. Those statements are different conditional probabilities and thus different numbers (and a Frequentist would say you can't get a probability of &beta;<sub>1</sub> anyways). \n",
    "- Our p-value is tied to our sample size N=157. If we had more or less data, we might have made a different decision. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c8b5d2",
   "metadata": {},
   "source": [
    "### Effects in the multivariable model\n",
    "As we saw in chapter 13, predictors in a multivariable model can be correlated. Thus if we're interested specifically in the effect of sex, we may want to *control for* what might be a better explanatory variable, like height. This would enable us to investigate if sex is a significant, unique effect in its own right or if it's only related to thumb length by virtue of being related to height. \n",
    "\n",
    "To do this, first we pick our variables. We'll use ```Thumb``` and ```Sex``` as before, as well as ```Height``` as another predictor. \n",
    "\n",
    "Second, specify the null hypothesis. Here we're still interested in the effect of sex in particular. We're just using height as a control. So the null hypothesis is still: \n",
    "\n",
    "$$H_0: b_1 = 0$$\n",
    "\n",
    "Third, specify and fit the model:\n",
    "\n",
    "$$\\hat{Y} = b_0 + b_1X_{1i} + b_2X_{2i}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dbebc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_obj <- lm(Thumb ~ Sex + Height, data = fingers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea946266",
   "metadata": {},
   "source": [
    "Fourth, find the p-value of b<sub>1</sub>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872874cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170b84b8",
   "metadata": {},
   "source": [
    "And finally, make a decision about whether or not sex is significant. \n",
    "\n",
    "Because sex and height are related, there is shared variation between them - both of them explain some of the same variation in thumb length. We need to put them both in a model together in order to disentangle what amount of *unique* variation they each explain. By doing this, we see that the effect of sex got smaller, after taking height into account - the coefficient fell from 6.447 to 2.783. This effect is so small, that it is no longer significant - the p-value is 0.132, which is above our &alpha; cutoff of 0.05. We'd interpret this to mean that in a world where there is *no* unique effect of sex (&beta;<sub>1</sub> = 0), a b<sub>1</sub> estimate of 2.783 would be a likely outcome. We can further see this by checking the 95% confidence interval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a833395",
   "metadata": {},
   "outputs": [],
   "source": [
    "confint(model_obj, \"Sexmale\", level=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33e4124",
   "metadata": {},
   "source": [
    "This range includes 0 as one of the &beta;<sub>1</sub>s that are likely to produce b<sub>1</sub> = 2.783. Thus, with the data we have, we fail to reject the null hypothesis that there is no unique effect of sex. We'd report this result as: \n",
    "\n",
    "\"The effect of sex was insignificant when controlling for height (*b* = 2.783, *p* = 0.132, 95% CI [-0.847, 6.413]).\"\n",
    "\n",
    "Note again that this does not mean &beta;<sub>1</sub> is definitely 0. There are still many non-zero values in the confidence interval that could be the truth. It could be that we only failed to find a significant effect because ```fingers``` only had a sample size of N=157. If we ran this model in a much larger dataset, we could create a narrower CI that might not overlap with 0. It's easier to distinguish small effects from &beta;<sub>1</sub> = 0 with more data. But in that case, it's always worthwhile to ask yourself if an effect that is significant, but with a very small coefficient, is worth anything to you in practical terms. Statistically significant doesn't mean *important*, only unlikely to be 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0db683",
   "metadata": {},
   "source": [
    "### Effects in interaction models\n",
    "\n",
    "When we test for interaction effects, we are often most interested in the interaction term. We want to know if the effect of one variable depends on values of the other (a non-zero interaction coefficient), or if the effects of each variable operate on the outcome variable independently of each other (a zero interaction coefficient). Thus, when we have hypotheses about interactions, we usually assess the significance of the interaction term. \n",
    "\n",
    "Let's test the hypothesis that there is an interaction between sex and height on thumb length. Our variables will be the same as in the multivariable case, ```Thumb```, ```Sex```, and ```Height```. \n",
    "\n",
    "Our null hypothesis is now for the interaction effect, and not the main effect of sex:\n",
    "\n",
    "$$H_0: b_3 = 0$$\n",
    "\n",
    "Specifying and fitting our model, we get: \n",
    "\n",
    "$$\\hat{Y} = b_0 + b_1X_{1i} + b_2X_{2i} + b_3X_{1i}*X_{2i}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d261913",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_obj <- lm(Thumb ~ Sex + Height + Sex*Height, data = fingers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a728545",
   "metadata": {},
   "source": [
    "Our hypothesis is about the interaction effect b<sub>3</sub>, so investigate the estimate and p-value on the line for ```Sexmale:Height```. We see that the p-value of the interaction is p = 0.953, which is not below the &alpha; criterion of 0.05. Thus we fail to reject the null hypothesis that there is no interaction. We can't be *sure* that there is no interaction truly in the population, but we are pretty confident that the true interaction effect is something close to zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0015ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "confint(model_obj, \"Sexmale:Height\", level = 0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd77096",
   "metadata": {},
   "source": [
    "Without a significant interaction, we would interpret the relationship between sex, height, and thumb length just as main effects. We'd report this as:\n",
    "\n",
    "\"There was no significant interaction between the effects of sex and height on thumb length (*b* = -0.030, *p* = 0.953, 95% CI [-1.024, 0.964]). Instead there was a significant main effect of height (*b* = 0.745, *p* = 0.010, 95% CI [0.178, 1.312]).\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db7bc9e",
   "metadata": {},
   "source": [
    "## Chapter summary\n",
    "\n",
    "After reading this chapter, you should be able to: \n",
    "- Define the null hypothesis\n",
    "- Explain what it means to reject the null hypothesis\n",
    "- Shuffle data to create null sampling distributions\n",
    "- Explain the alpha criterion\n",
    "- Define the meaning of a p-value\n",
    "- Find the p-value of a model estimate \n",
    "- Explain the limits of null hypothesis testing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}