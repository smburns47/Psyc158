

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Chapter 19 - Sources of Model Bias &#8212; Pomona Psych 158 Online Textbook</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapter-21';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Pomona College Psych 158 Online Textbook
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Unit 1 Describing Data</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-1.ipynb">Chapter 1 - Intro to Doing Statistics</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-2.ipynb">Chapter 2 - Statistical Reasoning</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-3.ipynb">Chapter 3 - What are Data</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-4.ipynb">Chapter 4 - Organizing Data</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-5.ipynb">Chapter 5 - Describing Data</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-6.ipynb">Chapter 6 - Variation in Multiple Variables</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-7.ipynb">Chapter 7 - Principles of Data Visualization</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Unit 2 - Modeling Data</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-8.ipynb">Chapter 8 - Where Data Come From</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-9.ipynb">Chapter 9 - Modeling the Data Generation Process</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-10.ipynb">Chapter 10 - Quantifying Model Error</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-11.ipynb">Chapter 11 - Adding an Explanatory Variable</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-12.ipynb">Chapter 12 - Quantitative Predictor Models</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-13.ipynb">Chapter 13 - Multivariable Models</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-14.ipynb">Chapter 14 - Models with Moderation</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Unit 3 - Evaluating Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-15.ipynb">Chapter 15 - Estimating Populations</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-16.ipynb">Chapter 16 - Significance Testing</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-17.ipynb">Chapter 17 - Significance Testing Whole Models</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-18.ipynb">Chapter 18 - Effect Sizes &amp; Power</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-19.ipynb">Chapter 19 - Alternate Approaches - Traditional Inference Methods</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-20.ipynb">Chapter 20 - Alternate Approaches - Bayesian Statistics</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-21.ipynb">Chapter 21 - Bias due to Improper Model Building</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-22.ipynb">Chapter 22 - Bias due to Improper Model Selection</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/smburns47/Psyc158/main?urlpath=tree/chapter-21.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onBinder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="initThebeSBT()"
  class="btn btn-sm btn-launch-thebe dropdown-item"
  title="Launch Thebe"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="btn__text-container">Live Code</span>
</button>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/smburns47/Psyc158" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/smburns47/Psyc158/issues/new?title=Issue%20on%20page%20%2Fchapter-21.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/chapter-21.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Chapter 19 - Sources of Model Bias</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#error-vs-bias">20.1 Error vs. bias</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#unbiased-vs-biased-estimates">20.2 Unbiased vs. biased estimates</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#checking-bias-in-models">20.3 Checking bias in models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assumption-1-representative-sample">20.4 Assumption 1: representative sample</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-check-for-assumption">How to check for assumption</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-to-do-if-assumption-is-violated">What to do if assumption is violated</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assumption-2-linear-relationship-between-variables">20.5 Assumption 2: linear relationship between variables</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">How to check for assumption</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">What to do if assumption is violated</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assumption-3-exogeneity">20.6 Assumption 3: exogeneity</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">How to check for assumption</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">What to do if assumption is violated</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assumption-4-no-multicollinearity">20.7 Assumption 4: no multicollinearity</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">How to check for assumption</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">What to do if assumption is violated</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assumption-5-normality-of-residuals">20.8 Assumption 5: Normality of residuals</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">How to check for assumption</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">What to do if assumption is violated</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assumption-6-homoscedasticity">20.9 Assumption 6: Homoscedasticity</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">How to check for assumption</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">What to do if assumption is violated</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assumption-7-independence-of-residuals">20.10 Assumption 7: independence of residuals</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">How to check for assumption</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">What to do if assumption is violated</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">20.11 Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#chapter-summary">Chapter Summary</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <p><a class="reference external" href="https://www.shannonmburns.com/Psyc158/intro.html">Back to Table of Contents</a></p>
<p><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-18.ipynb">Previous: Chapter 18 - Evaluating Effect Sizes</a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Run this first so it&#39;s ready by the time you need it</span>
<span class="nf">install.packages</span><span class="p">(</span><span class="s">&quot;dplyr&quot;</span><span class="p">)</span>
<span class="nf">install.packages</span><span class="p">(</span><span class="s">&quot;ggformula&quot;</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">dplyr</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">ggformula</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section class="tex2jax_ignore mathjax_ignore" id="chapter-19-sources-of-model-bias">
<h1>Chapter 19 - Sources of Model Bias<a class="headerlink" href="#chapter-19-sources-of-model-bias" title="Permalink to this heading">#</a></h1>
<section id="error-vs-bias">
<h2>20.1 Error vs. bias<a class="headerlink" href="#error-vs-bias" title="Permalink to this heading">#</a></h2>
<p>We use statistics to try and explain complex things in the world, and arrive at some general conclusions about what we can expect. More specifically in the context of the general linear model, we try to model the data generation process of data we care about and see what information helps us make the best predictions about those data.</p>
<p>We know from our discussion of model error that it is hard and maybe impossible to make perfectly correct predictions. In any data generation process, we might be able to figure out that using information from some predictor variables helps us explain some variation in an outcome variable and make <em>better</em> predictions, but there is almost always some error left unexplained.</p>
<img src="images/ch10-var1.png" width="500">
<p>For any particular prediction we make about the outcome value of one data point, that prediction is likely to be off by a bit. This amount that we typically miss by is the <strong>error</strong> of a model. We can quantify it by looking at the distribution of the residuals a model produces when making predictions. For example, let’s simulate a sample of data with a partially-known data generation process, and make predictions using the part of the model that we know:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">x</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">rnorm</span><span class="p">(</span><span class="m">100</span><span class="p">,</span><span class="m">0</span><span class="p">,</span><span class="m">1</span><span class="p">)</span><span class="w">  </span><span class="c1">#defining a random variable</span>
<span class="n">e</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">rnorm</span><span class="p">(</span><span class="m">100</span><span class="p">,</span><span class="m">0</span><span class="p">,</span><span class="m">1</span><span class="p">)</span><span class="w">  </span><span class="c1">#some unexplained error</span>
<span class="n">y</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">0.5</span><span class="o">*</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">e</span><span class="w">   </span><span class="c1">#what the true data generation process is </span>
<span class="n">sim_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">data.frame</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>

<span class="n">predicted_y</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">0.5</span><span class="o">*</span><span class="n">x</span><span class="w"> </span><span class="c1">#expected y when we don&#39;t know e</span>
</pre></div>
</div>
</div>
</div>
<p>If we plot the residuals of the model in this sample, making a histogram of the error distribution, we see that most of them are non-zero. We are missing our predictions by a bit.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">sim_df</span><span class="o">$</span><span class="n">model_resid</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sim_df</span><span class="o">$</span><span class="n">y</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">predicted_y</span>
<span class="nf">gf_histogram</span><span class="p">(</span><span class="o">~</span><span class="w"> </span><span class="n">model_resid</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sim_df</span><span class="p">)</span><span class="o">%&gt;%</span><span class="w"> </span><span class="nf">gf_vline</span><span class="p">(</span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">xintercept</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The spread of the error distribution tells us by how much we typically miss our predictions. A wide error distribution means a model has a lot of error. The narrower we can make the error distribution, and the less error there is, the better our model.</p>
<p>However, model error isn’t the only thing for us to be aware of when relying on the predictions a model makes. Let’s imagine a situation where the true data generation process only has a very small amount of unaccountable variation, so that we are able to explain almost all the error. We can still make very inaccurate predictions if we use the wrong coefficients in a model:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">x</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">rnorm</span><span class="p">(</span><span class="m">100</span><span class="p">,</span><span class="m">0</span><span class="p">,</span><span class="m">1</span><span class="p">)</span><span class="w">     </span><span class="c1">#defining a random variable</span>
<span class="n">e</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">rnorm</span><span class="p">(</span><span class="m">100</span><span class="p">,</span><span class="m">0</span><span class="p">,</span><span class="m">0.1</span><span class="p">)</span><span class="w">   </span><span class="c1">#tiny unexplained error</span>
<span class="n">y</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">0.5</span><span class="o">*</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">e</span><span class="w">      </span><span class="c1">#what the true data generation process is, almost no error </span>
<span class="n">sim_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">data.frame</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>

<span class="n">biased_y</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">0.5</span><span class="o">*</span><span class="n">x</span><span class="w"> </span><span class="c1">#using the wrong intercept value to make predictions for y</span>
<span class="n">sim_df</span><span class="o">$</span><span class="n">model_resid_biased</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sim_df</span><span class="o">$</span><span class="n">y</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">biased_y</span>
<span class="nf">gf_histogram</span><span class="p">(</span><span class="o">~</span><span class="w"> </span><span class="n">model_resid_biased</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sim_df</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="nf">gf_vline</span><span class="p">(</span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">xintercept</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Take a look at the center of both these error distributions.</p>
<p>In the first error distribution we made, few residual values were exactly equal to 0 (few predictions were perfect), but across all the residuals they clustered <em>around</em> 0. That means even if any one prediction is unlikely to be perfect, the predictions as a whole aren’t missing in any systematic way.</p>
<p>In contrast, the spread of the second error distribution is narrow, but way off 0. This means that every prediction we are making is wrong, and they’re all wrong in the same way.</p>
<p>This is known as <strong>bias</strong>. A model has error if its predictions are sometimes wrong. A model has bias if the predictions are wrong in a systematic way.</p>
<p>Way back in chapter 3 we used a bulls-eye metaphor to talk about measurement error vs. validity. We can use the same metaphor here. If the bulls-eye is the actual value of a datapoint on an outcome variable and each dot is a prediction, model error refers to the spread of those predictions while model bias refers to where those predictions are centered on.</p>
<img src="images/ch20-errorbias.png" width="500"></section>
<section id="unbiased-vs-biased-estimates">
<h2>20.2 Unbiased vs. biased estimates<a class="headerlink" href="#unbiased-vs-biased-estimates" title="Permalink to this heading">#</a></h2>
<p>What will make a model biased? Take a look at the model coefficients that were used to make predictions for both error distributions above. In the unbiased model, we used the values “2” and “0.5” as coefficients for the <code class="docutils literal notranslate"><span class="pre">intercept</span></code> and <code class="docutils literal notranslate"><span class="pre">x</span></code>, respectively, in order to make predictions about <code class="docutils literal notranslate"><span class="pre">y</span></code>. These are the exact same values we used in the true data generation process for y, so we know the only reason our predictions were off is because we didn’t know the value of <code class="docutils literal notranslate"><span class="pre">e</span></code> to include in the prediction equation.</p>
<p>In the biased model, we used “5” instead of “2” for the coefficient of the intercept. This made our predictions systematically overshoot the actual values of <code class="docutils literal notranslate"><span class="pre">y</span></code>.</p>
<p>Thus you can think of bias in terms of the value of the coefficient estimate in a model. If it is the same value as the population parameter is, a model is unbiased (even though it might have error). If the model coefficient is different than the population parameter, the model is biased.</p>
<p>We know from our discussion of sampling distributions that any one estimate derived from a data sample is unlikely to exactly match the population parameter. For example, let’s simulate an entire population of data with the same data generation process above, and fit a model in just a sample of it:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">x</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">rnorm</span><span class="p">(</span><span class="m">10000</span><span class="p">,</span><span class="m">0</span><span class="p">,</span><span class="m">1</span><span class="p">)</span><span class="w">  </span><span class="c1">#defining a random variable</span>
<span class="n">e</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">rnorm</span><span class="p">(</span><span class="m">10000</span><span class="p">,</span><span class="m">0</span><span class="p">,</span><span class="m">1</span><span class="p">)</span><span class="w">  </span><span class="c1">#some unexplained error</span>
<span class="n">y</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">0.5</span><span class="o">*</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">e</span><span class="w">     </span><span class="c1">#what the true data generation process is </span>
<span class="n">sim_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">data.frame</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>

<span class="nf">set.seed</span><span class="p">(</span><span class="m">10</span><span class="p">)</span>
<span class="n">sim_sample</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sample_n</span><span class="p">(</span><span class="n">sim_df</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="o">=</span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="n">replace</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span>

<span class="n">sim_model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">lm</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sim_sample</span><span class="p">)</span>
<span class="n">sim_model</span>
</pre></div>
</div>
</div>
</div>
<p>Whereas the true intercept and effect of x are 2 and 0.5 respectively, our estimates for those coefficients are 1.84 and 0.57. If we were to use these values to make predictions within the data sample we used for fitting the model:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">sim_sample</span><span class="o">$</span><span class="n">model_resid</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sim_sample</span><span class="o">$</span><span class="n">y</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span><span class="n">sim_model</span><span class="p">,</span><span class="w"> </span><span class="n">sim_sample</span><span class="p">)</span>
<span class="nf">mean</span><span class="p">(</span><span class="n">sim_sample</span><span class="o">$</span><span class="n">model_resid</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The residuals are centered on 0 (or some value so tiny it’s basically 0).</p>
<p>But remember we don’t ultimately care about making predictions about known data, but about <em>unknown</em> data in the population more generally. How would this exact model do in the rest of the population?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">sim_df</span><span class="o">$</span><span class="n">model_resid</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sim_df</span><span class="o">$</span><span class="n">y</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span><span class="n">sim_model</span><span class="p">,</span><span class="w"> </span><span class="n">sim_df</span><span class="p">)</span>
<span class="nf">mean</span><span class="p">(</span><span class="n">sim_df</span><span class="o">$</span><span class="n">model_resid</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>This error distribution has a non-zero center, which means the model is making biased predictions. It is slightly underestimating the true value of y in the population.</p>
<p>Technically any one model is going to make somewhat biased predictions. This is because of the sampling error that results in different coefficient estimates.</p>
<p>However, we also know from our discussion of sampling distributions that bigger sample sizes reduce the standard error of a sampling distribution. This means that model estimates are on average closer to the true population parameter in large samples, and would thus make less biased predictions:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">set.seed</span><span class="p">(</span><span class="m">10</span><span class="p">)</span>
<span class="n">sim_sample</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sample_n</span><span class="p">(</span><span class="n">sim_df</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="o">=</span><span class="m">5000</span><span class="p">,</span><span class="w"> </span><span class="n">replace</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span><span class="w"> </span><span class="c1">#picking a larger sample</span>

<span class="n">sim_model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">lm</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sim_sample</span><span class="p">)</span>
<span class="n">sim_df</span><span class="o">$</span><span class="n">model_resid</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sim_df</span><span class="o">$</span><span class="n">y</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span><span class="n">sim_model</span><span class="p">,</span><span class="w"> </span><span class="n">sim_df</span><span class="p">)</span>
<span class="nf">mean</span><span class="p">(</span><span class="n">sim_df</span><span class="o">$</span><span class="n">model_resid</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>In this way, fitting models in larger samples improves the accuracy of predictions from any one model.</p>
<p>But the prediction accuracy of one model is not actually the biggest concern we have about model bias. This is because, even though one sample estimate is unlikely to match the population parameter, estimates created from many samples will cluster around the true population parameter. This means that, while any one model may be biased, there’s no systematic <em>direction</em> in that bias. Due to this, a model coefficient is actually known as an <strong>unbiased estimator</strong>.</p>
<p>We call this an unbiased estimator even though any one coefficient estimate is likely to make biased predictions. We call it this because the amount of bias is only determined by sampling error. If we drew many samples and created many estimates, the distribution of those estimates would center on the true population parameter:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">x</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">rnorm</span><span class="p">(</span><span class="m">10000</span><span class="p">,</span><span class="m">0</span><span class="p">,</span><span class="m">1</span><span class="p">)</span><span class="w">  </span><span class="c1">#defining a random variable</span>
<span class="n">e</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">rnorm</span><span class="p">(</span><span class="m">10000</span><span class="p">,</span><span class="m">0</span><span class="p">,</span><span class="m">1</span><span class="p">)</span><span class="w">  </span><span class="c1">#some unexplained error</span>
<span class="n">y</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">0.5</span><span class="o">*</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">e</span><span class="w">     </span><span class="c1">#0.5 is the true b1 value </span>
<span class="n">sim_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">data.frame</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>

<span class="c1">#sampling many estimates of b1</span>
<span class="n">b1s</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">vector</span><span class="p">(</span><span class="n">length</span><span class="o">=</span><span class="m">1000</span><span class="p">)</span>
<span class="nf">for </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="m">1000</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">sim_sample</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sample_n</span><span class="p">(</span><span class="n">sim_df</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="o">=</span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="n">replace</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span><span class="w"> </span>
<span class="w">    </span><span class="n">sim_model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">lm</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sim_sample</span><span class="p">)</span>
<span class="w">    </span><span class="n">b1s</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sim_model</span><span class="o">$</span><span class="n">coefficients</span><span class="p">[[</span><span class="m">2</span><span class="p">]]</span>
<span class="p">}</span>

<span class="c1">#central tendency of b1 estimates</span>
<span class="nf">mean</span><span class="p">(</span><span class="n">b1s</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>A worse situation is when a model estimate is biased in a particular way, repeatedly. Thus, no matter the samples you draw, the central tendency of those estimates will not be the same as the population parameter. When this occurs, you have a <strong>biased estimator</strong>.</p>
<p>An example of a biased estimator is the standard deviation of a distribution - specifically, if we define variance to be the mean of the squared residuals,</p>
<div class="math notranslate nohighlight">
\[\frac{\sum_{i=1}^{N}(X_i-\bar{X})^2}{N}\]</div>
<p>To see this, suppose we have a sample that contains just two IQ observations, 98 and 100. This is a perfectly legitimate sample, even if it does have a sample size of N=2. It has a sample mean of 99, and the sample is just large enough for us to be able to observe some variability (two observations is the bare minimum number needed for any variability to be observed). For this sample, the average amount of deviation from the mean is 1.</p>
<p>What intuitions do we have about the population based on this sample? As far as the population mean goes, the best guess we can possibly make is the sample mean: if forced to guess, we’d probably guess that the population mean IQ is 99. What about the standard deviation? Is it also the standard deviation of the sample? Or maybe, since the sample standard deviation is only based on two observations, you may feel that we haven’t given the population “enough of a chance” to reveal its true variability to us. It’s not just that we suspect that the estimate is wrong: after all, with only two observations we expect it to be wrong to some degree. The worry is that the error is systematic, such that every sample of 2 observations would be wrong in the same direction.</p>
<p>We can use R to simulate the results of many samples to demonstrate this. Given the true population mean of IQ is 100 and the standard deviation is 15, we can use the rnorm() function to generate the results of an experiment in which we measure N=2 IQ scores, and calculate the sample standard deviation. If we do this over and over again, and plot a histogram of these sample standard deviations, we get the sampling distribution of the standard deviation (see figure below).</p>
<img src="images/ch16-sdestimate.png"  width="350">
<p>Even though the true population standard deviation is 15, the average of the sample standard deviations is only 8.5.</p>
<p>Now let’s extend the simulation. Instead of restricting ourselves to a sample size of N=2, let’s repeat the exercise again for sample sizes from 1 to 10. If we plot the average sample mean and average sample standard deviation as a function of sample size, you get the results shown in this next figure.</p>
<img src="images/ch16-estimatesim.png"  width="650">
<p>On the left hand side (panel A) is the average sample mean for each sample size, and on the right hand side (panel B) is the average standard deviation for each sample size. The two plots are quite different: no matter the sample size, the average sample mean from a sample distribution of the mean is equal to the population mean. It is an unbiased estimator. The plot on the right shows that the average sample standard deviation from a sample distribution of the standard deviation is smaller than the population standard deviation σ. As sample size gets larger the estimate is smaller by less, but no matter the sample the estimate is <em>always</em> smaller than the population parameter. It is a biased estimator.</p>
<p>For standard deviation we can fix this bias, by dividing the sum of the squared errors by the degrees of freedom instead of just N:</p>
<div class="math notranslate nohighlight">
\[s = \sqrt{\frac{\sum_{i=1}^{N}(X_i-\bar{X})^2}{N-1}}\]</div>
<p>This is why we use N-1 in the equation for standard deviation instead of N. It gives us a better estimate of the population standard deviation.</p>
<div class="alert alert-block alert-info">
<b>Biased predictions</b>: when the predictions of one model are systematically different than the true data values.
<br>
<b>Biased estimates</b>: when all statistic estimates are systematically different than the population parameter, no matter the sample they were fit in.
</div>
<p>Biased estimators are particularly dangerous for statistical inference because, if you didn’t know the true population parameter, you would have no idea that all the estimates you could draw from any sample would be biased. You could find a large effect size, replicate the experiment in a large expensive data sample, get a really precise estimate of that effect where you’re feeling very confident, and you might still have no idea that that estimate is biased and centered away from the true population parameter. So it is important to be aware of what situations lead to biased estimates, and what to do to fix those situations.</p>
</section>
<section id="checking-bias-in-models">
<h2>20.3 Checking bias in models<a class="headerlink" href="#checking-bias-in-models" title="Permalink to this heading">#</a></h2>
<p>When it comes to coefficient estimates from models, the general linear model framework typically produces unbiased estimates.</p>
<p>However, there are certain situations where this is not the case. This is because the general linear model makes particular assumptions about the data used to fit a model. It will only produce unbiased estimates if those assumptions are met. If the assumptions are violated, your estimates might be biased. For this reason it’s a good idea to check whether your data meets these assumptions, and take certain actions to fix things if the assumptions are violated.</p>
<p>Unfortunately this process isn’t always done (or at least reported) in published research. Most publications tell you how certain they are about their model estimates (significance testing &amp; confidence intervals), and many also publish how large their model estimate is (effect sizes). But both of these conclusions could be wrong if a model is biased. That’s why it’s important to check for bias too.</p>
<p>In the general linear model, there are 7 assumptions to look out for. We’ve actually already discussed most of them in different contexts, but now we will bring them together to see how exactly they determine model bias and what we can do about violations of these assumptions. In the sections below we will explain what these assumptions are, how to check for them, and what to do if they are violated. Each will be accompanied by a set of simulations so that you can see the sampling distribution of model estimates when the assumption is met and when it is violated.</p>
</section>
<section id="assumption-1-representative-sample">
<h2>20.4 Assumption 1: representative sample<a class="headerlink" href="#assumption-1-representative-sample" title="Permalink to this heading">#</a></h2>
<p>When you have a representative sample, the estimates you make in that sample have the best chance of resembling the true population parameter. The general linear model assumes that the relationship between variables within a sample is representative of the relationship between those variables in the population. If we get a sample where that relationship is substantially different, obviously we will get a coefficient estimate that is substantially different than the population parameter.</p>
<p>That can happen just by bad luck during random sampling: we just happen to draw a particularly strange sample by chance. But over many samples, random sampling will insure that on the whole our samples are representative and our estimates unbiased.</p>
<p>If there’s any reason our sampling process is not representative, those estimates might be biased even across many samples. For example, there is a famous saying that “money does not buy happiness”. If you were to test this statistically, you might look for a significant relationship between people’s happiness levels and how much money they make. But if you were to conduct this research only with college students (who come from a higher income bracket on average than the world population), you would only be investigating the relationship between money and happiness in people with more money, not across all income brackets. No matter what sample you drew, if it was not representative of the wider population, you might get a biased estimate (and you might miss the fact that <a class="reference external" href="http://content.time.com/time/magazine/article/0,9171,2019628,00.html">money is indeed correlated with happiness</a> at lower income levels where financial security is threatened). If your estimate is biased due to nonrepresentative sampling, it might apply to this specific population, but not <strong>generalize</strong> to other populations of people.</p>
<p>One way a sample can be unrepresentative is if you don’t have values from the full range of reasonable values on your outcome. We can see the impact of this with a simulation. First, we will simulate a medium relationship between two variables x and y and see what sort of sampling distribution we would get across many samples of those variables:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">x</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">runif</span><span class="p">(</span><span class="m">10000</span><span class="p">,</span><span class="m">0</span><span class="p">,</span><span class="m">1</span><span class="p">)</span><span class="w">    </span><span class="c1">#defining a random variable</span>
<span class="n">e</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">rnorm</span><span class="p">(</span><span class="m">10000</span><span class="p">,</span><span class="m">0</span><span class="p">,</span><span class="m">1</span><span class="p">)</span><span class="w">    </span><span class="c1">#some unexplained error</span>
<span class="n">y</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">0.3</span><span class="o">*</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">e</span><span class="w">  </span><span class="c1">#0.3 is the true b1 value in population </span>
<span class="n">sim_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">data.frame</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">)</span>

<span class="c1">#sampling many estimates of b1</span>
<span class="n">nobias_b1s</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">vector</span><span class="p">(</span><span class="n">length</span><span class="o">=</span><span class="m">1000</span><span class="p">)</span>
<span class="nf">for </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="m">1000</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">sim_sample</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sample_n</span><span class="p">(</span><span class="n">sim_df</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="o">=</span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="n">replace</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span><span class="w"> </span>
<span class="w">    </span><span class="n">sim_model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">lm</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sim_sample</span><span class="p">)</span><span class="w">  </span><span class="c1">#fitting a linear model instead of nonlinear</span>
<span class="w">    </span><span class="n">nobias_b1s</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sim_model</span><span class="o">$</span><span class="n">coefficients</span><span class="p">[[</span><span class="m">2</span><span class="p">]]</span>
<span class="p">}</span>

<span class="c1">#central tendency of b1 estimates</span>
<span class="n">nobias_b1s_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">data.frame</span><span class="p">(</span><span class="n">nobias_b1s</span><span class="p">)</span>
<span class="nf">gf_histogram</span><span class="p">(</span><span class="o">~</span><span class="w"> </span><span class="n">nobias_b1s</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">nobias_b1s_df</span><span class="p">,</span><span class="w"> </span><span class="n">fill</span><span class="o">=</span><span class="s">&quot;red&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="o">=</span><span class="m">0.5</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">    </span><span class="nf">gf_vline</span><span class="p">(</span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">xintercept</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.3</span><span class="p">)</span><span class="w"> </span><span class="c1">#true b1 is 0.3</span>
<span class="nf">mean</span><span class="p">(</span><span class="n">nobias_b1s</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Any one sample may find a b<sub>1</sub> estimate that is larger or smaller than 0.3, but in general they will cluster around 0.3.</p>
<p>However, let’s say we only sample people who are high on the outcome variable score. The full range of that variable is about -2 to 6, but we’ll only select datapoints that had y values of at least 4. We’ll make the sampling distribution for this situation as well, and plot it (in blue) on the same graph as the no bias sampling distribution (in red) so that you can compare. A vertical black line marks where the true population parameter is.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">x</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">runif</span><span class="p">(</span><span class="m">40000</span><span class="p">,</span><span class="m">0</span><span class="p">,</span><span class="m">1</span><span class="p">)</span><span class="w">    </span><span class="c1">#oversimulating since we&#39;re going to cut out ~3/4 of the data</span>
<span class="n">e</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">rnorm</span><span class="p">(</span><span class="m">40000</span><span class="p">,</span><span class="m">0</span><span class="p">,</span><span class="m">1</span><span class="p">)</span><span class="w">    </span>
<span class="n">y</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">0.3</span><span class="o">*</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">e</span><span class="w">       </span><span class="c1">#0.3 is the true b1 value in population </span>
<span class="n">sim_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">data.frame</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">)</span>
<span class="n">high_sim_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">filter</span><span class="p">(</span><span class="n">sim_df</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="o">&gt;</span><span class="m">4</span><span class="p">)</span><span class="w"> </span><span class="c1">#filtering to only large y values</span>

<span class="c1">#sampling many estimates of b1</span>
<span class="n">bias_b1s</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">vector</span><span class="p">(</span><span class="n">length</span><span class="o">=</span><span class="m">1000</span><span class="p">)</span>
<span class="nf">for </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="m">1000</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">sim_sample</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sample_n</span><span class="p">(</span><span class="n">high_sim_df</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="o">=</span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="n">replace</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span><span class="w"> </span>
<span class="w">    </span><span class="n">sim_model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">lm</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sim_sample</span><span class="p">)</span><span class="w">  </span><span class="c1">#fitting a linear model instead of nonlinear</span>
<span class="w">    </span><span class="n">bias_b1s</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sim_model</span><span class="o">$</span><span class="n">coefficients</span><span class="p">[[</span><span class="m">2</span><span class="p">]]</span>
<span class="p">}</span>

<span class="c1">#central tendency of b1 estimates</span>
<span class="n">bias_b1s_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">data.frame</span><span class="p">(</span><span class="n">bias_b1s</span><span class="p">)</span>
<span class="nf">gf_histogram</span><span class="p">(</span><span class="o">~</span><span class="w"> </span><span class="n">nobias_b1s</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">nobias_b1s_df</span><span class="p">,</span><span class="w"> </span><span class="n">fill</span><span class="o">=</span><span class="s">&quot;red&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="o">=</span><span class="m">0.5</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">    </span><span class="nf">gf_histogram</span><span class="p">(</span><span class="o">~</span><span class="w"> </span><span class="n">bias_b1s</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bias_b1s_df</span><span class="p">,</span><span class="w"> </span><span class="n">fill</span><span class="o">=</span><span class="s">&quot;blue&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="o">=</span><span class="m">0.5</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="nf">gf_vline</span><span class="p">(</span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">xintercept</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.3</span><span class="p">)</span><span class="w"> </span><span class="c1">#true b1 is 0.3</span>
<span class="nf">mean</span><span class="p">(</span><span class="n">bias_b1s</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The true relationship between x and y is 0.3, but by truncating the y variable, our sampling distribution is biased downward.</p>
<p>When your data is missing the full range of the outcome variable, there’s less variation present that would be explained by the predictor. Noise now has a bigger proportional presence, and the predictor can’t reliably explain noise.</p>
<img src="images/ch20-selectiony.gif" width="500">
<p><em><a class="reference external" href="https://sites.google.com/view/robertostling/home/teaching">gif source</a></em></p>
<p>Interestingly, the same is not true for truncating the x variable:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">x</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">runif</span><span class="p">(</span><span class="m">40000</span><span class="p">,</span><span class="m">0</span><span class="p">,</span><span class="m">1</span><span class="p">)</span><span class="w">    </span><span class="c1">#oversimulating since we&#39;re going to cut out ~3/4 of the data</span>
<span class="n">e</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">rnorm</span><span class="p">(</span><span class="m">40000</span><span class="p">,</span><span class="m">0</span><span class="p">,</span><span class="m">1</span><span class="p">)</span><span class="w">    </span>
<span class="n">y</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">0.3</span><span class="o">*</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">e</span><span class="w">       </span><span class="c1">#0.3 is the true b1 value in population </span>
<span class="n">sim_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">data.frame</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">)</span>
<span class="n">high_sim_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">filter</span><span class="p">(</span><span class="n">sim_df</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="o">&gt;</span><span class="m">0.75</span><span class="p">)</span><span class="w"> </span><span class="c1">#filtering to only large x values</span>

<span class="c1">#sampling many estimates of b1</span>
<span class="n">bias_b1s</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">vector</span><span class="p">(</span><span class="n">length</span><span class="o">=</span><span class="m">1000</span><span class="p">)</span>
<span class="nf">for </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="m">1000</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">sim_sample</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sample_n</span><span class="p">(</span><span class="n">high_sim_df</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="o">=</span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="n">replace</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span><span class="w"> </span>
<span class="w">    </span><span class="n">sim_model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">lm</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sim_sample</span><span class="p">)</span>
<span class="w">    </span><span class="n">bias_b1s</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sim_model</span><span class="o">$</span><span class="n">coefficients</span><span class="p">[[</span><span class="m">2</span><span class="p">]]</span>
<span class="p">}</span>

<span class="c1">#red = unbiased sampling distribution</span>
<span class="c1">#blue = biased sampling distribution</span>
<span class="n">bias_b1s_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">data.frame</span><span class="p">(</span><span class="n">bias_b1s</span><span class="p">)</span>
<span class="nf">gf_histogram</span><span class="p">(</span><span class="o">~</span><span class="w"> </span><span class="n">nobias_b1s</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">nobias_b1s_df</span><span class="p">,</span><span class="w"> </span><span class="n">fill</span><span class="o">=</span><span class="s">&quot;red&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="o">=</span><span class="m">0.5</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">    </span><span class="nf">gf_histogram</span><span class="p">(</span><span class="o">~</span><span class="w"> </span><span class="n">bias_b1s</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bias_b1s_df</span><span class="p">,</span><span class="w"> </span><span class="n">fill</span><span class="o">=</span><span class="s">&quot;blue&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="o">=</span><span class="m">0.5</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="nf">gf_vline</span><span class="p">(</span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">xintercept</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.3</span><span class="p">)</span><span class="w"> </span><span class="c1">#true b1 is 0.3</span>
<span class="nf">mean</span><span class="p">(</span><span class="n">bias_b1s</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>On average, the sampling distribution will still cluster around an effect of 0.3 for x. The difference here is that the sampling distribution is <em>wider</em>, despite having the same sample size. It isn’t the model coefficient that is biased in this case, but the <em>standard error</em> is biased to be too big. On average we will detect the right effect, but it will be harder to get an accurate picture of it. If the true effect is 0.3 but standard error is biased upward, we will have less power than we planned to find that effect significant - there will be more Type II errors.</p>
<img src="images/ch20-selectionx.gif" width="500">
<p><em><a class="reference external" href="https://sites.google.com/view/robertostling/home/teaching">gif source</a></em></p>
<p>This is only true however if the true relationship in the population is linear. If the slope of the regression line should change along different values of x, but you only sampled some of those values, you will miss the true contour of the regression line. This is the problem that happened in the money and happiness example discussed above.</p>
<p>Another instance of bias due to representativeness is when two predictor variables interact, such that predictor 2 influences the relationship between predictor 1 and the outcome, but we only collected data from some values of predictor 2. For example, if there’s a difference in how people give to charity between individualistic and collectivistic cultures, but you only sampled people born in the US.</p>
<p>Now, a nonrepresentative sample doesn’t ensure that your estimates will be biased. Estimates will only be biased if your sample is non-representative on variables that are involved in the data generation process you are investigating. If you are investigating the relationship between money and happiness but only have a sample of right-handed people, there’s a pretty good argument to make that being right or left handed doesn’t influence how happy money makes you. So the assumption of representativeness only applies to variables that are involved in the particular data generation process you are investigating.</p>
<section id="how-to-check-for-assumption">
<h3>How to check for assumption<a class="headerlink" href="#how-to-check-for-assumption" title="Permalink to this heading">#</a></h3>
<p>There aren’t great ways to check if your samples are representative. You need to know about the expected range of the variables in your model, and if there are any moderator variables out there that you haven’t included. If you aren’t familiar with your research domain enough to know these answers, it will be difficult to catch when your sample isn’t representative. You can try to include representativeness on major demographic variables like sex, race, etc. as a default to catch common reasons for nonrepresentativeness, but that doesn’t ensure you’re not missing something else important. Also, just because a study was run on only US college students, that doesn’t mean <em>for sure</em> that the sample is not representative. There has to be a good reason to expect truncated age and culture to make a difference on the estimated effect. The best you can do here is read about other research in your field to stay abreast of what others have found, and continue developing psychological research.</p>
</section>
<section id="what-to-do-if-assumption-is-violated">
<h3>What to do if assumption is violated<a class="headerlink" href="#what-to-do-if-assumption-is-violated" title="Permalink to this heading">#</a></h3>
<p>The time to deal with this assumption is before you collect data. Plan ahead to figure out what variables are important to be representative on, and do your best to recruit participants that have a variety of values on those variables. If for whatever reason you’re unable to do that, make sure you interpret your results within the subset of the population you <em>were</em> able to recruit, and not overgeneralize to people who are not represented in your sample. In other words, as we saw at the end of chapter 12 when talking about the limits of regression, we should’t extrapolate conclusions beyond the range of the data in our sample.</p>
</section>
</section>
<section id="assumption-2-linear-relationship-between-variables">
<h2>20.5 Assumption 2: linear relationship between variables<a class="headerlink" href="#assumption-2-linear-relationship-between-variables" title="Permalink to this heading">#</a></h2>
<p>The second assumption that needs to be met in order for the general linear model framework to produce unbiased estimates is that all the variables in the model are related to each other linearly.</p>
<p>We already discussed the idea of linear vs. nonlinear models at length in chapter 14, but it’s worth repeating here what happens if this assumption is violated and you misspecify a nonlinear model as a linear one. For variables to be related to each other linearly, that means they are combined only with addition - the raw values of each predictor variable are entered into the equation, and the parts of the model corresponding to each predictor variable are added together. If the true data generation process is nonlinear - involves a transformation of a predictor variable, or multiplying predictor variables with each other - you have to do that transformation to the data values before fitting the model.</p>
<p>Below is an example of what happens if we try to make predictions with a linear model, but the true data generation process was nonlinear.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">x</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">runif</span><span class="p">(</span><span class="m">10000</span><span class="p">,</span><span class="m">0</span><span class="p">,</span><span class="m">4</span><span class="p">)</span><span class="w">    </span><span class="c1">#defining a random variable</span>
<span class="n">e</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">rnorm</span><span class="p">(</span><span class="m">10000</span><span class="p">,</span><span class="m">0</span><span class="p">,</span><span class="m">1</span><span class="p">)</span><span class="w">    </span><span class="c1">#some unexplained error</span>
<span class="n">x_squared</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">x</span><span class="o">**</span><span class="m">2</span>
<span class="n">y</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">0.5</span><span class="o">*</span><span class="n">x_squared</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">e</span><span class="w">  </span><span class="c1">#0.5 is the true b1 value in a quadratic model </span>
<span class="n">sim_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">data.frame</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">x_squared</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">)</span>

<span class="c1">#sampling many estimates of unbiased b1</span>
<span class="n">nobias_b1s</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">vector</span><span class="p">(</span><span class="n">length</span><span class="o">=</span><span class="m">1000</span><span class="p">)</span>
<span class="nf">for </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="m">1000</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">sim_sample</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sample_n</span><span class="p">(</span><span class="n">sim_df</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="o">=</span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="n">replace</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span><span class="w"> </span>
<span class="w">    </span><span class="n">sim_model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">lm</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">x_squared</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sim_sample</span><span class="p">)</span><span class="w">  </span>
<span class="w">    </span><span class="n">nobias_b1s</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sim_model</span><span class="o">$</span><span class="n">coefficients</span><span class="p">[[</span><span class="m">2</span><span class="p">]]</span>
<span class="p">}</span>

<span class="c1">#sampling many estimates of biased b1</span>
<span class="n">bias_b1s</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">vector</span><span class="p">(</span><span class="n">length</span><span class="o">=</span><span class="m">1000</span><span class="p">)</span>
<span class="nf">for </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="m">1000</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">sim_sample</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sample_n</span><span class="p">(</span><span class="n">sim_df</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="o">=</span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="n">replace</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span><span class="w"> </span>
<span class="w">    </span><span class="n">sim_model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">lm</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sim_sample</span><span class="p">)</span><span class="w">  </span><span class="c1">#fitting a linear model instead of nonlinear</span>
<span class="w">    </span><span class="n">bias_b1s</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sim_model</span><span class="o">$</span><span class="n">coefficients</span><span class="p">[[</span><span class="m">2</span><span class="p">]]</span>
<span class="p">}</span>

<span class="c1">#red = unbiased sampling distribution</span>
<span class="c1">#blue = biased sampling distribution</span>
<span class="n">nobias_b1s_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">data.frame</span><span class="p">(</span><span class="n">nobias_b1s</span><span class="p">)</span>
<span class="n">bias_b1s_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">data.frame</span><span class="p">(</span><span class="n">bias_b1s</span><span class="p">)</span>
<span class="nf">gf_histogram</span><span class="p">(</span><span class="o">~</span><span class="w"> </span><span class="n">nobias_b1s</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">nobias_b1s_df</span><span class="p">,</span><span class="w"> </span><span class="n">fill</span><span class="o">=</span><span class="s">&quot;red&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="o">=</span><span class="m">0.5</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">    </span><span class="nf">gf_histogram</span><span class="p">(</span><span class="o">~</span><span class="w"> </span><span class="n">bias_b1s</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bias_b1s_df</span><span class="p">,</span><span class="w"> </span><span class="n">fill</span><span class="o">=</span><span class="s">&quot;blue&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="o">=</span><span class="m">0.5</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="nf">gf_vline</span><span class="p">(</span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">xintercept</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.5</span><span class="p">)</span><span class="w"> </span><span class="c1">#true b1 is 0.5</span>
<span class="nf">mean</span><span class="p">(</span><span class="n">bias_b1s</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We set the true β<sub>1</sub> in the data generation process to be 0.5, but the sampling distribution of b<sub>1</sub> estimates centers on ~2. By misspecifying the model, we are overestimating the population β<sub>1</sub> and guessing that the effect of <code class="docutils literal notranslate"><span class="pre">x</span></code> is larger than it actually is.</p>
<p>It’s also possible to underestimate the coefficient parameter due to a model misspecification:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">x</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">runif</span><span class="p">(</span><span class="m">10000</span><span class="p">,</span><span class="m">0</span><span class="p">,</span><span class="m">4</span><span class="p">)</span><span class="w">    </span><span class="c1">#defining a random variable</span>
<span class="n">e</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">rnorm</span><span class="p">(</span><span class="m">10000</span><span class="p">,</span><span class="m">0</span><span class="p">,</span><span class="m">1</span><span class="p">)</span><span class="w">    </span><span class="c1">#some unexplained error</span>
<span class="n">logx</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">log</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">0.5</span><span class="o">*</span><span class="n">logx</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">e</span><span class="w">  </span><span class="c1">#0.5 is the true b1 value in a log-transform model </span>
<span class="n">sim_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">data.frame</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">logx</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">)</span>

<span class="c1">#sampling many estimates of unbiased b1</span>
<span class="n">nobias_b1s</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">vector</span><span class="p">(</span><span class="n">length</span><span class="o">=</span><span class="m">1000</span><span class="p">)</span>
<span class="nf">for </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="m">1000</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">sim_sample</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sample_n</span><span class="p">(</span><span class="n">sim_df</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="o">=</span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="n">replace</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span><span class="w"> </span>
<span class="w">    </span><span class="n">sim_model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">lm</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">logx</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sim_sample</span><span class="p">)</span><span class="w">  </span>
<span class="w">    </span><span class="n">nobias_b1s</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sim_model</span><span class="o">$</span><span class="n">coefficients</span><span class="p">[[</span><span class="m">2</span><span class="p">]]</span>
<span class="p">}</span>

<span class="c1">#sampling many estimates of biased b1</span>
<span class="n">bias_b1s</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">vector</span><span class="p">(</span><span class="n">length</span><span class="o">=</span><span class="m">1000</span><span class="p">)</span>
<span class="nf">for </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="m">1000</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">sim_sample</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sample_n</span><span class="p">(</span><span class="n">sim_df</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="o">=</span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="n">replace</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span><span class="w"> </span>
<span class="w">    </span><span class="n">sim_model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">lm</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sim_sample</span><span class="p">)</span><span class="w">  </span><span class="c1">#fitting a linear model instead of nonlinear</span>
<span class="w">    </span><span class="n">bias_b1s</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sim_model</span><span class="o">$</span><span class="n">coefficients</span><span class="p">[[</span><span class="m">2</span><span class="p">]]</span>
<span class="p">}</span>

<span class="c1">#red = unbiased sampling distribution</span>
<span class="c1">#blue = biased sampling distribution</span>
<span class="n">bias_b1s_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">data.frame</span><span class="p">(</span><span class="n">bias_b1s</span><span class="p">)</span>
<span class="nf">gf_histogram</span><span class="p">(</span><span class="o">~</span><span class="w"> </span><span class="n">nobias_b1s</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">nobias_b1s_df</span><span class="p">,</span><span class="w"> </span><span class="n">fill</span><span class="o">=</span><span class="s">&quot;red&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="o">=</span><span class="m">0.5</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">    </span><span class="nf">gf_histogram</span><span class="p">(</span><span class="o">~</span><span class="w"> </span><span class="n">bias_b1s</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bias_b1s_df</span><span class="p">,</span><span class="w"> </span><span class="n">fill</span><span class="o">=</span><span class="s">&quot;blue&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="o">=</span><span class="m">0.5</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="nf">gf_vline</span><span class="p">(</span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">xintercept</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.5</span><span class="p">)</span><span class="w"> </span><span class="c1">#true b1 is 0.5</span>
<span class="nf">mean</span><span class="p">(</span><span class="n">bias_b1s</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>In each of these cases, if we were to take a model estimate at face value without investigating whether the data should actually be fit with a linear model, we’d make incorrect conclusions about the true effect size.</p>
<section id="id1">
<h3>How to check for assumption<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h3>
<p>If you are fitting a simple model with one predictor, you can check whether the best fitting line between the raw values of the predictor and the outcome has a distinctly curved shape:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">#adding a line with gf_smooth() will let you see whether the best fitting line should be curved or straight</span>
<span class="nf">gf_point</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">subsample</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="nf">gf_smooth</span><span class="p">(</span><span class="n">.</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>If it is quite curved, that indicates you should probably use a nonlinear model.</p>
<p>If you have a multivariable model, the way to tell is to plot the predictions the linear model would make on the x-axis and the residuals of the model on the y-axis:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">subsample</span><span class="o">$</span><span class="n">badpredicted</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span><span class="n">badmodel</span><span class="p">,</span><span class="w"> </span><span class="n">subsample</span><span class="p">)</span>
<span class="n">subsample</span><span class="o">$</span><span class="n">badresid</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">subsample</span><span class="o">$</span><span class="n">y</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span><span class="n">badmodel</span><span class="p">,</span><span class="w"> </span><span class="n">subsample</span><span class="p">)</span>

<span class="nf">gf_point</span><span class="p">(</span><span class="n">badresid</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">badpredicted</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="o">=</span><span class="n">subsample</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="nf">gf_smooth</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>If this plot has a clearly curved shape, that means there is a relationship between what a model predicts and how far off that prediction is. The model is systematically under or over predicting for certain data points and is violating the assumption of linearity. On the other hand, if the assumption is met, this predictor-residual plot should have a mostly straight across line:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">subsample</span><span class="o">$</span><span class="n">goodpredicted</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span><span class="n">goodmodel</span><span class="p">,</span><span class="w"> </span><span class="n">subsample</span><span class="p">)</span>
<span class="n">subsample</span><span class="o">$</span><span class="n">goodresid</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">subsample</span><span class="o">$</span><span class="n">y</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span><span class="n">goodmodel</span><span class="p">,</span><span class="w"> </span><span class="n">subsample</span><span class="p">)</span>

<span class="nf">gf_point</span><span class="p">(</span><span class="n">goodresid</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">goodpredicted</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="o">=</span><span class="n">subsample</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="nf">gf_smooth</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>The line in this plot is a little wobbly, but there’s no big curve emerging and no clear relationship between a model’s predictions and its residuals.</p>
</section>
<section id="id2">
<h3>What to do if assumption is violated<a class="headerlink" href="#id2" title="Permalink to this heading">#</a></h3>
<p>If this assumption is violated, you should investigate what sort of nonlinear shape would best explain the relationship between the predictor and the outcome and fit a model with that sort of structure instead of a linear model.</p>
</section>
</section>
<section id="assumption-3-exogeneity">
<h2>20.6 Assumption 3: exogeneity<a class="headerlink" href="#assumption-3-exogeneity" title="Permalink to this heading">#</a></h2>
<p><strong>Exogeneity</strong> refers to a state of a model where the unique variation explained by a predictor is <em>actually</em> explained by that predictor, and not by another variable that is left out of the model. If that variation is better explained by a variable that has been left out of the model, that is called <strong>endogeneity</strong> or <strong>omitted variable bias</strong>.</p>
<p>We’ve seen in other situations how predictor variables can share variance, and how adding multiple variables in a model lets us figure out which ones explain more unique variance than the other. If two predictors share variation but we leave one of them out of the model, the general linear model will lump all that shared variation into the one predictor still present. It will look like the remaining predictor is uniquely related to the outcome in a way that it actually isn’t.</p>
<p>Let’s see this in an example where we create some variables causally: <code class="docutils literal notranslate"><span class="pre">x1</span></code> is a random variable, while <code class="docutils literal notranslate"><span class="pre">x2</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> are each separately built out of adding some error to <code class="docutils literal notranslate"><span class="pre">x1</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">x2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">runif</span><span class="p">(</span><span class="m">10000</span><span class="p">,</span><span class="m">0</span><span class="p">,</span><span class="m">4</span><span class="p">)</span><span class="w">    </span><span class="c1">#defining a random variable, the true explanation of others</span>
<span class="n">ex</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">rnorm</span><span class="p">(</span><span class="m">10000</span><span class="p">,</span><span class="m">0</span><span class="p">,</span><span class="m">1</span><span class="p">)</span><span class="w">    </span><span class="c1">#some unexplained error</span>
<span class="n">x1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">x2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">ex</span><span class="w">             </span><span class="c1">#x2 explains x1</span>
<span class="n">ey</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">rnorm</span><span class="p">(</span><span class="m">10000</span><span class="p">,</span><span class="m">0</span><span class="p">,</span><span class="m">1</span><span class="p">)</span><span class="w">    </span><span class="c1">#some unexplained error</span>
<span class="n">y</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">x2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">ey</span><span class="w">              </span><span class="c1">#x2 is the only one that explains y</span>
<span class="n">sim_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">data.frame</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="w"> </span><span class="n">x2</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>If we build a model out of both x’s, we can see that <code class="docutils literal notranslate"><span class="pre">x2</span></code> is a significant predictor of <code class="docutils literal notranslate"><span class="pre">y</span></code> but <code class="docutils literal notranslate"><span class="pre">x1</span></code> is not:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">summary</span><span class="p">(</span><span class="nf">lm</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">x1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">x2</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="o">=</span><span class="n">sim_df</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>This is because the only reason <code class="docutils literal notranslate"><span class="pre">x1</span></code> would be related to <code class="docutils literal notranslate"><span class="pre">y</span></code> is because <code class="docutils literal notranslate"><span class="pre">x2</span></code> explains both of them.</p>
<p>However, if we leave out <code class="docutils literal notranslate"><span class="pre">x2</span></code> as a predictor and only use <code class="docutils literal notranslate"><span class="pre">x1</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">summary</span><span class="p">(</span><span class="nf">lm</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">x1</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="o">=</span><span class="n">sim_df</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>All the variation shared between <code class="docutils literal notranslate"><span class="pre">x1</span></code> and <code class="docutils literal notranslate"><span class="pre">x2</span></code> but specifically attributable to <code class="docutils literal notranslate"><span class="pre">x2</span></code> is now given to <code class="docutils literal notranslate"><span class="pre">x1</span></code>. It looks like it has a bigger effect on <code class="docutils literal notranslate"><span class="pre">y</span></code> than it really does in the true data generation process.</p>
<p>In the simulation we set above, the true effect of <code class="docutils literal notranslate"><span class="pre">x1</span></code> is ~0. An unbiased estimate of this effect would give us repeated b estimates that cluster around 0. But if we commit the omitted variable bias and leave <code class="docutils literal notranslate"><span class="pre">x2</span></code> out, the estimate for <code class="docutils literal notranslate"><span class="pre">x1</span></code> becomes biased:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">#sampling many estimates of unbiased b1</span>
<span class="n">nobias_b1s</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">vector</span><span class="p">(</span><span class="n">length</span><span class="o">=</span><span class="m">1000</span><span class="p">)</span>
<span class="nf">for </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="m">1000</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">sim_sample</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sample_n</span><span class="p">(</span><span class="n">sim_df</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="o">=</span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="n">replace</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span><span class="w"> </span>
<span class="w">  </span><span class="n">sim_model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">lm</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">x1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">x2</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sim_sample</span><span class="p">)</span><span class="w">  </span>
<span class="w">  </span><span class="n">nobias_b1s</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sim_model</span><span class="o">$</span><span class="n">coefficients</span><span class="p">[[</span><span class="m">2</span><span class="p">]]</span>
<span class="p">}</span>

<span class="c1">#sampling many estimates of biased b1</span>
<span class="n">bias_b1s</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">vector</span><span class="p">(</span><span class="n">length</span><span class="o">=</span><span class="m">1000</span><span class="p">)</span>
<span class="nf">for </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="m">1000</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">sim_sample</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sample_n</span><span class="p">(</span><span class="n">sim_df</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="o">=</span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="n">replace</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span><span class="w"> </span>
<span class="w">  </span><span class="n">sim_model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">lm</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">x1</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sim_sample</span><span class="p">)</span><span class="w">  </span><span class="c1">#fitting an endogenous model</span>
<span class="w">  </span><span class="n">bias_b1s</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sim_model</span><span class="o">$</span><span class="n">coefficients</span><span class="p">[[</span><span class="m">2</span><span class="p">]]</span>
<span class="p">}</span>

<span class="c1">#red = unbiased sampling distribution</span>
<span class="c1">#blue = biased sampling distribution</span>
<span class="n">nobias_b1s_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">data.frame</span><span class="p">(</span><span class="n">nobias_b1s</span><span class="p">)</span>
<span class="n">bias_b1s_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">data.frame</span><span class="p">(</span><span class="n">bias_b1s</span><span class="p">)</span>
<span class="nf">gf_histogram</span><span class="p">(</span><span class="o">~</span><span class="w"> </span><span class="n">nobias_b1s</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">nobias_b1s_df</span><span class="p">,</span><span class="w"> </span><span class="n">fill</span><span class="o">=</span><span class="s">&quot;red&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="o">=</span><span class="m">0.5</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">    </span><span class="nf">gf_histogram</span><span class="p">(</span><span class="o">~</span><span class="w"> </span><span class="n">bias_b1s</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bias_b1s_df</span><span class="p">,</span><span class="w"> </span><span class="n">fill</span><span class="o">=</span><span class="s">&quot;blue&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="o">=</span><span class="m">0.5</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="nf">gf_vline</span><span class="p">(</span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">xintercept</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="p">)</span><span class="w"> </span><span class="c1">#true b1 is 0</span>
<span class="nf">mean</span><span class="p">(</span><span class="n">bias_b1s</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>When this assumption is violated, the p-values are affected as well:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">set.seed</span><span class="p">(</span><span class="m">70</span><span class="p">)</span>
<span class="n">subsample</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sample_n</span><span class="p">(</span><span class="n">sim_df</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="o">=</span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="n">replace</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span>
<span class="n">goodmodel</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">lm</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">x1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">x2</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">subsample</span><span class="p">)</span>
<span class="nf">summary</span><span class="p">(</span><span class="n">goodmodel</span><span class="p">)</span><span class="o">$</span><span class="n">coefficients</span><span class="p">[</span><span class="m">3</span><span class="p">,</span><span class="m">4</span><span class="p">]</span><span class="w"> </span><span class="c1">#return p-value of x2</span>

<span class="n">badmodel</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">lm</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">x1</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">subsample</span><span class="p">)</span>
<span class="nf">summary</span><span class="p">(</span><span class="n">badmodel</span><span class="p">)</span><span class="o">$</span><span class="n">coefficients</span><span class="p">[</span><span class="m">2</span><span class="p">,</span><span class="m">4</span><span class="p">]</span><span class="w"> </span><span class="c1">#return p-value of x2</span>
</pre></div>
</div>
</div>
</div>
<p>Without <code class="docutils literal notranslate"><span class="pre">x2</span></code> in the model, we would erroneously conclude that <code class="docutils literal notranslate"><span class="pre">x1</span></code> has a large, signicant effect on <code class="docutils literal notranslate"><span class="pre">y</span></code> when in fact <code class="docutils literal notranslate"><span class="pre">x2</span></code> is the true explanatory variable.</p>
<section id="id3">
<h3>How to check for assumption<a class="headerlink" href="#id3" title="Permalink to this heading">#</a></h3>
<p>To check for whether or not the assumption of exogeneity is met or violated, you first have to think about what variable(s) might be a better explanation of <code class="docutils literal notranslate"><span class="pre">y</span></code>, and make sure to collect data on those variables. Then, you can fit a model between just <code class="docutils literal notranslate"><span class="pre">y</span></code> and <code class="docutils literal notranslate"><span class="pre">x1</span></code> (the model that may or may not be violating assumptions), and check whether the residuals of that model are correlated with the potentially omitted variable:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">subsample</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sample_n</span><span class="p">(</span><span class="n">sim_df</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="o">=</span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="n">replace</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span>
<span class="n">badmodel</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">lm</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">x1</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="o">=</span><span class="n">subsample</span><span class="p">)</span>
<span class="n">subsample</span><span class="o">$</span><span class="n">badresid</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">subsample</span><span class="o">$</span><span class="n">y</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span><span class="n">badmodel</span><span class="p">,</span><span class="w"> </span><span class="n">subsample</span><span class="p">)</span>

<span class="c1">#correlation between model residuals and the omitted variable</span>
<span class="nf">gf_point</span><span class="p">(</span><span class="n">badresid</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">x2</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="o">=</span><span class="n">subsample</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="nf">gf_lm</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>If there looks to be a correlation between the omitted variable and the model residuals, that means there is still unexplained variation in the model that the omitted variable could explain. On the other hand, if this correlation is close to 0, then the omitted variable would not explain any extra variation in the outcome variable after taking into account the included variable:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">#if we instead left x1 out of the model and only included good predictor x2</span>
<span class="n">goodmodel</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">lm</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">x2</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="o">=</span><span class="n">subsample</span><span class="p">)</span>
<span class="n">subsample</span><span class="o">$</span><span class="n">goodresid</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">subsample</span><span class="o">$</span><span class="n">y</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span><span class="n">goodmodel</span><span class="p">,</span><span class="w"> </span><span class="n">subsample</span><span class="p">)</span>

<span class="c1">#correlation between model residuals and the omitted variable</span>
<span class="nf">gf_point</span><span class="p">(</span><span class="n">goodresid</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">x1</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="o">=</span><span class="n">subsample</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="nf">gf_lm</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="id4">
<h3>What to do if assumption is violated<a class="headerlink" href="#id4" title="Permalink to this heading">#</a></h3>
<p>If it looks like an omitted variable should be included, the solution is easy - include it! That way you can figure out the true unique effect of each explanatory variable.</p>
</section>
</section>
<section id="assumption-4-no-multicollinearity">
<h2>20.7 Assumption 4: no multicollinearity<a class="headerlink" href="#assumption-4-no-multicollinearity" title="Permalink to this heading">#</a></h2>
<p>Speaking of correlated predictors: if you have two predictors <code class="docutils literal notranslate"><span class="pre">x1</span></code> and <code class="docutils literal notranslate"><span class="pre">x2</span></code> that are both possible explanatory variables of <code class="docutils literal notranslate"><span class="pre">y</span></code>, you should include them both in a general linear model. However, it is still dangerous to make conclusions about the model estimates if the predictors are highly correlated with each other. This is because, if the variation in <code class="docutils literal notranslate"><span class="pre">y</span></code> explained by the predictors is almost entirely overlapping, that means there is almost no unique variation attributable to either one. In that case, minute differences in the values of each variable can result in huge swings of the model estimates. This situation is called <strong>multicollinearity</strong>.</p>
<p>Here’s an example of what multicollinearity can do to model estimates. We’ll again generate predictor variables that are closely related to eachother, but this time both are a part of the data generation process:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">x1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">runif</span><span class="p">(</span><span class="m">10000</span><span class="p">,</span><span class="m">0</span><span class="p">,</span><span class="m">5</span><span class="p">)</span><span class="w">    </span><span class="c1">#defining a random variable</span>
<span class="n">ex</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">rnorm</span><span class="p">(</span><span class="m">10000</span><span class="p">,</span><span class="m">0</span><span class="p">,</span><span class="m">0.5</span><span class="p">)</span><span class="w">  </span><span class="c1">#some unexplained error between predictors</span>
<span class="n">x2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">x1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">ex</span><span class="w">             </span><span class="c1">#another, highly related predictor variable</span>
<span class="n">e</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">rnorm</span><span class="p">(</span><span class="m">10000</span><span class="p">,</span><span class="m">0</span><span class="p">,</span><span class="m">1</span><span class="p">)</span><span class="w">     </span><span class="c1">#some unexplained error</span>
<span class="n">y</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">0.5</span><span class="o">*</span><span class="n">x1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">e</span><span class="w">           </span><span class="c1">#true data generation process only involves x1</span>
<span class="n">sim_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">data.frame</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="w"> </span><span class="n">x2</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>In this data generation process, the true effect of <code class="docutils literal notranslate"><span class="pre">x1</span></code> should be 0.5. If we repeatedly sample and estimate the effect of <code class="docutils literal notranslate"><span class="pre">x1</span></code> in a multivariable model when there is multicollinearity:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">#sampling many estimates of unbiased b1</span>
<span class="n">nobias_b1s</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">vector</span><span class="p">(</span><span class="n">length</span><span class="o">=</span><span class="m">1000</span><span class="p">)</span>
<span class="nf">for </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="m">1000</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">sim_sample</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sample_n</span><span class="p">(</span><span class="n">sim_df</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="o">=</span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="n">replace</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span><span class="w"> </span>
<span class="w">  </span><span class="n">sim_model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">lm</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">x1</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sim_sample</span><span class="p">)</span><span class="w"> </span>
<span class="w">  </span><span class="n">nobias_b1s</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sim_model</span><span class="o">$</span><span class="n">coefficients</span><span class="p">[[</span><span class="m">2</span><span class="p">]]</span>
<span class="p">}</span>

<span class="c1">#sampling many estimates of biased b1</span>
<span class="n">bias_b1s</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">vector</span><span class="p">(</span><span class="n">length</span><span class="o">=</span><span class="m">1000</span><span class="p">)</span>
<span class="nf">for </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="m">1000</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">sim_sample</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sample_n</span><span class="p">(</span><span class="n">sim_df</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="o">=</span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="n">replace</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span><span class="w"> </span>
<span class="w">  </span><span class="n">sim_model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">lm</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">x1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">x2</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sim_sample</span><span class="p">)</span><span class="w"> </span>
<span class="w">  </span><span class="n">bias_b1s</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sim_model</span><span class="o">$</span><span class="n">coefficients</span><span class="p">[[</span><span class="m">2</span><span class="p">]]</span>
<span class="p">}</span>

<span class="c1">#red = unbiased sampling distribution</span>
<span class="c1">#blue = biased sampling distribution</span>
<span class="n">nobias_b1s_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">data.frame</span><span class="p">(</span><span class="n">nobias_b1s</span><span class="p">)</span>
<span class="n">bias_b1s_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">data.frame</span><span class="p">(</span><span class="n">bias_b1s</span><span class="p">)</span>
<span class="nf">gf_histogram</span><span class="p">(</span><span class="o">~</span><span class="w"> </span><span class="n">nobias_b1s</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">nobias_b1s_df</span><span class="p">,</span><span class="w"> </span><span class="n">fill</span><span class="o">=</span><span class="s">&quot;red&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="o">=</span><span class="m">0.5</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">    </span><span class="nf">gf_histogram</span><span class="p">(</span><span class="o">~</span><span class="w"> </span><span class="n">bias_b1s</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bias_b1s_df</span><span class="p">,</span><span class="w"> </span><span class="n">fill</span><span class="o">=</span><span class="s">&quot;blue&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="o">=</span><span class="m">0.5</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="nf">gf_vline</span><span class="p">(</span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">xintercept</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.5</span><span class="p">)</span><span class="w"> </span><span class="c1">#true b1 is 0.5</span>
<span class="nf">mean</span><span class="p">(</span><span class="n">bias_b1s</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We actually don’t get much bias in the estimator at all! So why is multicollinearity a problem? Well, consider the standard errors of both sampling distributions (the widths). The standard error of the sampling distribution is much higher in the case of multicollinearity. When standard error is higher, the variation in estimates from sample to sample is higher. This means that, even if the sampling distribution of β<sub>1</sub> is centered on the correct population parameter, any one b<sub>1</sub> estimate is likely to be farther away from the true population parameter.</p>
<p>This has the unfortunate effect of inflating Type I error of <code class="docutils literal notranslate"><span class="pre">x2</span></code>, <em>and</em> inflating Type II error of <code class="docutils literal notranslate"><span class="pre">x1</span></code>. It is both more likely to get sample estimates that look large when the true β<sub>1</sub> = 0, and it is more likely to get sample estimates that look small when the true β<sub>1</sub> is not 0. When multicollinearity is in place, we get an unbiased estimate of the effect size, but standard error will be biased high and thus it is harder to trust our significance decisions.</p>
<section id="id5">
<h3>How to check for assumption<a class="headerlink" href="#id5" title="Permalink to this heading">#</a></h3>
<p>In general, multicollinearity starts to become a potential issue if two predictors are correlated at least r &gt; 0.8. So to check for multicollinearity, you can look at the correlations between your model predictors before you fit a model. To do so, use the <code class="docutils literal notranslate"><span class="pre">cor()</span></code> function on the two variables in question:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">x1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">runif</span><span class="p">(</span><span class="m">10000</span><span class="p">,</span><span class="m">0</span><span class="p">,</span><span class="m">5</span><span class="p">)</span><span class="w">    </span>
<span class="n">ex</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">rnorm</span><span class="p">(</span><span class="m">10000</span><span class="p">,</span><span class="m">0</span><span class="p">,</span><span class="m">0.5</span><span class="p">)</span><span class="w">  </span>
<span class="n">x2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">x1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">ex</span>
<span class="nf">cor</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="w"> </span><span class="n">x2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>If you have many predictor variables which you want to check correlations for, <code class="docutils literal notranslate"><span class="pre">cor()</span></code> also works on an entire data frame. If you call it on a data frame instead of two individual predictors, it will return a <strong>correlation matrix</strong> showing the pairwise correlations between every unique combination of variables in the dataset:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">cor</span><span class="p">(</span><span class="n">sim_df</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Another way analysts check for multicollinearity is to use a tool called the Variance Inflation Factor (VIF). This checks how much standard errors are inflated by due to a relationship between predictors. The <code class="docutils literal notranslate"><span class="pre">car</span></code> package has a function <code class="docutils literal notranslate"><span class="pre">vif()</span></code> that will return the VIF values for each predictor in a model:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">install.packages</span><span class="p">(</span><span class="s">&quot;car&quot;</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">car</span><span class="p">)</span>

<span class="n">mc_model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">lm</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">x1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">x2</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sim_df</span><span class="p">)</span>
<span class="nf">vif</span><span class="p">(</span><span class="n">mc_model</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>To interpret these numbers, it is generally accepted that a VIF of at least 2.5 means multicollinearity is inflating standard errors enough to be concerned. The VIF of both predictors in our model is 9.28. That is above 2.5, so we should interpret that to mean any p-values from this model are untrustworthy.</p>
</section>
<section id="id6">
<h3>What to do if assumption is violated<a class="headerlink" href="#id6" title="Permalink to this heading">#</a></h3>
<p>The upside of multicollinearity is that, if two predictors share a lot of variation, you don’t lose much predictive value by removing one. The second predictor is essentially redundant. So one solution to multicollinearity is to drop one variable from the model.</p>
<p>The variable you should choose to drop is the one you think is least likely to be the cause of the outcome variable. If you’re unsure about the causal structure, another option is to average together the values of both predictors:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">x1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">runif</span><span class="p">(</span><span class="m">10000</span><span class="p">,</span><span class="m">0</span><span class="p">,</span><span class="m">5</span><span class="p">)</span><span class="w">    </span><span class="c1">#defining a random variable</span>
<span class="n">ex</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">rnorm</span><span class="p">(</span><span class="m">10000</span><span class="p">,</span><span class="m">0</span><span class="p">,</span><span class="m">0.5</span><span class="p">)</span><span class="w">  </span><span class="c1">#some unexplained error between predictors</span>
<span class="n">x2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">x1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">ex</span><span class="w">             </span><span class="c1">#another, highly related predictor variable</span>
<span class="n">e</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">rnorm</span><span class="p">(</span><span class="m">10000</span><span class="p">,</span><span class="m">0</span><span class="p">,</span><span class="m">1</span><span class="p">)</span><span class="w">     </span><span class="c1">#some unexplained error</span>
<span class="n">y</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">0.5</span><span class="o">*</span><span class="n">x1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">e</span>
<span class="n">x_combined</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">x1</span><span class="o">+</span><span class="n">x2</span><span class="p">)</span><span class="o">/</span><span class="m">2</span>
<span class="n">sim_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">data.frame</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span><span class="w"> </span><span class="n">x2</span><span class="p">,</span><span class="w"> </span><span class="n">x_combined</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">)</span>

<span class="c1">#sampling distribution of b1 when using x_combined as sole predictor</span>
<span class="n">combined_b1s</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">vector</span><span class="p">(</span><span class="n">length</span><span class="o">=</span><span class="m">1000</span><span class="p">)</span>
<span class="nf">for </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="m">1000</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">sim_sample</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sample_n</span><span class="p">(</span><span class="n">sim_df</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="o">=</span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="n">replace</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span><span class="w"> </span>
<span class="w">  </span><span class="n">sim_model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">lm</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">x_combined</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sim_sample</span><span class="p">)</span><span class="w"> </span>
<span class="w">  </span><span class="n">combined_b1s</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sim_model</span><span class="o">$</span><span class="n">coefficients</span><span class="p">[[</span><span class="m">2</span><span class="p">]]</span>
<span class="p">}</span>

<span class="c1">#red = unbiased sampling distribution</span>
<span class="c1">#blue = combined sampling distribution</span>
<span class="n">nobias_b1s_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">data.frame</span><span class="p">(</span><span class="n">nobias_b1s</span><span class="p">)</span>
<span class="n">combined_b1s_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">data.frame</span><span class="p">(</span><span class="n">combined_b1s</span><span class="p">)</span>
<span class="nf">gf_histogram</span><span class="p">(</span><span class="o">~</span><span class="w"> </span><span class="n">nobias_b1s</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">nobias_b1s_df</span><span class="p">,</span><span class="w"> </span><span class="n">fill</span><span class="o">=</span><span class="s">&quot;red&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="o">=</span><span class="m">0.5</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">    </span><span class="nf">gf_histogram</span><span class="p">(</span><span class="o">~</span><span class="w"> </span><span class="n">combined_b1s</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">combined_b1s_df</span><span class="p">,</span><span class="w"> </span><span class="n">fill</span><span class="o">=</span><span class="s">&quot;blue&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="o">=</span><span class="m">0.5</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="nf">gf_vline</span><span class="p">(</span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">xintercept</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.5</span><span class="p">)</span><span class="w"> </span><span class="c1">#true b1 is 0.5</span>
<span class="nf">mean</span><span class="p">(</span><span class="n">bias_b1s</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>This is the process that researchers follow when trying to address measurement error with multiple measures of a variable (e.g., with different questions on the same topic in a survey). Ideally, the only reason participants would answer differently on these questions is due to measurement error - they should be representing essentially the same information. Including each individual question in a model would thus be likely to introduce multicollinearity. Averaging them together gives you one composite measurement to make predictions with instead.</p>
</section>
</section>
<section id="assumption-5-normality-of-residuals">
<h2>20.8 Assumption 5: Normality of residuals<a class="headerlink" href="#assumption-5-normality-of-residuals" title="Permalink to this heading">#</a></h2>
<p>The general linear model also assumes that the residuals of a model will be normally distributed. If the error distribution is instead highly skewed, that means a few data points are particularly weird and having a stronger influence on the model estimate than the rest of the non-weird data.</p>
<p>Here’s an exaple of what non-normal residuals might look like:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">x</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">runif</span><span class="p">(</span><span class="m">10000</span><span class="p">,</span><span class="m">0</span><span class="p">,</span><span class="m">5</span><span class="p">)</span><span class="w">           </span><span class="c1">#defining a random variable</span>
<span class="n">e_norm</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">runif</span><span class="p">(</span><span class="m">10000</span><span class="p">,</span><span class="m">0</span><span class="p">,</span><span class="m">1</span><span class="p">)</span>
<span class="n">e_skew</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">log</span><span class="p">(</span><span class="nf">runif</span><span class="p">(</span><span class="m">10000</span><span class="p">,</span><span class="m">0</span><span class="p">,</span><span class="m">1</span><span class="p">))</span><span class="w"> </span><span class="c1">#error is skewed, not normal</span>
<span class="n">e_skew</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">e_skew</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nf">mean</span><span class="p">(</span><span class="n">e_skew</span><span class="p">)</span><span class="w"> </span><span class="c1">#mean centering the error term</span>
<span class="n">y_norm</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">0.5</span><span class="o">*</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">e_norm</span>
<span class="n">y_skew</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">0.5</span><span class="o">*</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">e_skew</span>
<span class="n">sim_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">data.frame</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y_skew</span><span class="p">,</span><span class="w"> </span><span class="n">y_norm</span><span class="p">)</span>
<span class="nf">gf_histogram</span><span class="p">(</span><span class="o">~</span><span class="w"> </span><span class="n">e_skew</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sim_df</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">#sampling distribution of b1 with normal errors</span>
<span class="n">nobias_b1s</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">vector</span><span class="p">(</span><span class="n">length</span><span class="o">=</span><span class="m">1000</span><span class="p">)</span>
<span class="nf">for </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="m">1000</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">sim_sample</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sample_n</span><span class="p">(</span><span class="n">sim_df</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="o">=</span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="n">replace</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span><span class="w"> </span>
<span class="w">  </span><span class="n">sim_model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">lm</span><span class="p">(</span><span class="n">y_norm</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sim_sample</span><span class="p">)</span><span class="w">  </span>
<span class="w">  </span><span class="n">nobias_b1s</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sim_model</span><span class="o">$</span><span class="n">coefficients</span><span class="p">[[</span><span class="m">2</span><span class="p">]]</span>
<span class="p">}</span>

<span class="c1">#sampling distribution of b1 with skewed errors</span>
<span class="n">bias_b1s</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">vector</span><span class="p">(</span><span class="n">length</span><span class="o">=</span><span class="m">1000</span><span class="p">)</span>
<span class="nf">for </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="m">1000</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">sim_sample</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sample_n</span><span class="p">(</span><span class="n">sim_df</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="o">=</span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="n">replace</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span><span class="w"> </span>
<span class="w">  </span><span class="n">sim_model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">lm</span><span class="p">(</span><span class="n">y_skew</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sim_sample</span><span class="p">)</span><span class="w">  </span>
<span class="w">  </span><span class="n">bias_b1s</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sim_model</span><span class="o">$</span><span class="n">coefficients</span><span class="p">[[</span><span class="m">2</span><span class="p">]]</span>
<span class="p">}</span>

<span class="c1">#red = unbiased sampling distribution</span>
<span class="c1">#blue = biased sampling distribution</span>
<span class="n">nobias_b1s_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">data.frame</span><span class="p">(</span><span class="n">nobias_b1s</span><span class="p">)</span>
<span class="n">bias_b1s_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">data.frame</span><span class="p">(</span><span class="n">bias_b1s</span><span class="p">)</span>
<span class="nf">gf_histogram</span><span class="p">(</span><span class="o">~</span><span class="w"> </span><span class="n">nobias_b1s</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">nobias_b1s_df</span><span class="p">,</span><span class="w"> </span><span class="n">fill</span><span class="o">=</span><span class="s">&quot;red&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="o">=</span><span class="m">0.5</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">    </span><span class="nf">gf_histogram</span><span class="p">(</span><span class="o">~</span><span class="w"> </span><span class="n">bias_b1s</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bias_b1s_df</span><span class="p">,</span><span class="w"> </span><span class="n">fill</span><span class="o">=</span><span class="s">&quot;blue&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="o">=</span><span class="m">0.5</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="nf">gf_vline</span><span class="p">(</span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">xintercept</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.5</span><span class="p">)</span><span class="w"> </span><span class="c1">#true b1 is 0.5</span>
<span class="nf">mean</span><span class="p">(</span><span class="n">bias_b1s</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>In this case, the sampling distribution is not really biased and the standard error is about what is should be.</p>
<p>The general linear model is actually quite robust to this assumption violation. The only time you need to be concerned about it is if you have a rather small sample (N&lt;50 or so). That’s because outliers with a large error term have more influence on model estimates when there aren’t many other data points to balance it out. If your dataset is large, outliers are less influential. This assumption was investigated and identified at a time when research was done more often with small samples, but in today’s age we better understand the value of large samples. Thus, this assumption has become less important.</p>
<p>Note that this assumption is not about the normality of the <em>variables</em> in a model. That is a common misconception. It is specifically about the normality of the residuals.</p>
<section id="id7">
<h3>How to check for assumption<a class="headerlink" href="#id7" title="Permalink to this heading">#</a></h3>
<p>To check for the normality of residuals, simply plot the histogram of the model residuals and see if the distribution shape is strongly not normal:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">subsample</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sample_n</span><span class="p">(</span><span class="n">sim_df</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="o">=</span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="n">replace</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span>
<span class="n">sub_model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">lm</span><span class="p">(</span><span class="n">y</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">subsample</span><span class="p">)</span>
<span class="n">subsample</span><span class="o">$</span><span class="n">resid</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">subsample</span><span class="o">$</span><span class="n">y</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span><span class="n">sub_model</span><span class="p">,</span><span class="w"> </span><span class="n">subsample</span><span class="p">)</span>
<span class="nf">gf_histogram</span><span class="p">(</span><span class="o">~</span><span class="w"> </span><span class="n">resid</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">subsample</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="id8">
<h3>What to do if assumption is violated<a class="headerlink" href="#id8" title="Permalink to this heading">#</a></h3>
<p>There are a number of options available for addressing violations of the normality assumption. One might choose to transform the outcome variable, or use a version of the general linear model called <a class="reference external" href="https://stats.oarc.ucla.edu/r/dae/robust-regression/">Robust Regression</a> that downweights the influence of data outliers.</p>
<p>But as mentioned, if you have a sample that is not small, you don’t need to worry about violating this assumption. So another option is just to collect a sample large enough not to worry. As we’ve seen in other chapters, a larger sample is a good idea anyways if you have the resources to collect it!</p>
</section>
</section>
<section id="assumption-6-homoscedasticity">
<h2>20.9 Assumption 6: Homoscedasticity<a class="headerlink" href="#assumption-6-homoscedasticity" title="Permalink to this heading">#</a></h2>
<p>This long word refers to the assumption that residuals have constant variance at each level of model prediction.</p>
<p>The best way to understand this is visually. We will make a plot where, as a predictor <code class="docutils literal notranslate"><span class="pre">x</span></code> increases, the error in the model increases as well:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">x</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">runif</span><span class="p">(</span><span class="m">10000</span><span class="p">,</span><span class="m">0</span><span class="p">,</span><span class="m">5</span><span class="p">)</span><span class="w">             </span><span class="c1">#defining a random variable</span>
<span class="n">e_het</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">x</span><span class="o">*</span><span class="nf">rnorm</span><span class="p">(</span><span class="m">10000</span><span class="p">,</span><span class="m">0</span><span class="p">,</span><span class="m">1</span><span class="p">)</span><span class="w">    </span><span class="c1">#error gets wider as a function of predictor</span>
<span class="n">e_hom</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">rnorm</span><span class="p">(</span><span class="m">10000</span><span class="p">,</span><span class="m">0</span><span class="p">,</span><span class="m">1</span><span class="p">)</span>
<span class="n">y_het</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">0.5</span><span class="o">*</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">e_het</span>
<span class="n">y_hom</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">0.5</span><span class="o">*</span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">e_hom</span>
<span class="n">sim_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">data.frame</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y_het</span><span class="p">,</span><span class="w"> </span><span class="n">y_hom</span><span class="p">)</span>

<span class="n">subsample</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sample_n</span><span class="p">(</span><span class="n">sim_df</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="o">=</span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="n">replace</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span>
<span class="n">sub_model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">lm</span><span class="p">(</span><span class="n">y_het</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">subsample</span><span class="p">)</span>
<span class="n">subsample</span><span class="o">$</span><span class="n">resid</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">subsample</span><span class="o">$</span><span class="n">y_het</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span><span class="n">sub_model</span><span class="p">,</span><span class="w"> </span><span class="n">subsample</span><span class="p">)</span>
<span class="nf">gf_point</span><span class="p">(</span><span class="n">resid</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="o">=</span><span class="n">subsample</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>When the assumption of <strong>homoscedasticity</strong> is met, this plot should look like a consistent cloud where the range of residuals is about the same for every level of x. If there is instead a clear cone shape like in this plot, where residuals are more variable at certain levels of x, the assumption is violated. Instead, this is a case of <strong>heteroscedasticity</strong>.</p>
<p>This situation will occur if measurement error is correlated with values of the predictor. When this assumption is violated, model estimates are unbiased across many samples, but are more variable. This means the standard error is inflated and we have a harder time trusting our significance testing results.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">#sampling distribution of b1 with homoscedastic errors</span>
<span class="n">nobias_b1s</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">vector</span><span class="p">(</span><span class="n">length</span><span class="o">=</span><span class="m">1000</span><span class="p">)</span>
<span class="nf">for </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="m">1000</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">sim_sample</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sample_n</span><span class="p">(</span><span class="n">sim_df</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="o">=</span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="n">replace</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span><span class="w"> </span>
<span class="w">  </span><span class="n">sim_model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">lm</span><span class="p">(</span><span class="n">y_hom</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sim_sample</span><span class="p">)</span><span class="w">  </span><span class="c1">#fitting a linear model instead of nonlinear</span>
<span class="w">  </span><span class="n">nobias_b1s</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sim_model</span><span class="o">$</span><span class="n">coefficients</span><span class="p">[[</span><span class="m">2</span><span class="p">]]</span>
<span class="p">}</span>

<span class="c1">#sampling distribution of b1 with heteroscedastic errors</span>
<span class="n">bias_b1s</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">vector</span><span class="p">(</span><span class="n">length</span><span class="o">=</span><span class="m">1000</span><span class="p">)</span>
<span class="nf">for </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="m">1000</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">sim_sample</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sample_n</span><span class="p">(</span><span class="n">sim_df</span><span class="p">,</span><span class="w"> </span><span class="n">size</span><span class="o">=</span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="n">replace</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">)</span><span class="w"> </span>
<span class="w">  </span><span class="n">sim_model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">lm</span><span class="p">(</span><span class="n">y_het</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sim_sample</span><span class="p">)</span><span class="w">  </span><span class="c1">#fitting a linear model instead of nonlinear</span>
<span class="w">  </span><span class="n">bias_b1s</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sim_model</span><span class="o">$</span><span class="n">coefficients</span><span class="p">[[</span><span class="m">2</span><span class="p">]]</span>
<span class="p">}</span>

<span class="c1">#red = unbiased sampling distribution</span>
<span class="c1">#blue = biased sampling distribution</span>
<span class="n">nobias_b1s_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">data.frame</span><span class="p">(</span><span class="n">nobias_b1s</span><span class="p">)</span>
<span class="n">bias_b1s_df</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">data.frame</span><span class="p">(</span><span class="n">bias_b1s</span><span class="p">)</span>
<span class="nf">gf_histogram</span><span class="p">(</span><span class="o">~</span><span class="w"> </span><span class="n">nobias_b1s</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">nobias_b1s_df</span><span class="p">,</span><span class="w"> </span><span class="n">fill</span><span class="o">=</span><span class="s">&quot;red&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="o">=</span><span class="m">0.5</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w">    </span><span class="nf">gf_histogram</span><span class="p">(</span><span class="o">~</span><span class="w"> </span><span class="n">bias_b1s</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">bias_b1s_df</span><span class="p">,</span><span class="w"> </span><span class="n">fill</span><span class="o">=</span><span class="s">&quot;blue&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="o">=</span><span class="m">0.5</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="nf">gf_vline</span><span class="p">(</span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">xintercept</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.5</span><span class="p">)</span><span class="w"> </span><span class="c1">#true b1 is 0.5</span>
<span class="nf">mean</span><span class="p">(</span><span class="n">bias_b1s</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="id9">
<h3>How to check for assumption<a class="headerlink" href="#id9" title="Permalink to this heading">#</a></h3>
<p>To check whether residuals are homoskedastic or heteroskedastic, one should make the same plot as above and look for the distinctive cone shape in the cloud of residuals.</p>
</section>
<section id="id10">
<h3>What to do if assumption is violated<a class="headerlink" href="#id10" title="Permalink to this heading">#</a></h3>
<p>Transformation of the dependent variable or robust regression are also ways of dealing with this assumption violation.</p>
</section>
</section>
<section id="assumption-7-independence-of-residuals">
<h2>20.10 Assumption 7: independence of residuals<a class="headerlink" href="#assumption-7-independence-of-residuals" title="Permalink to this heading">#</a></h2>
<p>Finally, the last assumption of the general linear model is that residuals are independent of each other. That means, even if you know the residual of one data point, you’re not able to predict what the residual will be for the next data point.</p>
<p>Violations of this assumption occur when the data points themselves are not independent of each other. Values on either the predictors and/or the outcomes for one data point help narrow down the possible values of another data point. The most common situation for this to occur is when you have time series data, or clustered data.</p>
<p>Time series data is data that is collected over time and has a distinct order - data point #2 comes after data point #1, #3 after #2, etc. Often in time series data, the value of the prior data point bleeds over into the value of the following data point. This is easy to see in plots of time series like yearly temperatures. Temperature in your area can vary from 0 to 80 depending on the time of year, but if you knew it was 60 degrees yesterday, it’s likely that the temperature today will be something similar.</p>
<img src="images/ch20-temperature.png" width="600">
<p>Another example on non-independence among data points is when data come from similar sources. Imagine you are testing the effect of time spent studying on test grades, and you collect data from students in three different sections of the same class. Each class has its own quirks - one might be at the end of the day so students are often tired, one might have a particularly tough professor whose tests are hard to study for, etc. In this case, the relationship between study time and test grade might depend on which class you’re in. We would call this data clustered, within class.</p>
<p>Another example of clustered data are multiple measurements taken from the same person. Say you are training students on a new study technique, so you measure their grades before learning the technique, and then again after learning it. You have two data points per person, called <strong>paired samples</strong> or <strong>repeated measures</strong>. You can imagine how well this technique works will depend on the particular person using it, so these data are clustered within person.</p>
<p>Here’s a simulation what happens to models when you violate the assumption of independence with paired samples. We’ll measure the test scores of each student in the class twice, before and after study training:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">pretest</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">71</span><span class="p">,</span><span class="w"> </span><span class="m">73</span><span class="p">,</span><span class="w"> </span><span class="m">83</span><span class="p">,</span><span class="w"> </span><span class="m">93</span><span class="p">,</span><span class="w"> </span><span class="m">74</span><span class="p">,</span><span class="w"> </span><span class="m">84</span><span class="p">,</span><span class="w"> </span><span class="m">70</span><span class="p">,</span><span class="w"> </span><span class="m">88</span><span class="p">,</span><span class="w"> </span><span class="m">64</span><span class="p">,</span><span class="w"> </span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="m">67</span><span class="p">,</span><span class="w"> </span><span class="m">72</span><span class="p">,</span><span class="w"> </span><span class="m">63</span><span class="p">,</span><span class="w"> </span><span class="m">86</span><span class="p">,</span><span class="w"> </span><span class="m">81</span><span class="p">)</span>
<span class="n">posttest</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">75</span><span class="p">,</span><span class="w"> </span><span class="m">73</span><span class="p">,</span><span class="w"> </span><span class="m">82</span><span class="p">,</span><span class="w"> </span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="m">82</span><span class="p">,</span><span class="w"> </span><span class="m">84</span><span class="p">,</span><span class="w"> </span><span class="m">77</span><span class="p">,</span><span class="w"> </span><span class="m">89</span><span class="p">,</span><span class="w"> </span><span class="m">60</span><span class="p">,</span><span class="w"> </span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="m">67</span><span class="p">,</span><span class="w"> </span><span class="m">82</span><span class="p">,</span><span class="w"> </span><span class="m">66</span><span class="p">,</span><span class="w"> </span><span class="m">87</span><span class="p">,</span><span class="w"> </span><span class="m">80</span><span class="p">)</span>
<span class="n">student</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="m">3</span><span class="p">,</span><span class="w"> </span><span class="m">4</span><span class="p">,</span><span class="w"> </span><span class="m">5</span><span class="p">,</span><span class="w"> </span><span class="m">6</span><span class="p">,</span><span class="w"> </span><span class="m">7</span><span class="p">,</span><span class="w"> </span><span class="m">8</span><span class="p">,</span><span class="w"> </span><span class="m">9</span><span class="p">,</span><span class="w"> </span><span class="m">10</span><span class="p">,</span><span class="w"> </span><span class="m">11</span><span class="p">,</span><span class="w"> </span><span class="m">12</span><span class="p">,</span><span class="w"> </span><span class="m">13</span><span class="p">,</span><span class="w"> </span><span class="m">14</span><span class="p">,</span><span class="w"> </span><span class="m">15</span><span class="p">)</span>

<span class="n">test_scores</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">data.frame</span><span class="p">(</span><span class="n">student</span><span class="p">,</span><span class="w"> </span><span class="n">pretest</span><span class="p">,</span><span class="w"> </span><span class="n">posttest</span><span class="p">)</span>
<span class="n">test_scores</span>
</pre></div>
</div>
</div>
</div>
<p>At first glance at this study, we may be tempted to treat “score” as the outcome variable, and “pre/post timing” as the explanatory variable. This way we could see if there’s a significant difference in test scores from before vs. after the study training. So maybe we’d actually want to arrange the dataset this way:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">score</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">71</span><span class="p">,</span><span class="w"> </span><span class="m">75</span><span class="p">,</span><span class="w"> </span><span class="m">73</span><span class="p">,</span><span class="w"> </span><span class="m">73</span><span class="p">,</span><span class="w"> </span><span class="m">83</span><span class="p">,</span><span class="w"> </span><span class="m">82</span><span class="p">,</span><span class="w"> </span><span class="m">93</span><span class="p">,</span><span class="w"> </span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="m">74</span><span class="p">,</span><span class="w"> </span><span class="m">82</span><span class="p">,</span><span class="w"> </span><span class="m">84</span><span class="p">,</span><span class="w"> </span><span class="m">84</span><span class="p">,</span><span class="w"> </span><span class="m">70</span><span class="p">,</span><span class="w"> </span><span class="m">77</span><span class="p">,</span><span class="w"> </span><span class="m">88</span><span class="p">,</span><span class="w"> </span><span class="m">89</span><span class="p">,</span><span class="w"> </span><span class="m">64</span><span class="p">,</span><span class="w"> </span><span class="m">60</span><span class="p">,</span><span class="w"> </span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="m">100</span><span class="p">,</span>
<span class="w">          </span><span class="m">67</span><span class="p">,</span><span class="w"> </span><span class="m">67</span><span class="p">,</span><span class="w"> </span><span class="m">72</span><span class="p">,</span><span class="w"> </span><span class="m">82</span><span class="p">,</span><span class="w"> </span><span class="m">63</span><span class="p">,</span><span class="w"> </span><span class="m">66</span><span class="p">,</span><span class="w"> </span><span class="m">86</span><span class="p">,</span><span class="w"> </span><span class="m">87</span><span class="p">,</span><span class="w"> </span><span class="m">81</span><span class="p">,</span><span class="w"> </span><span class="m">80</span><span class="p">)</span>
<span class="n">timing</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s">&quot;before&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;post&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;before&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;post&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;before&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;post&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;before&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;post&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;before&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;post&quot;</span><span class="p">,</span><span class="w"> </span>
<span class="w">           </span><span class="s">&quot;before&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;post&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;before&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;post&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;before&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;post&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;before&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;post&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;before&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;post&quot;</span><span class="p">,</span><span class="w"> </span>
<span class="w">           </span><span class="s">&quot;before&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;post&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;before&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;post&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;before&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;post&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;before&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;post&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;before&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;post&quot;</span><span class="p">)</span>
<span class="n">student</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="m">3</span><span class="p">,</span><span class="w"> </span><span class="m">3</span><span class="p">,</span><span class="w"> </span><span class="m">4</span><span class="p">,</span><span class="w"> </span><span class="m">4</span><span class="p">,</span><span class="w"> </span><span class="m">5</span><span class="p">,</span><span class="w"> </span><span class="m">5</span><span class="p">,</span><span class="w"> </span><span class="m">6</span><span class="p">,</span><span class="w"> </span><span class="m">6</span><span class="p">,</span><span class="w"> </span><span class="m">7</span><span class="p">,</span><span class="w"> </span><span class="m">7</span><span class="p">,</span><span class="w"> </span><span class="m">8</span><span class="p">,</span><span class="w"> </span><span class="m">8</span><span class="p">,</span><span class="w"> </span><span class="m">9</span><span class="p">,</span><span class="w"> </span><span class="m">9</span><span class="p">,</span><span class="w"> </span><span class="m">10</span><span class="p">,</span><span class="w"> </span><span class="m">10</span><span class="p">,</span><span class="w"> </span><span class="m">11</span><span class="p">,</span><span class="w"> </span><span class="m">11</span><span class="p">,</span><span class="w"> </span><span class="m">12</span><span class="p">,</span><span class="w"> </span><span class="m">12</span><span class="p">,</span><span class="w"> </span>
<span class="w">            </span><span class="m">13</span><span class="p">,</span><span class="w"> </span><span class="m">13</span><span class="p">,</span><span class="w"> </span><span class="m">14</span><span class="p">,</span><span class="w"> </span><span class="m">14</span><span class="p">,</span><span class="w"> </span><span class="m">15</span><span class="p">,</span><span class="w"> </span><span class="m">15</span><span class="p">)</span>

<span class="n">test_scores2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">data.frame</span><span class="p">(</span><span class="n">student</span><span class="p">,</span><span class="w"> </span><span class="n">score</span><span class="p">,</span><span class="w"> </span><span class="n">timing</span><span class="p">)</span>
<span class="n">test_scores2</span>
</pre></div>
</div>
</div>
</div>
<p>This way we could build a model Y<sub>i</sub> = b<sub>0</sub> + b<sub>1</sub>X</sub>i + e<sub>i</sub> where Y<sub>i</sub> is each test score, b<sub>0</sub> is the mean of scores in the “before” group, b<sub>1</sub> is the difference in means between “before” and “post”, and X<sub>i</sub> is whether a score was collected before the studying training or post-training.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">summary</span><span class="p">(</span><span class="nf">lm</span><span class="p">(</span><span class="n">score</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">timing</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">test_scores2</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>The problem with this approach is that this assumes each test score is independent - that student 10’s before score has no bearing on student 10’s post score. But as we just talked about, this is likely not the case. Both of these scores are drawn from the population of student 10, which is probably different than the population of scores for other students. Each student is independent of each other, but within one student their scores are not independent.</p>
<p>If your data are non-independent like this, adding new data from the same person to a dataset doesn’t buy you a whole new degree of freedom - some of that information is already present in the dataset in the form of the person’s previous data. Building a model on non-independent data means the model will overestimate the degrees of freedom available in the model. The model estimates will be unbiased, but the standard errors (which are calculated with degrees of freedom) will be biased downward. Our significance test decisions will be wrong more often.</p>
<section id="id11">
<h3>How to check for assumption<a class="headerlink" href="#id11" title="Permalink to this heading">#</a></h3>
<p>There are some sophisticated ways to see if data are non-independent within clusters. For time series, you can calculate the <a class="reference external" href="https://corporatefinanceinstitute.com/resources/data-science/autocorrelation/">autocorrelation</a> in the data. For clustered data, there’s a measure called the <a class="reference external" href="https://www.statisticshowto.com/intraclass-correlation/">Intraclass Correlation</a>. For the purposes of this class, let’s stick to using our intuition about whether or not our data are a time series, or coming from clustered sources.</p>
</section>
<section id="id12">
<h3>What to do if assumption is violated<a class="headerlink" href="#id12" title="Permalink to this heading">#</a></h3>
<p>The easiest way to deal with clustered data is to make it so you only have one datapoint from each independent source. In the case where there are two scores per person, that could mean calculating one change score per person, rather than having separate pre and post scores:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">test_scores</span><span class="o">$</span><span class="n">testchange</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">test_scores</span><span class="o">$</span><span class="n">posttest</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">test_scores</span><span class="o">$</span><span class="n">pretest</span>

<span class="n">test_scores</span>
</pre></div>
</div>
</div>
</div>
<p>This solves our data independence issue. Now we have just one datapoint per person, and we know the people are independent of each other. Now we can build a model with the change scores as our outcome variable. Specifically, if we’re interested in asking whether those changes tended to be non-zero, we can use the empty model to find the average score change:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">summary</span><span class="p">(</span><span class="nf">lm</span><span class="p">(</span><span class="n">testchange</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="kc">NULL</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">test_scores</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>The estimates for the effect of time from both versions of this model are the same, but the standard errors are different and thus the p-values are different.</p>
<p>Sometimes it doesn’t make sense to combine scores within independent sources, however. In the case where student data are clustered within three classes, combining data into the class level would only leave behind three data points. In cases like this, there are advanced statistical methods called <strong>multilevel modeling</strong> or <strong>mixed effects modeling</strong>. A common graduate school class for psychologists is a stats class on just these methods.</p>
<p>For time series, there are also advanced methods designed for those kind of data in particular.</p>
<p>In the context of this class, we won’t deal with these more advanced types of non-independent data. But you should be aware of them so that you don’t violate this assumption in the future.</p>
</section>
</section>
<section id="summary">
<h2>20.11 Summary<a class="headerlink" href="#summary" title="Permalink to this heading">#</a></h2>
<p>In summary, one should combine bias investigation with significance testing and effect size evaluation when building models in order to make the best conclusions about your data. This will enable you to understand your model’s performance in terms of its magnitude, uncertainty, and bias.</p>
<p>For a quick reference to the assumptions that the general linear model makes and what kind of bias happens when these assumptions are violated, look over the table below.</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head text-center"><p>GLM Assumption</p></th>
<th class="head text-center"><p>Violation</p></th>
<th class="head text-center"><p>Biased estimate?</p></th>
<th class="head text-center"><p>Biased standard error?</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p>Representative sample</p></td>
<td class="text-center"><p>non-representative sample</p></td>
<td class="text-center"><p>√</p></td>
<td class="text-center"><p>√</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>Linear relationship</p></td>
<td class="text-center"><p>non-linear relationship</p></td>
<td class="text-center"><p>√</p></td>
<td class="text-center"><p></p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>Exogeneity</p></td>
<td class="text-center"><p>Endogeneity/omitted variable</p></td>
<td class="text-center"><p>√</p></td>
<td class="text-center"><p>√</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>No multicollinearity</p></td>
<td class="text-center"><p>Multicollinearity</p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p>√</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>Normal residuals</p></td>
<td class="text-center"><p>Non-normal residuals</p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p>none if N is large</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p>Homoskedasticity</p></td>
<td class="text-center"><p>Heteroskedasticity</p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p>√</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p>Independent residuals</p></td>
<td class="text-center"><p>Non-independent residuals</p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p>√</p></td>
</tr>
</tbody>
</table>
</section>
<section id="chapter-summary">
<h2>Chapter Summary<a class="headerlink" href="#chapter-summary" title="Permalink to this heading">#</a></h2>
<p>After reading this chapter, you should be able to:</p>
<ul class="simple">
<li><p>Explain the difference between error and bias</p></li>
<li><p>Explain the difference between biased and unbiased estimators</p></li>
<li><p>Describe the assumptions of the general linear model where violations lead to model bias</p></li>
<li><p>Remember whether violations of each assumption lead to biased model estimates and/or biased standard errors</p></li>
</ul>
<p><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-20.ipynb">Next: Chapter 20 - Bayesian Statistics</a></p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "smburns47/Psyc158",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "r"
        },
        kernelOptions: {
            name: "ir",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'ir'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#error-vs-bias">20.1 Error vs. bias</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#unbiased-vs-biased-estimates">20.2 Unbiased vs. biased estimates</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#checking-bias-in-models">20.3 Checking bias in models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assumption-1-representative-sample">20.4 Assumption 1: representative sample</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-check-for-assumption">How to check for assumption</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-to-do-if-assumption-is-violated">What to do if assumption is violated</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assumption-2-linear-relationship-between-variables">20.5 Assumption 2: linear relationship between variables</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">How to check for assumption</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">What to do if assumption is violated</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assumption-3-exogeneity">20.6 Assumption 3: exogeneity</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">How to check for assumption</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">What to do if assumption is violated</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assumption-4-no-multicollinearity">20.7 Assumption 4: no multicollinearity</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">How to check for assumption</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">What to do if assumption is violated</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assumption-5-normality-of-residuals">20.8 Assumption 5: Normality of residuals</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">How to check for assumption</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">What to do if assumption is violated</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assumption-6-homoscedasticity">20.9 Assumption 6: Homoscedasticity</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">How to check for assumption</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">What to do if assumption is violated</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assumption-7-independence-of-residuals">20.10 Assumption 7: independence of residuals</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">How to check for assumption</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">What to do if assumption is violated</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">20.11 Summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#chapter-summary">Chapter Summary</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Shannon Burns
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>