

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Chapter 11 - Adding an Explanatory Variable &#8212; Pomona Psych 158 Online Textbook</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapter-11';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Pomona College Psych 158 Online Textbook
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Unit 1 Describing Data</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-1.ipynb">Chapter 1 - Introduction to Statistical Thinking</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-2.ipynb">Chapter 2 - What are Data</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-3.ipynb">Chapter 3 - Organizing Data</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-4.ipynb">Chapter 4 - Cleaning Data</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-5.ipynb">Chapter 5 - Describing Data</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-6.ipynb">Chapter 6 - Variation in Multiple Variables</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-7.ipynb">Chapter 7 - Principles of Data Visualization</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Unit 2 - Modeling Data</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-8.ipynb">Chapter 8 - Where Data Come From</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-9.ipynb">Chapter 9 - Modeling the Data Generation Process</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-10.ipynb">Chapter 10 - Quantifying Model Error</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-11.ipynb">Chapter 11 - Adding an Explanatory Variable</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-12.ipynb">Chapter 12 - Quantitative Predictor Models</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-13.ipynb">Chapter 13 - Multivariable Models</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-14.ipynb">Chapter 14 - Models with Moderation</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Unit 3 - Evaluating Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-15.ipynb">Chapter 15 - Estimating Populations</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-16.ipynb">Chapter 16 - Significance Testing</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-17.ipynb">Chapter 17 - Significance Testing Whole Models</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-18.ipynb">Chapter 18 - Effect Sizes &amp; Statistical Power</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-19.ipynb">Chapter 19 - Model Bias</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-20.ipynb">Chapter 20 - Alternate Approaches - Traditional Statistical Tools</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-21.ipynb">Chapter 21 - Lying with Statistics</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/smburns47/Psyc158/main?urlpath=tree/chapter-11.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onBinder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="initThebeSBT()"
  class="btn btn-sm btn-launch-thebe dropdown-item"
  title="Launch Thebe"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="btn__text-container">Live Code</span>
</button>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/smburns47/Psyc158" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/smburns47/Psyc158/issues/new?title=Issue%20on%20page%20%2Fchapter-11.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/chapter-11.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Chapter 11 - Adding an Explanatory Variable</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#explaining-variation">11.1 Explaining variation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#adding-an-explanatory-variable-to-the-model">11.2 Adding an explanatory variable to the model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#specifying-the-model-form">11.3 Specifying the model form</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fitting-the-one-variable-model">11.4 Fitting the one-variable model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generating-predictions-from-the-model">11.5 Generating predictions from the model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quantifying-model-fit">11.6 Quantifying model fit</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#improvement-over-empty-model">11.7 Improvement over empty model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-historical-note-the-t-test">11.8 A historical note - the t-test</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#one-sample-t-test">One-sample t-test</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#independent-samples-t-test">Independent samples t-test</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#chapter-summary">Chapter summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#new-concepts">New concepts</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <p><a class="reference external" href="https://www.shannonmburns.com/Psyc158/intro.html">Back to Table of Contents</a></p>
<p><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-10.ipynb">Previous: Chapter 10 - Quantifying Model Error</a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Run this first so it&#39;s ready by the time you need it</span>
<span class="nf">install.packages</span><span class="p">(</span><span class="s">&quot;ggformula&quot;</span><span class="p">)</span>
<span class="nf">install.packages</span><span class="p">(</span><span class="s">&quot;supernova&quot;</span><span class="p">)</span>
<span class="nf">install.packages</span><span class="p">(</span><span class="s">&quot;dplyr&quot;</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">ggformula</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">supernova</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">dplyr</span><span class="p">)</span>

<span class="n">studentdata</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">read.csv</span><span class="p">(</span><span class="s">&quot;https://raw.githubusercontent.com/smburns47/Psyc158/main/studentdata.csv&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section class="tex2jax_ignore mathjax_ignore" id="chapter-11-adding-an-explanatory-variable">
<h1>Chapter 11 - Adding an Explanatory Variable<a class="headerlink" href="#chapter-11-adding-an-explanatory-variable" title="Permalink to this heading">#</a></h1>
<p>As we discussed previously, in the absence of other information about the objects being studied, the mean of our sample (an estimate of the population mean) is the best single-number we have for predicting the value of any one data point. Making a statistical model for this prediction (called the null model) would look like:</p>
<div class="math notranslate nohighlight">
\[ Y_i = b_0 + e_i \]</div>
<p>Where <span class="math notranslate nohighlight">\(b_0\)</span> represents the mean of our data sample for predicting the values of <span class="math notranslate nohighlight">\(Y_i\)</span>. <span class="math notranslate nohighlight">\(b_0\)</span> is our model, our prediction, of what <span class="math notranslate nohighlight">\(\hat{Y}_i\)</span> will be, while <span class="math notranslate nohighlight">\(e_i\)</span> is all the variation in <span class="math notranslate nohighlight">\(Y_i\)</span> that we couldn’t explain with this model. Since <span class="math notranslate nohighlight">\(b_0\)</span> is the same number for every data point, if <span class="math notranslate nohighlight">\(Y_i\)</span> has any variation at all, <span class="math notranslate nohighlight">\(e_i\)</span> will be substantial. So the null model isn’t all that useful for making predictions in the end.</p>
<p>However, we don’t have to be limited to using <em>just</em> a single number like the mean to make predictions. After all, we’re interested in the data generation process, and we probably think there is some explanatory variable that contributes to the value of an outcome variable. For instance, earlier in chapter 9 we created a hypothesis about what contributes to reaction time of labeling the color of a word:</p>
<div class="math notranslate nohighlight">
\[reaction time_i = \beta_1congruence_i + \beta_2speed_i + \beta_3font_i + \beta_4coordination_i\]</div>
<p>This statistical model clearly has more things going into the equation. In this chapter we’ll learn how to add more pieces to a statistical model to start incorporating our thoughts about the data generation process and make better predictions. We started with the null model in order to get some important ideas across, but certainly that’s not where we want to end up. It is time we start building models that include explanatory variables. We will still use the null model, but only as a reference point.</p>
<section id="explaining-variation">
<h2>11.1 Explaining variation<a class="headerlink" href="#explaining-variation" title="Permalink to this heading">#</a></h2>
<p>Let’s first review what we mean by explaining variation. In chapter 6, we developed an intuitive idea of explanation by comparing the distribution of one variable across two different groups. So, for example, we looked at the distribution of thumb length broken down by sex, which we can see in the two histograms below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">gf_histogram</span><span class="p">(</span><span class="o">~</span><span class="w"> </span><span class="n">Thumb</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">studentdata</span><span class="p">,</span><span class="w"> </span><span class="n">fill</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">Sex</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span>
<span class="w"> </span><span class="nf">gf_facet_grid</span><span class="p">(</span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">Sex</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">.</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>You can clearly see that sex explains some of the variation in thumb length <em>in our data</em>. (This might not be true in the population; it’s always possible that we are being fooled by a sample that doesn’t accurately represent what’s true in the population.) When we break up thumb length by sex it looks like two separate, though overlapping distributions. In general, males have longer thumbs than females in our data.</p>
<p>If we assume that this relationship (between sex and thumb length) exists in the population, and not just in our data, we can use it to help us make a better prediction about a future observation. If you know that someone is male, you would make a different prediction of their thumb length than if you knew they were female.</p>
<p>It seems, then, that if we were to use a statistical model to make predictions about a person’s thumb length, somehow incorporating information about their sex would be helpful - our predictions would be more accurate on average, and there would be less overall error in the model.</p>
</section>
<section id="adding-an-explanatory-variable-to-the-model">
<h2>11.2 Adding an explanatory variable to the model<a class="headerlink" href="#adding-an-explanatory-variable-to-the-model" title="Permalink to this heading">#</a></h2>
<p>In the previous chapters we introduced the idea of a statistical model as an equation that is meant to represent our best guess of the data generation process. This model generates a predicted score for each observation. We developed what we called the null model, in which we use the mean as the predicted score for each observation.</p>
<p>We represented this model in General Linear Model (GLM) notation like this:</p>
<div class="math notranslate nohighlight">
\[ Y_i = b_0 + e_i \]</div>
<p>where <span class="math notranslate nohighlight">\(b_0\)</span> represents the mean of the outcome variable in the sample. When we use the notation of the GLM, we must define the meaning of each symbol in context. <span class="math notranslate nohighlight">\(Y_i\)</span>, for example, could mean lots of different things, depending on what our outcome variable is. But we will always use it to represent the outcome variable.</p>
<p>It is also important to remember that <span class="math notranslate nohighlight">\(b_0\)</span> is just an estimate of the true mean in the population. To distinguish the true mean, which is unknown, from the estimate of the true mean we construct from our data, we use the Greek letter <span class="math notranslate nohighlight">\(\beta_0\)</span> and write the null model like this:</p>
<div class="math notranslate nohighlight">
\[ Y_i = \beta_0 + \epsilon_i \]</div>
<p>In the case of thumb length, the null model states that the DATA (each data point, represented as <span class="math notranslate nohighlight">\(Y_i\)</span>, which is each person’s measured thumb length), can be thought of as being generated by the combination of two inputs: the MODEL, represented as <span class="math notranslate nohighlight">\(\beta_0\)</span> (which is the mean thumb length for everyone, usually called the <strong>Grand Mean</strong>) plus ERROR, which is each person’s residual from the model, represented by <span class="math notranslate nohighlight">\(\epsilon_i\)</span>.</p>
<div class="alert alert-block alert-info">
<b>Note</b>: We use the term Grand Mean to refer to the mean of everyone's outcome value, in order to distinguish it clearly from other means such as the mean for subgroups within the sample.
</div>
<p>It’s useful to illustrate the null model (and what we’re about to do to it) with our <code class="docutils literal notranslate"><span class="pre">tiny_data</span></code> dataset. <code class="docutils literal notranslate"><span class="pre">tiny_data</span></code>, you will recall, contains six people’s thumb lengths randomly selected from our complete <code class="docutils literal notranslate"><span class="pre">studentdata</span></code> dataset. This time, we’ll also include their value on the <code class="docutils literal notranslate"><span class="pre">Sex</span></code> variable as well.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">student_ID</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="m">3</span><span class="p">,</span><span class="w"> </span><span class="m">4</span><span class="p">,</span><span class="w"> </span><span class="m">5</span><span class="p">,</span><span class="w"> </span><span class="m">6</span><span class="p">)</span>
<span class="n">Thumb</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">56</span><span class="p">,</span><span class="w"> </span><span class="m">60</span><span class="p">,</span><span class="w"> </span><span class="m">61</span><span class="p">,</span><span class="w"> </span><span class="m">63</span><span class="p">,</span><span class="w"> </span><span class="m">64</span><span class="p">,</span><span class="w"> </span><span class="m">68</span><span class="p">)</span>
<span class="n">Sex</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s">&quot;female&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;female&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;female&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;male&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;male&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;male&quot;</span><span class="p">)</span>

<span class="n">tiny_data</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">data.frame</span><span class="p">(</span><span class="n">student_ID</span><span class="p">,</span><span class="w"> </span><span class="n">Thumb</span><span class="p">,</span><span class="w"> </span><span class="n">Sex</span><span class="p">)</span>
<span class="n">tiny_data</span>
</pre></div>
</div>
</div>
</div>
<p>We can put this data into a basic scatter plot with <code class="docutils literal notranslate"><span class="pre">Sex</span></code> on the x-axis and <code class="docutils literal notranslate"><span class="pre">Thumb</span></code> on the y-axis in order to visualize how <code class="docutils literal notranslate"><span class="pre">Sex</span></code> might explain <code class="docutils literal notranslate"><span class="pre">Thumb</span></code>.</p>
<img src="images/ch11-nullmodel.png" width="650">
<p>In the above plot, we drew a blue horizontal line in order to mark where the Grand Mean of the whole <code class="docutils literal notranslate"><span class="pre">tiny_data</span></code> dataset is. This is the same value as <span class="math notranslate nohighlight">\(b_0\)</span> - in other words, this is what we would predict everyone’s thumb length to be if we were using the empty or null model. But there is plenty of error to this prediction - no data point is on this line. We could calculate the RMSE to find out how large the residuals are on average.</p>
<p>Now let’s try to take into account the effect of <code class="docutils literal notranslate"><span class="pre">Sex</span></code> and improve our prediction. One thing we could do is, instead of using the Grand Mean of <code class="docutils literal notranslate"><span class="pre">Thumb</span></code> to predict everyone’s thumb length, we could first consider whether or not the prediction we want to make is for a male or female. Then, we could use the mean of <em>just that group</em> in order to make our prediction.</p>
<img src="images/ch11-sexpredictor.png" width="650">
<p>A model that takes <code class="docutils literal notranslate"><span class="pre">Sex</span></code> into account generates a different prediction for a male than it does for a female. Error is still measured the same way, as the deviation of each person’s measured thumb length from their predicted thumb length. But this time, the prediction is each person’s group mean (male or female) instead of the Grand Mean, and this prediction varies as a function of the predictor variable <code class="docutils literal notranslate"><span class="pre">Sex</span></code>.</p>
</section>
<section id="specifying-the-model-form">
<h2>11.3 Specifying the model form<a class="headerlink" href="#specifying-the-model-form" title="Permalink to this heading">#</a></h2>
<p>The null model only had one parameter in it, the Grand Mean <span class="math notranslate nohighlight">\(b_0\)</span>. To take into account the effect of Sex, we need to add in another parameter that will change our prediction depending on whether someone is male or female. This makes the Sex model a two-parameter model.</p>
<p>One way to specify this equation is to use the mean of one group (say, females) as <span class="math notranslate nohighlight">\(b_0\)</span>, and then add an extra amount to that value if someone is actually male. In other words, we could specify another parameter, <span class="math notranslate nohighlight">\(b_1\)</span>, as the <em>difference</em> between male and female mean thumb lengths. But this should only be added if someone is male, so let’s multiply this parameter by 1 if Sex is “male”, and by 0 if Sex if “female” (effectively ignoring this addition in the case of females).</p>
<p>Here is how to write this in GLM form:</p>
<div class="math notranslate nohighlight">
\[ Y_i = b_0 + b_1X_i + e_i \]</div>
<p>In this equation, <span class="math notranslate nohighlight">\(b_0\)</span> is the group mean for “female” and <span class="math notranslate nohighlight">\(b_1\)</span> is the difference between the group means of male and female. <span class="math notranslate nohighlight">\(X_i\)</span> is a variable, in this case <code class="docutils literal notranslate"><span class="pre">Sex</span></code> (which takes the value of either 0 for female or 1 for male).</p>
<p>We’re still using the DATA = MODEL + ERROR framework for this. Except this time, our MODEL takes into account the value of Sex and has multiple components (<span class="math notranslate nohighlight">\(b_0 + b_1X_i\)</span>) instead of one component (<span class="math notranslate nohighlight">\(b_0\)</span>). <span class="math notranslate nohighlight">\(b_0\)</span> also no longer stands for the Grand Mean of this sample, but the <em>group</em> mean of whatever group we assigned to be 0 in the Boolean variable Sex (called the <strong>reference group</strong>). <span class="math notranslate nohighlight">\(b_1\)</span> is called the <strong>effect</strong> of the predictor, because it’s the effect on our prediction of Y by changing the value of X.</p>
<p>Defining the meaning of <span class="math notranslate nohighlight">\(b_0\)</span> and <span class="math notranslate nohighlight">\(b_1\)</span> this way is for a particular reason - it helps us calculate predictions for <span class="math notranslate nohighlight">\(Y_i\)</span> no matter the value of <span class="math notranslate nohighlight">\(X_i\)</span>. To calculate our prediction of each person’s thumb, we’d fill in the parameters with the group mean of female (59) and the effect of Sex, the difference between the group means of male and female (65 - 59 = 6):</p>
<div class="math notranslate nohighlight">
\[ \hat{Y}_i = 59 + 6X_i \]</div>
<p>This equation would make one prediction (59) when the value of <span class="math notranslate nohighlight">\(X_i\)</span> is 0 (female), and a different prediction (65) when the value of <span class="math notranslate nohighlight">\(X_i\)</span> is 1 (male).</p>
<img src="images/ch11-sexmodel.png" width="650"></section>
<section id="fitting-the-one-variable-model">
<h2>11.4 Fitting the one-variable model<a class="headerlink" href="#fitting-the-one-variable-model" title="Permalink to this heading">#</a></h2>
<p>Now that you have learned how to specify a model with an explanatory variable (also frequently called a predictor), let’s learn how to fit the model using R.</p>
<p>Fitting a model, as a reminder, simply means automatically calculating the parameter estimates that minimize error in our data sample. We use the word “fitting” because we want to calculate the best estimate, the one that will result in the least amount of error and best “fit” our data. For the tiny data set, we could calculate the parameter estimates by hand — it’s just a matter of calculating the mean for males and the mean for females. But when the data set is larger, it is much easier to use code.</p>
<p>Using R, we will first fit the Sex model to the tiny dataset, just so you can see that R gives you the same parameter estimates you got before. After that we will fit it to the complete data set.</p>
<p>Here’s the model form we are going to fit:</p>
<div class="math notranslate nohighlight">
\[ Y_i = b_0 + b_1X_i + e_i \]</div>
<p>Note that the parts that are going to have different values for each observation (<span class="math notranslate nohighlight">\(X_i\)</span> and <span class="math notranslate nohighlight">\(Y_i\)</span>) are called variables (because they vary). The parts that are going to have the same value for each observation (<span class="math notranslate nohighlight">\(b_0\)</span> and <span class="math notranslate nohighlight">\(b_1\)</span>) are called parameter estimates.</p>
<p>We do not need to estimate the variables. Each student in the dataset already has a score for the outcome variable (<span class="math notranslate nohighlight">\(Y_i\)</span>) and the explanatory variable (<span class="math notranslate nohighlight">\(X_i\)</span>), and these scores vary across students. Notice that the subscript <em>i</em> is attached to the parts that are different for each person.</p>
<p>We do need to estimate the parameters because, as discussed previously, they are features of the population, and thus are unknown. The parameter estimates we calculate are those that best fit our particular sample of data. But we would have probably gotten different estimates if we had a different sample. Thus, it is important to keep in mind that these estimates are only that, and they are undoubtedly a bit off. Calling them estimates keeps us humble!</p>
<p>Parameter estimates don’t vary from person to person, so they don’t carry the subscript <em>i</em>.</p>
<p>To fit the Sex model we use <code class="docutils literal notranslate"><span class="pre">lm()</span></code> again. This time, instead of the right side of the formula being <code class="docutils literal notranslate"><span class="pre">NULL</span></code>, we have a variable to put there. Thus, the formula argument of <code class="docutils literal notranslate"><span class="pre">lm()</span></code> is <code class="docutils literal notranslate"><span class="pre">Thumb</span> <span class="pre">~</span> <span class="pre">Sex</span></code>. This means we are asking R to find a statistical model where “Thumb varies as a function of Sex.”</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">lm</span><span class="p">(</span><span class="n">Thumb</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">Sex</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tiny_data</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Notice that the estimates are exactly what we used earlier: the first estimate, for <span class="math notranslate nohighlight">\(b_0\)</span>, is 59 (the mean for females); the second, <span class="math notranslate nohighlight">\(b_1\)</span>, is 6, which is the number of millimeters you need to add to the female average thumb length to get average male thumb length.</p>
<p>Notice also that the estimate for <span class="math notranslate nohighlight">\(b_0\)</span> is labeled “intercept” in the output. You have encountered the concept of intercept before, when you studied how to plot a line in algebra. Remember that <span class="math notranslate nohighlight">\(y = mx + b\)</span> is the equation for a line? <span class="math notranslate nohighlight">\(m\)</span> represents the slope of the line, and <span class="math notranslate nohighlight">\(b\)</span> represents the y-intercept. The General Linear Model notation is similar to this. It’s like if we drew a line between the prediction for females and the prediction for males: the intercept is the value of Y when X=0 (female) and the effect of the predictor (labeled “Sexmale” in the output) is the slope between X=0 and X=1 (aka, the difference between males and females).</p>
<img src="images/ch11-glmeq.png" width="500">
<img src="images/ch11-glmeq2.png" width="500">
<p>If you want — and it’s a good idea — you can save the results of this model fit in a model object. Here’s the code to save the model fit in an object called <code class="docutils literal notranslate"><span class="pre">tiny_sex_model</span></code>. Once you’ve saved the model, If you want to see what the model estimates are, you can just type the name of the model and you will get the same output as above.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">tiny_sex_model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">lm</span><span class="p">(</span><span class="n">Thumb</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">Sex</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tiny_data</span><span class="p">)</span>

<span class="c1">#type the name of the saved model below to print out its output</span>
</pre></div>
</div>
</div>
</div>
<p>Now that we have estimates for the two parameters (intercept and effect of Sex), we can put them in our model statement to yield: <span class="math notranslate nohighlight">\(\hat{Y}_i = 59 + 6X_i\)</span>.</p>
<p>You may have noticed that the values of Sex in <code class="docutils literal notranslate"><span class="pre">tiny_data</span></code> are the categorical strings <code class="docutils literal notranslate"><span class="pre">female</span></code> or <code class="docutils literal notranslate"><span class="pre">male</span></code>, and not 1 or 0. We were able to run <code class="docutils literal notranslate"><span class="pre">lm()</span></code> anyway, so it seems like R is able to handle converting categorical data to Boolean data. But how does R know which level of Sex should be 0 and which should be 1? The answer to this question is, R doesn’t really know. If Sex is character data, it’s just taking whatever group comes first alphabetically (in this case,  <code class="docutils literal notranslate"><span class="pre">female</span></code>) and making it the reference group. If Sex is a factor variable, the reference group is whichever level of the variable was specified to be the first one (by appearing first in the vector passed to <code class="docutils literal notranslate"><span class="pre">levels</span> <span class="pre">=</span> </code> in the <code class="docutils literal notranslate"><span class="pre">factor()</span></code> function). The mean of the reference group is the first parameter estimate (<span class="math notranslate nohighlight">\(b_0\)</span> or the Intercept in the <code class="docutils literal notranslate"><span class="pre">lm()</span></code> output). R then takes the difference between the reference group and the second group (in this case, <code class="docutils literal notranslate"><span class="pre">male</span></code>) and represents it with <span class="math notranslate nohighlight">\(b_1\)</span>.</p>
<p>Let’s say, just for fun, that you changed the code for <code class="docutils literal notranslate"><span class="pre">female</span></code> into <code class="docutils literal notranslate"><span class="pre">woman</span></code> in the data frame. Because <code class="docutils literal notranslate"><span class="pre">male</span></code> now comes first in the alphabet, <code class="docutils literal notranslate"><span class="pre">male</span></code> becomes the reference group, and its mean is now the estimate for the intercept (<span class="math notranslate nohighlight">\(b_0\)</span>) when we automatically fit the model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">tiny_data</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">mutate</span><span class="p">(</span><span class="n">tiny_data</span><span class="p">,</span><span class="w"> </span><span class="n">Sex</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">case_match</span><span class="p">(</span><span class="n">Sex</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;female&quot;</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="s">&quot;woman&quot;</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;male&quot;</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="s">&quot;male&quot;</span><span class="p">))</span>
<span class="n">tiny_data</span>
<span class="c1">#male now comes first in the alphabet, so that value is the reference group</span>
<span class="nf">lm</span><span class="p">(</span><span class="n">Thumb</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">Sex</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tiny_data</span><span class="p">)</span>
<span class="c1">#b0 is now the group mean of &#39;male&#39; and b1 is the difference between male and woman</span>
</pre></div>
</div>
</div>
</div>
<p>Making a categorical predictor like this into Boolean values is also called making a <strong>dummy variable</strong>. This does not get saved to your data frame - it’s just a temporary computation R makes under the hood. You could supply a Sex variable as a Boolean of 0s and 1s already, but if you don’t R will automatically translate a categorical variable into a dummy variable for the purpose of fitting the model.</p>
<p>Now that you have looked in detail at the tiny set of data, let’s find the best estimates for using <code class="docutils literal notranslate"><span class="pre">Sex</span></code> to predict <code class="docutils literal notranslate"><span class="pre">Thumb</span></code> in our bigger dataset <code class="docutils literal notranslate"><span class="pre">studentdata</span></code>. Do so by modifying the null model code below. What would be <span class="math notranslate nohighlight">\(b_0\)</span> and what would be <span class="math notranslate nohighlight">\(b_1\)</span>?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># fit and store a model where Sex predicts Thumb in studentdata</span>
<span class="n">sex_model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span>

<span class="c1"># this prints out the model estimates</span>
<span class="n">sex_model</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="generating-predictions-from-the-model">
<h2>11.5 Generating predictions from the model<a class="headerlink" href="#generating-predictions-from-the-model" title="Permalink to this heading">#</a></h2>
<p>Now that you have fit the Sex model, you can use your estimates to make predictions about future observations. Doing this requires you to use your model as an equation. In this case, you will put in a value for your explanatory variable (Sex), and get out a predicted thumb length.</p>
<p>Recall that the basic form of our model equation looks like this:</p>
<div class="math notranslate nohighlight">
\[ Y_i = b_0 + b_1X_i + e_i \]</div>
<p>Once fit, we can print out the coefficients of the model, and then replace <span class="math notranslate nohighlight">\(b_0\)</span> with the Intercept coefficient value and replace <span class="math notranslate nohighlight">\(b_1\)</span> with the ‘Sexmale’ coefficient value. Then, in order to make a prediction about the value of Thumb for any one person, we remove the error term. We also change the <span class="math notranslate nohighlight">\(Y_i\)</span> to <span class="math notranslate nohighlight">\(\hat{Y}_i\)</span>, which indicates a predicted score for person <em>i</em>. Our prediction equation, then, looks like this:</p>
<div class="math notranslate nohighlight">
\[ \hat{Y}_i = 58.59 + 6.056X_i\]</div>
<p>We leave out the error term because every person will have a different residual. If we knew their residual ahead of time, we could predict their score exactly. But since we don’t when making a new prediction, all we can do is predict their score based on the information available to us - the group means and the new observation’s Sex value.</p>
<p>This prediction equation is straightforward to use. If we want to predict what the next observed thumb length will be, we can see that if the next student sampled is female, their predicted thumb length is 58.59. If they are male, the prediction is 58.59 + 6.056, or 64.646.</p>
<p>If you want to extract each coefficient by itself with code, you can also use the commands below:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">sex_model</span><span class="o">$</span><span class="n">coefficients</span><span class="p">[</span><span class="m">1</span><span class="p">]</span><span class="w"> </span><span class="c1">#this will print out the first coefficient, b0</span>
<span class="n">sex_model</span><span class="o">$</span><span class="n">coefficients</span><span class="p">[</span><span class="m">2</span><span class="p">]</span><span class="w"> </span><span class="c1">#this will print out the second coefficient, b1</span>
</pre></div>
</div>
</div>
</div>
<p>You can use the <code class="docutils literal notranslate"><span class="pre">$</span></code> operator here to pull out the property <code class="docutils literal notranslate"><span class="pre">coefficients</span></code> from the <code class="docutils literal notranslate"><span class="pre">sex_model</span></code> object. This is a vector that contains every coefficient in the model. To get just <span class="math notranslate nohighlight">\(b_0\)</span> or <span class="math notranslate nohighlight">\(b_1\)</span>, we use indexing.</p>
<p>As we did in chapter 9, we also will want to generate model predictions for our sample data. It seems odd to predict values when we already know the actual values. But it’s very useful to do so, because then we can calculate residuals from the model predictions and measure our model’s performance.</p>
<p>To get predicted values from the <code class="docutils literal notranslate"><span class="pre">sex_model</span></code>, we use the <code class="docutils literal notranslate"><span class="pre">fitted.values</span></code> property or the <code class="docutils literal notranslate"><span class="pre">predict()</span></code> function:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">sex_model</span><span class="o">$</span><span class="n">fitted.values</span>
</pre></div>
</div>
</div>
</div>
<p>This is a big output, but the results are just what we’ve already done - for each observation, their predicted thumb length is the mean of female students if their value on Sex is <code class="docutils literal notranslate"><span class="pre">female</span></code> (58.59), or the mean of male students if their value on <code class="docutils literal notranslate"><span class="pre">Sex</span></code> is <code class="docutils literal notranslate"><span class="pre">male</span></code> (64.646).</p>
</section>
<section id="quantifying-model-fit">
<h2>11.6 Quantifying model fit<a class="headerlink" href="#quantifying-model-fit" title="Permalink to this heading">#</a></h2>
<p>Why should we take Sex into account in the first place? Using two parameters in our model instead of one makes it more complex, or less <strong>parsimonious</strong>. We’ll talk more later about the importance of parsimony, but for now we should just know that it’s harder to work with a more complex model than a simpler one. Thus, there should be a good reason for making it more complex - it should reduce the error in our model predictions.</p>
<p>Let’s try it out. We’ll add columns of predictions to <code class="docutils literal notranslate"><span class="pre">tiny_data</span></code>, calculate the residuals, and then calculate the RMSE.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">tiny_data</span><span class="o">$</span><span class="n">GrandMean</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">62</span><span class="p">,</span><span class="w"> </span><span class="m">62</span><span class="p">,</span><span class="w"> </span><span class="m">62</span><span class="p">,</span><span class="w"> </span><span class="m">62</span><span class="p">,</span><span class="w"> </span><span class="m">62</span><span class="p">,</span><span class="w"> </span><span class="m">62</span><span class="p">)</span><span class="w"> </span><span class="c1">#predictions using grand mean</span>
<span class="n">tiny_data</span><span class="o">$</span><span class="n">GroupMeans</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">59</span><span class="p">,</span><span class="w"> </span><span class="m">59</span><span class="p">,</span><span class="w"> </span><span class="m">59</span><span class="p">,</span><span class="w"> </span><span class="m">65</span><span class="p">,</span><span class="w"> </span><span class="m">65</span><span class="p">,</span><span class="w"> </span><span class="m">65</span><span class="p">)</span><span class="w"> </span><span class="c1">#predictions using group means</span>
<span class="n">tiny_data</span><span class="o">$</span><span class="n">GrandResid</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tiny_data</span><span class="o">$</span><span class="n">Thumb</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">tiny_data</span><span class="o">$</span><span class="n">GrandMean</span><span class="w"> </span><span class="c1">#grand mean prediction residuals</span>
<span class="n">tiny_data</span><span class="o">$</span><span class="n">GroupResid</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tiny_data</span><span class="o">$</span><span class="n">Thumb</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">tiny_data</span><span class="o">$</span><span class="n">GroupMeans</span><span class="w"> </span><span class="c1">#group mean prediction residuals</span>

<span class="c1">#equation for RMSE in null model </span>
<span class="nf">sqrt</span><span class="p">(</span><span class="nf">sum</span><span class="p">(</span><span class="n">tiny_data</span><span class="o">$</span><span class="n">GrandResid</span><span class="o">^</span><span class="m">2</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="nf">length</span><span class="p">(</span><span class="n">tiny_data</span><span class="o">$</span><span class="n">GrandResid</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="m">1</span><span class="p">))</span>
<span class="c1">#equation for RMSE in Sex model </span>
<span class="nf">sqrt</span><span class="p">(</span><span class="nf">sum</span><span class="p">(</span><span class="n">tiny_data</span><span class="o">$</span><span class="n">GroupResid</span><span class="o">^</span><span class="m">2</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="nf">length</span><span class="p">(</span><span class="n">tiny_data</span><span class="o">$</span><span class="n">GroupResid</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="m">2</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>Success! If we make predictions about people’s thumb lengths using the Grand Mean, on average we’re about 4.05mm off. But if we take into account each person’s sex for making the prediction, we’re only about 2.65mm off. Not perfect, but we almost halved our error!</p>
<p>You may have noticed that, in order to calculate RMSE for the one- and two-parameter models above, we used slightly different equations. Specifically, in the null model it was calculated as:</p>
<div class="math notranslate nohighlight">
\[RMSE_{null} = \sqrt{\frac{1}{N-1}\sum_{i=1}^{N}(Y_i-\hat{Y}_{i-empty})^2}\]</div>
<p>and in the predictor model it was calculated as:</p>
<div class="math notranslate nohighlight">
\[RMSE_{sex} = \sqrt{\frac{1}{N-2}\sum_{i=1}^{N}(Y_i-\hat{Y}_{i-sex})^2}\]</div>
<p>The difference is that we divided the sum of squares in the null model by N - 1, and in the Sex model we divided by N - 2. We’ve already talked about how, if this measure should be the root <em>mean</em> squared error, it’s weird that we’re not actually <em>calculating the mean of the error</em> (which would be found by dividing by N only). Now, we can learn more about why that is.</p>
<p>If we were to only divide by N, that would actually be fine for finding the RMSE <em>of this specific sample</em>. It would be the root of the mean squared error, exactly as it sounds. However, remember again that our ultimate goal is not to make a model <em>for this sample</em>, but to estimate the data generation process <em>for the whole population</em>. As it turns out, if we divide by only N for finding the RMSE of a sample, we will systematically underestimate how much error our model would have in the population. We’ll demonstrate this in more detail in a later chapter.</p>
<p>In order for us to correct for this underestimation, we need to divide by a slightly smaller number than N: N - 1 in the empty model, or N - 2 in the predictor model. This replacement term is called the <strong>degrees of freedom</strong> in the model.</p>
<p>What are degrees of freedom? In essense, they are the number of unique pieces of information in a dataset, or the number of ways the dataset can vary. You might think that, if a dataset has 6 items (N=6), then there should be 6 unique pieces of information there, right? Each observation can vary in its own way? That would be true, until you bring a parameter estimate of that data into play. Once we have an estimate about the dataset as a whole (say, the mean as <span class="math notranslate nohighlight">\(b_0\)</span> in the null model), that actually takes away one way the dataset can vary - it takes away one degree of freedom.</p>
<p>Let’s demonstrate this with our tiny dataset. We have a set of thumb lengths, [56, 60, 61, 63, 64, 68]. If we were missing one (our set looked like [56, 60, 61, 63, 64, ?]), we wouldn’t have any way of knowing what that sixth item should be - it is free to vary. But if we have the 5 known items, <em>and</em> we have an estimate of the mean of the sample (grand mean = 62), the missing 6th item can <em>only</em> be 68 in order to keep that grand mean estimate at 62. It is not free to vary. Thus, when we have an estimate of the mean of a sample, the degrees of freedom are N - 1. We only need to know 5 of the items in the sample in order to know the whole sample.</p>
<p>When we extend to the Sex model, we have two parameter estimates - <span class="math notranslate nohighlight">\(b_0\)</span> and <span class="math notranslate nohighlight">\(b_1\)</span>. We could be missing one value from each sex subgroup (2 datapoints total), and still solve for all the values in the dataset since we have each group mean. Thus, the degrees of freedom for a two-parameter model is N - 2. To generalize this, the degrees of freedom of any model is <em>sample size - number of parameters</em>, or <em>N - k</em> for short.</p>
<p>When calculating error in a model, dividing by degrees of freedom instead of sample size keeps us from underestimating the error in the population.</p>
<p>You can also use <code class="docutils literal notranslate"><span class="pre">supernova()</span></code> to get an ANOVA table for models with a predictor variable, not just null models. This way we can find RMSE without a big long line of code and math. Write some code below that will run the function on the model object the same way we did last chapter.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># ANOVA table of an empty model</span>
<span class="n">null_model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">lm</span><span class="p">(</span><span class="n">Thumb</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="kc">NULL</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">studentdata</span><span class="p">)</span>
<span class="nf">supernova</span><span class="p">(</span><span class="n">null_model</span><span class="p">)</span>

<span class="c1"># write code to use supernova() on the sex_model object instead</span>
</pre></div>
</div>
</div>
</div>
<p>How do the two ANOVA tables compare? It looks like the table for <code class="docutils literal notranslate"><span class="pre">sex_model</span></code> has the same information from the empty model, but with a lot of new numbers added also. Let’s break this down.</p>
<p>As you will recall from last chapter, the sum of squares (SS in the table) is the sum of squared residuals from our model. We will refer to it as <span class="math notranslate nohighlight">\(SS_{total}\)</span> to diferentiate it from the values on other lines. In both tables, this appears on the line “Total (empty model)” in order to give you a comparison for how much the sum of squares is when just using the Grand Mean to make predictions. It is the total variation in the outcome variable that we hope to explain.</p>
<p>df on the same line stands for degrees of freedom, which you now know the meaning of: there are 157 people in the sample with valid thumb measurements, and we’re estimating one parameter in the null model, so degrees of freedom are 157 - 1 = 156.</p>
<p>Mean squared error (MS) on the same line is the sum of squares divided by the degrees of freedom. To get root mean squared error (RMSE), we’d take the square root of this value.</p>
<p>Now, for the new lines of numbers. The line “Error (from model)” was filled out because we now have a model that is not empty - there’s an explanatory variable in it. SS, df, and MS mean the same thing for this model as they did in the empty model: they reflect the amount of error that is left unexplained by the model, and how many degrees of freedom there are after estimating the model parameters (157 observations - 2 parameters = 155 df). This SS value is known as <span class="math notranslate nohighlight">\(SS_{error}\)</span>. We calculate <span class="math notranslate nohighlight">\(SS_{error}\)</span> in much the same way we calculate <span class="math notranslate nohighlight">\(SS_{total}\)</span>, except this time we use residuals from the Sex model predictions instead of from the null model.</p>
<p>The key thing to look for is whether or not error left over in the predictor model is less than in the null model. If so, that means it was valuable to add our variable! It helped explain some variance in the outcome variable, enabling us to make better predictions and produce smaller residuals. This difference between the null model and the full model is on the top line. <span class="math notranslate nohighlight">\(SS_{model}\)</span> refers to how much of the unexplained error from the null model was explained by adding a variable in the full model. To calculate <span class="math notranslate nohighlight">\(SS_{model}\)</span>, we simply need to subtract <span class="math notranslate nohighlight">\(SS_{error}\)</span> (error from the Sex model predictions) from <span class="math notranslate nohighlight">\(SS_{total}\)</span> (error from the null model predictions). Thus:</p>
<div class="math notranslate nohighlight">
\[ SS_{model} = SS_{total} - SS_{error} \]</div>
</section>
<section id="improvement-over-empty-model">
<h2>11.7 Improvement over empty model<a class="headerlink" href="#improvement-over-empty-model" title="Permalink to this heading">#</a></h2>
<p>Statistical modeling is all about explaining variation. <span class="math notranslate nohighlight">\(SS_{total}\)</span> tells us how much total variation there is to be explained in an outcome variable. When we fit a model (as we have done with the Sex model), that model explains some of the total variation, and leaves some of that variation still unexplained.</p>
<p>These relationships are visualized in the diagram below: Total SS can be seen as the sum of the Model SS (the amount of variation explained by a more complex model) and SS Error, which is the amount left unexplained after fitting the model. SS Total = SS Model + SS Error.</p>
<img src="images/ch11-explainedvar.png" width="750">
<p>We can see this in the ANOVA table: the first two rows (Model and Error) add up to the Total SS. So, 1192.747 + 11777.738 = 12970.485.</p>
<p>We have now quantified how much error from <span class="math notranslate nohighlight">\(SS_{total}\)</span> has been explained by our model: <span class="math notranslate nohighlight">\(SS_{model}\)</span>, or in our specific case with <code class="docutils literal notranslate"><span class="pre">Sex</span></code> predicting <code class="docutils literal notranslate"><span class="pre">Thumb</span></code> in <code class="docutils literal notranslate"><span class="pre">studentdata</span></code>, 1192.747 square millimeters. Is that good? Is that a lot of explained variation? Remember, <span class="math notranslate nohighlight">\(SS_{total}\)</span> in influenced by how many datapoints are in the dataset, so it’s hard to interpret whether 1192.747 is a lot or a little. It would be easier to understand if we knew the <em>proportion</em> of total error that has been reduced rather than the raw amount of error reduced measured in mm<sup>2</sup>.</p>
<p>If you take another look back at the ANOVA table produced above, you will see another column labelled PRE. PRE stands for <strong>Proportional Reduction in Error.</strong></p>
<p>PRE is calculated using the sums of squares. It is simply <span class="math notranslate nohighlight">\(SS_{model}\)</span> (i.e., the variation explained by the model) divided by <span class="math notranslate nohighlight">\(SS_{total}\)</span> (the total variation in the outcome variable there is to explain). We can represent this in an equation:</p>
<div class="math notranslate nohighlight">
\[ PRE = \frac{SS_{model}}{SS_{total}} \]</div>
<p>Based on this equation, PRE can be interpreted as the proportion of total variation in the outcome variable that is explained by the explanatory variable. It tells us something about the overall strength of our statistical model. Because PRE in the ANOVA table is 0.092, that means <code class="docutils literal notranslate"><span class="pre">Sex</span></code> explained 9.2% of the variance in <code class="docutils literal notranslate"><span class="pre">Thumb</span></code>. Most of the original variance is still there (we didn’t explain it perfectly), but we made a reduction in error.</p>
<p>It is important to remember that <span class="math notranslate nohighlight">\(SS_{model}\)</span> in the numerator of the equation above represents the <em>reduction</em> in error when going from the null model to the more complex model, which includes an explanatory variable. To make this clearer we can re-write the above equation like this:</p>
<div class="math notranslate nohighlight">
\[ PRE = \frac{SS_{total} - SS_{error}}{SS_{total}} \]</div>
<p>The numerator of this formula starts with the error from the null model <span class="math notranslate nohighlight">\(SS_{total}\)</span>, and then subtracts the error from the full model <span class="math notranslate nohighlight">\(SS_{error}\)</span> to get the error reduced by the full model <span class="math notranslate nohighlight">\(SS_{model}\)</span>. Dividing this reduction in error by the <span class="math notranslate nohighlight">\(SS_{total}\)</span> yields the proportion of total error in the null model that has been reduced by the full model.</p>
<div class="alert alert-block alert-info">
<b>Note</b>: We're calling the comparison of a more complex model to the empty model PRE since that's what supernova() calls it, but it goes by other names as well. In more traditional statistics it is referred to as $\eta^2$ (eta squared) or $R^2$. For now all you need to know is: these are different terms used to refer to the same thing - reduction of error accomplished by a more complex model.
</div>  
<p>Although right now we’re using PRE to compare a model with one predictor to the null model, the comparison doesn’t need to be to the null model. In fact, PRE can be used to compare any two models as long as they’re modeling the same outcome variable.</p>
<p>Let’s fit another model, this time trying to predict thumb lengths based on which campus a student came from (UCLA or Claremont).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">campus_model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">lm</span><span class="p">(</span><span class="n">Thumb</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">Campus</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">studentdata</span><span class="p">)</span>

<span class="c1">#create an ANOVA table for campus_model</span>
</pre></div>
</div>
</div>
</div>
<p>This model has a much smaller PRE - it explains next to no variance in thumb lengths.</p>
<p>Why might a more complex model not reduce error? This is worth thinking about, because it’s exactly what differentiates the predictor model from the null model. Let’s compare the two:</p>
<p>One-predictor model: $<span class="math notranslate nohighlight">\(Y_i = b_0 + b_1X_i + e_i\)</span>$</p>
<p>Null model: $<span class="math notranslate nohighlight">\(Y_i = b_0 + e_i\)</span>$</p>
<p>In the one variable model, if <span class="math notranslate nohighlight">\(b_1 = 0\)</span>, then <span class="math notranslate nohighlight">\(b_1X_i\)</span> effectively drops out of the equation - any value of X multiplied by 0 is just 0. Then the equation is no different from the null model. Since <span class="math notranslate nohighlight">\(b_1\)</span> refers to the difference in group means, this would be the case where the means of UCLA and Claremont thumbs lengths are the same. In other words, Campus does NOT explain any variation in Thumb. In the ANOVA table, this would look like <span class="math notranslate nohighlight">\(SS_{model}\)</span> of 0 (or close to it) - there is effectively no difference in the error between the null model and the full model. In practice, <span class="math notranslate nohighlight">\(SS_{model}\)</span> is almost never 0. Adding an explanatory variable almost always explains some amount of variance, even if it’s tiny. In a later chapter we will explore the question of how <em>much</em> error reduction is enough for us to care about.</p>
</section>
<section id="a-historical-note-the-t-test">
<h2>11.8 A historical note - the t-test<a class="headerlink" href="#a-historical-note-the-t-test" title="Permalink to this heading">#</a></h2>
<p>In this course we are relying on the general linear model framework to formalize and test our ideas of the data generation process. The main logic of the general linear model is that we can create a (simplified) statistical model that is an estimate of at least a part the of true data generation process, and see whether predictions using that model are more accurate than predictions made using a null model.</p>
<p>This is not the only approach to statistics, however. Indeed until recently, it was not the most common. The traditional approach to statistics was to develop a separate tool for each kind of dataset. In addition, the goal wasn’t about improving accuracy of data predictions, but describing the distinguishability of data from different groups. In this course it is your professor’s opinion that this is not the best way to teach statistics, since it relies on more abstract concepts from the get-go and using separate tests makes it harder to remember the logic of all of them. But because so many researchers learned statistics with these tools, they are often still found in research publications. Thus it would be good for you to at least know what they are, and what version of the general linear model they map onto.</p>
<section id="one-sample-t-test">
<h3>One-sample t-test<a class="headerlink" href="#one-sample-t-test" title="Permalink to this heading">#</a></h3>
<p>The one-sample t-test is a tool to evaluate whether the mean of one sample is <em>significantly different</em> than a particular number. We have not covered the logic of <em>statistical significance</em> yet - we will get to it later in the course. But a quick understanding of it right now for the purpose of the t-test is that this tool is asking the probability of whether a distribution of data in a sample was taken from a population where the true mean <em>μ</em> is a particular value.</p>
<p>A t-test outputs a coefficient, known as a <em>t-value</em>. This coefficient can be calculated as:</p>
<div class="math notranslate nohighlight">
\[ t_{one} = \frac{\bar{X} - \mu}{\frac{s}{\sqrt{n}}} \]</div>
<p>Where <span class="math notranslate nohighlight">\(\bar{X}\)</span> is the mean of a data sample, <span class="math notranslate nohighlight">\(\mu\)</span> is some hypothesized population mean, <span class="math notranslate nohighlight">\(s\)</span> is the standard deviation of the sample, and <span class="math notranslate nohighlight">\(n\)</span> is the sample size. <span class="math notranslate nohighlight">\(\mu\)</span> is a hypothesized <em>population</em> mean, hence why it is written with the Greek letter <span class="math notranslate nohighlight">\(\mu\)</span>. Often, this hypothesized population mean is 0 (but not always). The t-test thus measures how far away the mean of a sample is from 0, and whether that’s exceptional or not considering the spread of the distribution. The bigger the t-value, the more unlikely it is that a population with mean 0 created this sample, and thus the more certain we are that some other population did instead.</p>
<p>A t-value isn’t the same thing as any value in our general linear model, but is based on the difference between the Grand Mean and a hypothesized population mean. Because of this you can use <span class="math notranslate nohighlight">\(b_0\)</span> in the null form of the General Linear Model for a similar purpose. In this approach, we could ask whether setting <span class="math notranslate nohighlight">\(b_0\)</span> to the mean of our sample makes better predictions than setting <span class="math notranslate nohighlight">\(b_0\)</span> to 0. Thus if you encounter one-sample t-tests in the wild, just think of using the null form of the general linear model for the same purpose.</p>
</section>
<section id="independent-samples-t-test">
<h3>Independent samples t-test<a class="headerlink" href="#independent-samples-t-test" title="Permalink to this heading">#</a></h3>
<p>A related type of t-test compares the means of two samples to each other to ask whether we think the same underlying population created them, or not. The computation for an independent-samples t-test is:</p>
<div class="math notranslate nohighlight">
\[ t_{independent} = \frac{\bar{X}_1 - \bar{X}_2}{s_p\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}} \]</div>
<p>where <span class="math notranslate nohighlight">\(s_p\)</span> stands for the <em>pooled</em> standard deviation, a way of combining the standard deviations of two samples:</p>
<div class="math notranslate nohighlight">
\[s_p = \sqrt{\frac{(n_1 - 1)s^2_1 + (n_2 - 1)s^2_2}{n_1 + n_2 + 2}} \]</div>
<p>The independent samples t-test asks whether two groups are likely from the same population. These two groups are usually split based on an explanatory variable - e.g., the thumb lengths of females in our study would be one group, and the thumb lengths of males would be another one. Once separated, those distributions are compared to each other. If an independent samples t-test value is large, it is unlikely that both groups came from the same population.</p>
<p>In the general linear model, we can achieve a similar goal by looking at the value of <span class="math notranslate nohighlight">\(b_1\)</span> in a predictor model. If <span class="math notranslate nohighlight">\(b_1\)</span> is large, that means there is a large difference between female and male group means - we’d make substantially different predictions for someone who is female compared to someone who is male. If <span class="math notranslate nohighlight">\(b_1\)</span> is small, there is not much difference between the two groups (and perhaps the small difference is only there by random chance).</p>
</section>
</section>
<section id="chapter-summary">
<h2>Chapter summary<a class="headerlink" href="#chapter-summary" title="Permalink to this heading">#</a></h2>
<p>After reading this chapter, you should be able to:</p>
<ul class="simple">
<li><p>Understand the difference between an empty model and a one-variable model</p></li>
<li><p>Explain what <span class="math notranslate nohighlight">\(b_0\)</span> and <span class="math notranslate nohighlight">\(b_1\)</span> mean in a one-variable model</p></li>
<li><p>Write the equation form of the one-variable statistical equation</p></li>
<li><p>Fit a one-variable model in R with lm()</p></li>
<li><p>Explain what degrees of freedom are</p></li>
<li><p>Identify the various error components in an ANOVA table of the one-variable model</p></li>
<li><p>Define what proportional reduction in error means</p></li>
<li><p>Calculate proportional reduction in error from SS</p></li>
<li><p>Know what general linear model specification maps onto one-sample and independent t-tests</p></li>
</ul>
</section>
<section id="new-concepts">
<h2>New concepts<a class="headerlink" href="#new-concepts" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Grand Mean</strong>: The mean of an outcome variable for all people in the dataset.</p></li>
<li><p><strong>reference group</strong>: In a statistical model with a categorical variable as a predictor, the reference group is the level of the categorical variable for which <span class="math notranslate nohighlight">\(b_0\)</span> is the group mean and to which all other group means are compared.</p></li>
<li><p><strong>effect</strong>: The effect of a predictor in a model is the extent to which outcome variable predictions are changed based on different values in this predictor.</p></li>
<li><p><strong>dummy variable</strong>: A predictor variable in a model that has been converted to Boolean in order to be useful for mathematical calculation. Meant to represent categorical membership (i.e. 1=in some category, 0=not in this category).</p></li>
<li><p><strong>parsimony</strong>: A statistical model is parsimonious if it is simple with relatively few predictor variables. Parsimonious models explain less error in a data sample but have a better chance at generalizing to other samples.</p></li>
<li><p><strong>degrees of freedom</strong>: The number of values in a data sample that are free to vary after the calculation of some statistic(s). In general the degrees of freedom for a dataset are N-k, where N is the sample size and k is the number of parameters calculated on the dataset.</p></li>
<li><p><strong>proportional reduction in error</strong>: The proportion of variation in an outcome variable that a statistical model is able to explain.</p></li>
</ul>
<p><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-12.ipynb">Next: Chapter 12 - Quantitative Predictor Models</a></p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "smburns47/Psyc158",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "r"
        },
        kernelOptions: {
            name: "ir",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'ir'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#explaining-variation">11.1 Explaining variation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#adding-an-explanatory-variable-to-the-model">11.2 Adding an explanatory variable to the model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#specifying-the-model-form">11.3 Specifying the model form</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fitting-the-one-variable-model">11.4 Fitting the one-variable model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#generating-predictions-from-the-model">11.5 Generating predictions from the model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quantifying-model-fit">11.6 Quantifying model fit</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#improvement-over-empty-model">11.7 Improvement over empty model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-historical-note-the-t-test">11.8 A historical note - the t-test</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#one-sample-t-test">One-sample t-test</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#independent-samples-t-test">Independent samples t-test</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#chapter-summary">Chapter summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#new-concepts">New concepts</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Shannon Burns
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>