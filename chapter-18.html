

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Chapter 18 - Effect Sizes &amp; Statistical Power &#8212; Pomona Psych 158 Online Textbook</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'chapter-18';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Pomona College Psych 158 Online Textbook
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Unit 1 Describing Data</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-1.ipynb">Chapter 1 - Introduction to Statistical Thinking</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-2.ipynb">Chapter 2 - What are Data</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-3.ipynb">Chapter 3 - Organizing Data</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-4.ipynb">Chapter 4 - Cleaning Data</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-5.ipynb">Chapter 5 - Describing Data</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-6.ipynb">Chapter 6 - Variation in Multiple Variables</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-7.ipynb">Chapter 7 - Principles of Data Visualization</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Unit 2 - Modeling Data</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-8.ipynb">Chapter 8 - Where Data Come From</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-9.ipynb">Chapter 9 - Modeling the Data Generation Process</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-10.ipynb">Chapter 10 - Quantifying Model Error</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-11.ipynb">Chapter 11 - Adding an Explanatory Variable</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-12.ipynb">Chapter 12 - Quantitative Predictor Models</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-13.ipynb">Chapter 13 - Multivariable Models</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-14.ipynb">Chapter 14 - Models with Moderation</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Unit 3 - Evaluating Models</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-15.ipynb">Chapter 15 - Estimating Populations</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-16.ipynb">Chapter 16 - Significance Testing</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-17.ipynb">Chapter 17 - Significance Testing Whole Models</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-18.ipynb">Chapter 18 - Effect Sizes &amp; Statistical Power</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-19.ipynb">Chapter 19 - Model Bias</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-20.ipynb">Chapter 20 - Alternate Approaches - Traditional Statistical Tools</a></li>
<li class="toctree-l1"><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-21.ipynb">Chapter 21 - Lying with Statistics</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://mybinder.org/v2/gh/smburns47/Psyc158/main?urlpath=tree/chapter-18.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onBinder"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="_static/images/logo_binder.svg">
  </span>
<span class="btn__text-container">Binder</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="initThebeSBT()"
  class="btn btn-sm btn-launch-thebe dropdown-item"
  title="Launch Thebe"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="btn__text-container">Live Code</span>
</button>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/smburns47/Psyc158" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/smburns47/Psyc158/issues/new?title=Issue%20on%20page%20%2Fchapter-18.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/chapter-18.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Chapter 18 - Effect Sizes & Statistical Power</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#unstandardized-effect-sizes">18.1 Unstandardized effect sizes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#standardized-effect-sizes">18.2 Standardized effect sizes</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#standardized-model-coefficient">Standardized model coefficient</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#correlation-coefficient-r">Correlation coefficient r</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#r-sup-2-sup">R<sup>2</sup></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cohen-s-d">Cohen’s d</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cohen-s-f-sup-2-sup">Cohen’s f<sup>2</sup></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-good-effect-size">18.3 What is a good effect size?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#statistical-power">18.4 Statistical power</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#power-planning">18.5 Power planning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#chapter-summary">Chapter summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#new-concepts">New concepts</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#new-r-functionality">New R functionality</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <p><a class="reference external" href="https://www.shannonmburns.com/Psyc158/intro.html">Back to Table of Contents</a></p>
<p><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-17.ipynb">Previous: Chapter 17 - Significance Testing Whole Models</a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1"># Run this first so it&#39;s ready by the time you need it</span>
<span class="nf">install.packages</span><span class="p">(</span><span class="s">&quot;dplyr&quot;</span><span class="p">)</span>
<span class="nf">install.packages</span><span class="p">(</span><span class="s">&quot;ggformula&quot;</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">dplyr</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">ggformula</span><span class="p">)</span>
<span class="n">GSS</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">read.csv</span><span class="p">(</span><span class="s">&quot;https://raw.githubusercontent.com/smburns47/Psyc158/main/GSS.csv&quot;</span><span class="p">)</span>

<span class="c1">#smaller dataset, more similar in size to most psychology studies</span>
<span class="nf">set.seed</span><span class="p">(</span><span class="m">10</span><span class="p">)</span>
<span class="n">GSS_subset</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">slice_sample</span><span class="p">(</span><span class="n">GSS</span><span class="p">,</span><span class="w"> </span><span class="n">n</span><span class="o">=</span><span class="m">100</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section class="tex2jax_ignore mathjax_ignore" id="chapter-18-effect-sizes-statistical-power">
<h1>Chapter 18 - Effect Sizes &amp; Statistical Power<a class="headerlink" href="#chapter-18-effect-sizes-statistical-power" title="Permalink to this heading">#</a></h1>
<p>In the previous chapters we discussed how we can use data to test hypotheses. This approach gives us p-values, which we can use to decide if we think it is likely or not that the true effect or model in a population is 0. But that sort of decision is ultimately a binary answer: we either reject or fail to reject the null hypothesis. We neglect a lot of the information a model gives us if p-values are the only thing we pay attention to.</p>
<p>Consider the following situation. You are testing the efficacy of a new drug for treating depression, and find that the effect of the drug has a p-value of &lt;0.01. Taking the drug significantly improves depression! In a simple world, that would be all you need to send the drug into production and start selling it to people. However, we don’t live in a simple world. It takes money and time to produce medicine, which are resources taken away from producing other things. Your bosses (and the taxpayers funding Medicare) want that money and time to be worth it.</p>
<p>Of course it’s worth it you say, because the effect is significant! But what if you learned that the actual <em>amount</em> the drug improved depression by was 1 point on a 10 point scale? Is that worth it? What if it instead improved depression by only 0.1 points? What if it improved depression by only a little bit, and also came with side effects like acne and stomach upset? Would you make the same drug production decision across all of these scenarios?</p>
<p>We know from our practice with p-values that even tiny effects can be significant if we have enough data. If your only goal is to publish a paper saying that an effect probably exists, then a p-value is enough for that purpose. But if you care about applying your new knowledge in the real world, a p-value doesn’t tell us everything. We don’t know if the <strong>effect size</strong> is large enough to matter. So while null hypothesis testing tells us whether an effect likely exists in some way (statistical significance), effect sizes tell us how meaningful that effect is (called <strong>practical significance</strong>). Good statistical analyses should report both p-values and effect sizes in order to give the full picture about a statistical model.</p>
<section id="unstandardized-effect-sizes">
<h2>18.1 Unstandardized effect sizes<a class="headerlink" href="#unstandardized-effect-sizes" title="Permalink to this heading">#</a></h2>
<p>How do you measure an effect size? Actually, throughout this course we’ve already been doing it! An effect size is any number expressing the magnitude of an effect. When fitting models, we have interpreted the b coefficients estimated by those models in the context of what difference in predictions we would make for different values of input variables. For a b coefficient of 5, we’d change our prediction of an outcome variable by 5 units for every one unit increase in the predictor variable. Thus, the predictor’s value has a big impact on what our outcome predictions are. For a b coefficient of 0.1, we’d change our outcome prediction only a little bit. The predictor has a smaller impact on predictions. These numbers are all effect sizes. They express the size of the effect a predictor variable has on the predictions of an outcome variable.</p>
<p>Likewise, RMSE is an effect size because it tells us the magnitude of prediction errors to expect when using a whole model.</p>
<p>The b coefficients and RMSE are specifically known as <strong>unstandardized effect sizes.</strong> An unstandardized effect size is the magnitude of an effect expressed in the units of the variables.</p>
<p>Unstandardized effect sizes are useful for making decisions about particular situations. Drug dose, cholesterol, height, etc. all having meaningful units by which their quantity is expressed. An unstandardized effect size representing a number in these units tells you how much of a drug you have to give to get an expected outcome, and amount of cholesterol in the blood is well-mapped to heart risks. We interact with the world through the units of quantities, and we best understand effects in these same units.</p>
<p>However, we have less understanding about units in domains that we’re not already familiar with or that are abstract concepts. E.g., if a model suggested that drinking alcohol before bed is associated with a decrease in sleep length of 2 hours, we intuitively know what that means. We all sleep, and we’ve all felt the effects of 8 hours versus 6 hours of sleep. But if that model instead suggested that drinking alcohol before bed is associated with a 2 point decrease in cognitive fluency, what does that mean? What concrete things define cognitive fluency? What does fluency = 5 feel like compared to fluency = 7?</p>
<p>Unstandardized coefficients are useful for understanding the practical significance of effects in units that we know about, but are less useful for abstract measurements or domains we are unfamiliar with. In that case, we can standardize the effect size.</p>
</section>
<section id="standardized-effect-sizes">
<h2>18.2 Standardized effect sizes<a class="headerlink" href="#standardized-effect-sizes" title="Permalink to this heading">#</a></h2>
<p><strong>Standardized effect sizes</strong> remove the real-world units from variables. We’ve done a version of this already when learning about z-scores. A standardized variable is one that has been converted to z-scores. Z-scores, to review, express the value of a variable in terms of how many standard deviations it is away from the variable mean. By doing this, we don’t need to have domain expertise in the variable to intrinsically understand what certain values mean. Could you say what a temperature of 350F means relative to other typical temperatures for baking? What about a <a class="reference external" href="https://en.wikipedia.org/wiki/Gas_mark">Gas mark</a> of 5? You’re likely more familiar with the former than the latter. But if we convert each to z-scores and find that both have a z-score of -1, we can easily undertand either of those values relative to their distribution.</p>
<p>A standardized effect size operates similarly. You don’t need to know about a variable’s units to understand the magnitude of a standardized effect size. Doing this has three benefits for understanding statistical models:</p>
<ol class="arabic simple">
<li><p>Standardized effect sizes help you evaluate how big or small an effect is when the units of measurement aren’t intuitive. We don’t have to know the typical range or mean of a variable already in order to interpret a standardized effect size - its value has all the information wrapped into it.</p></li>
<li><p>Standardized effect sizes can help you compare results across studies. Many variables are measured on different scales in different studies. This isn’t likely to happen with a variable like temperature, but there are multiple anxiety scales to choose from, each of which is on a different scale. Including standardized effect size statistics can help readers understand trends or differences across studies. It can also help us compare effect sizes of variables in the same model that are measured in different units (although remember we need to use hypothesis testing to figure out if a difference in estimated effect is statistically significant).</p></li>
<li><p>Standardized effect sizes let us plan our research more easily. We’ll cover this process more later in this chapter, but the short of it is that there are tools we can use to figure out what sample size we should collect if we want to be able to find a particular effect size as statistically significant. These tools use standardized effect sizes.</p></li>
</ol>
<p>Next we will cover how to calculate some of the most common standardized effect statistics.</p>
<section id="standardized-model-coefficient">
<h3>Standardized model coefficient<a class="headerlink" href="#standardized-model-coefficient" title="Permalink to this heading">#</a></h3>
<p>An unstandardized model coefficient is the coefficient estimate a model gives when the model is fit directly onto raw variable values. It is interpreted in the context of those variable units.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">unstandardized_model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">lm</span><span class="p">(</span><span class="n">highest_year_of_school_completed</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">highest_year_school_completed_father</span><span class="p">,</span><span class="w"> </span>
<span class="w">                           </span><span class="n">data</span><span class="o">=</span><span class="n">GSS_subset</span><span class="p">)</span>
<span class="nf">summary</span><span class="p">(</span><span class="n">unstandardized_model</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>When fitting this model, we get a <span class="math notranslate nohighlight">\(b_0\)</span> estimate of 9.2 and a <span class="math notranslate nohighlight">\(b_1\)</span> estimate of 0.388. These mean, in the context of the variable units, that we expect someone to have 9.2 years of education if their father had 0 years of schooling. For every one additional year of school their father had, we’d expect the respondent’s education to be higher by 0.388 years.</p>
<p>A standardized model coefficient, in contrast, is the estimate a model gives when those variables have been z-scored first. The code below will pick out the relevant variables for our model and z-score them all at the same time.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">GSS_zscore</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">select</span><span class="p">(</span><span class="n">GSS_subset</span><span class="p">,</span><span class="w"> </span><span class="n">highest_year_of_school_completed</span><span class="p">,</span><span class="w"> </span><span class="n">highest_year_school_completed_father</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="nf">na.omit</span><span class="p">(</span><span class="n">.</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> </span><span class="c1">#removes rows with any NAs</span>
<span class="w">    </span><span class="nf">scale</span><span class="p">(</span><span class="n">.</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span>
<span class="w">    </span><span class="nf">as.data.frame</span><span class="p">(</span><span class="n">.</span><span class="p">)</span>

<span class="n">standardized_model</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">lm</span><span class="p">(</span><span class="n">highest_year_of_school_completed</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">highest_year_school_completed_father</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="o">=</span><span class="n">GSS_zscore</span><span class="p">)</span>
<span class="nf">summary</span><span class="p">(</span><span class="n">standardized_model</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>These b estimates are different than before. Now, <span class="math notranslate nohighlight">\(b_0\)</span> is basically 0 and <span class="math notranslate nohighlight">\(b_1 = 0.538\)</span>. This is because these coefficients are now in the context of z-scores instead of years of education. We’d interpret these as expecting someone to have an education level that is the mean (z = 0) if their father had a mean amount of education (z = 0). This makes sense, because a regression line always runs through the mean of both the predictor and outcome variable. We’d also expect that education level to increase by 0.538 SDs for every 1 SD increase in father’s education, according to the value of the standardized <span class="math notranslate nohighlight">\(b_1\)</span>.</p>
<img src="images/ch18-unstandardvstandard.png" width="900">
<p>Also notice the difference in p-values between these models. The unstandardized <span class="math notranslate nohighlight">\(b_0\)</span> is significant (p = 2.40e-15), but the standardized <span class="math notranslate nohighlight">\(b_0\)</span> is not (p = 1). This is because the null hypothesis is that <span class="math notranslate nohighlight">\(\beta_0 = 0\)</span> for both models. In the unstandardized model, 9.2 is a number that is way above 0, and is thus unlikely to be produced by a population unstandardized-<span class="math notranslate nohighlight">\(\beta_0 = 0\)</span> years. In the standardized model, standardized-<span class="math notranslate nohighlight">\(\beta_0 = 0\)</span> means unstandardized-<span class="math notranslate nohighlight">\(\beta_0\)</span> = the mean of of education. A z-score of approximately 0 isn’t different than 0 at all, so we fail to reject that null hypothesis.</p>
<p>However, the p-values of both <span class="math notranslate nohighlight">\(b_1\)</span> estimates are the same. This is because, regardless of the units, the strength of the effect is still the same. There’s no difference in the significance of effect coefficients between a standardized and unstandardized model. This is why you can choose to express the effect size of an estimate in either standardized or unstandardized terms.</p>
<p>Note that in order to find this standardized b estimate, we have to z-score BOTH the predictor and the outcome. If we only z-scored the predictor and not the outcome, the meaning of the b estimate would be “change in years of education for every 1 SD increase in father’s education.” It would be a sort of half-standardized effect estimate, but we want a fully standardized one in order to compare effect sizes between different analyses and studies.</p>
<p>When talking about standardized vs. unstandardized model coefficients, because they are different numeric values, make sure you are being explicit about which one you are using. Oftentimes in published research, unstandardized coefficients are reported as <span class="math notranslate nohighlight">\(b\)</span> and standardized coefficients are reported as <span class="math notranslate nohighlight">\(\beta\)</span> (that’s not to say they’re the same as population estimates, but you start running out of letters in the alphabet eventually…).</p>
</section>
<section id="correlation-coefficient-r">
<h3>Correlation coefficient r<a class="headerlink" href="#correlation-coefficient-r" title="Permalink to this heading">#</a></h3>
<p>In chapter 12 we learned about the correlation coefficient <span class="math notranslate nohighlight">\(r\)</span>. It is frequently used to represent a standardized relationship strength between two continuous variables. It can range from -1 (perfect negative correlation) to 1 (perfect positive correlation), with 0 meaning no relationship.</p>
<p><span class="math notranslate nohighlight">\(r\)</span> is the same thing as the standardized b coefficient from a simple linear model with just one predictor:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">standardized_model</span><span class="o">$</span><span class="n">coefficients</span><span class="p">[[</span><span class="m">2</span><span class="p">]]</span>

<span class="nf">cor</span><span class="p">(</span><span class="n">GSS_subset</span><span class="o">$</span><span class="n">highest_year_of_school_completed</span><span class="p">,</span><span class="w"> </span><span class="n">GSS_subset</span><span class="o">$</span><span class="n">highest_year_school_completed_father</span><span class="p">,</span>
<span class="w">   </span><span class="n">use</span><span class="o">=</span><span class="s">&quot;pairwise.complete.obs&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Note that if you have a multivariable model, the standardized b coefficients for each predictor are <em>partial correlations</em>, not the same thing as <span class="math notranslate nohighlight">\(r\)</span>. They are the correlation with the outcome variable’s residuals after taking into account the effects of the other predictors.</p>
</section>
<section id="r-sup-2-sup">
<h3>R<sup>2</sup><a class="headerlink" href="#r-sup-2-sup" title="Permalink to this heading">#</a></h3>
<p>While standardized <span class="math notranslate nohighlight">\(b\)</span> and correlation <span class="math notranslate nohighlight">\(r\)</span> speak to the relationship between an outcome variable and one predictor, <span class="math notranslate nohighlight">\(R^2\)</span> is a standardized effect size for how much variation an entire model explains. It tells us the proportional reduction in error of a full model compared to the null model, and its meaning is the same regardless of the units used for the variables in the model.</p>
<p>In the case of a one-variable model, it is actually the same thing as the square of the correlation coefficient:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">cor</span><span class="p">(</span><span class="n">GSS_subset</span><span class="o">$</span><span class="n">highest_year_of_school_completed</span><span class="p">,</span><span class="w"> </span><span class="n">GSS_subset</span><span class="o">$</span><span class="n">highest_year_school_completed_father</span><span class="p">,</span>
<span class="w">   </span><span class="n">use</span><span class="o">=</span><span class="s">&quot;pairwise.complete.obs&quot;</span><span class="p">)</span><span class="o">^</span><span class="m">2</span>

<span class="nf">summary</span><span class="p">(</span><span class="n">unstandardized_model</span><span class="p">)</span><span class="o">$</span><span class="n">r.squared</span>
</pre></div>
</div>
</div>
</div>
<p>That’s where the name “<span class="math notranslate nohighlight">\(R^2\)</span>” comes from. But of course it can also be used to to account for the proportion of error explained by a model with more than one predictor.</p>
</section>
<section id="cohen-s-d">
<h3>Cohen’s d<a class="headerlink" href="#cohen-s-d" title="Permalink to this heading">#</a></h3>
<p>Standardized <span class="math notranslate nohighlight">\(b\)</span>, <span class="math notranslate nohighlight">\(r\)</span>, and <span class="math notranslate nohighlight">\(R^2\)</span> are all effect sizes that we have seen and used before. There are two more measures of effect size that are common to see in psychology research that we haven’t yet encountered.</p>
<p>One of them is called <strong>Cohen’s d</strong>. This measure was created by an influential quantitative psychologist named Jacob Cohen in the 1950s. It is used specifically to represent the magnitude of difference between two categorical groups independent of real-world units. Cohen’s <span class="math notranslate nohighlight">\(d\)</span> is related to the concept of z-score, in that it indicates the size of a group mean difference in standard deviation units. It is calculated as:</p>
<div class="math notranslate nohighlight">
\[ d = \frac{\bar{X_1} - \bar{X_2}}{s_{pooled}} \]</div>
<p><span class="math notranslate nohighlight">\(s_{pooled}\)</span> is a weighted average of the standard deviations from both groups:</p>
<div class="math notranslate nohighlight">
\[s_{pooled} = \frac{df_1s_1 + df_2s_2}{df_1 + df_2}\]</div>
<p>Cohen’s <span class="math notranslate nohighlight">\(d\)</span> can range between 0 and infinity, but in practice is rarely more than 2. The bigger the <span class="math notranslate nohighlight">\(d\)</span> value, the bigger an effect is. To find Cohen’s <span class="math notranslate nohighlight">\(d\)</span> in R, you can pass a group model formula and dataset argument to the <code class="docutils literal notranslate"><span class="pre">cohens_d()</span></code> function in the <code class="docutils literal notranslate"><span class="pre">effectsize</span></code> package as if you were fitting the model:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">install.packages</span><span class="p">(</span><span class="s">&quot;effectsize&quot;</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">effectsize</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">born_in_US</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">GSS_subset</span><span class="o">$</span><span class="n">highest_year_of_school_completed</span><span class="p">[</span><span class="n">GSS_subset</span><span class="o">$</span><span class="n">born_in_us</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s">&quot;Yes&quot;</span><span class="p">]</span>
<span class="n">born_outside_US</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">GSS_subset</span><span class="o">$</span><span class="n">highest_year_of_school_completed</span><span class="p">[</span><span class="n">GSS_subset</span><span class="o">$</span><span class="n">born_in_us</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s">&quot;No&quot;</span><span class="p">]</span>

<span class="nf">paste0</span><span class="p">(</span><span class="s">&#39;mean education born in US: &#39;</span><span class="p">,</span><span class="w"> </span><span class="nf">mean</span><span class="p">(</span><span class="n">born_in_US</span><span class="p">,</span><span class="w"> </span><span class="n">na.rm</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">))</span>
<span class="nf">paste0</span><span class="p">(</span><span class="s">&#39;mean education born outside US: &#39;</span><span class="p">,</span><span class="w"> </span><span class="nf">mean</span><span class="p">(</span><span class="n">born_outside_US</span><span class="p">,</span><span class="w"> </span><span class="n">na.rm</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">))</span>

<span class="c1">#effect size for difference in mean education for people born in or outside of US</span>
<span class="c1"># 0 = in, 1 = outside</span>
<span class="nf">cohens_d</span><span class="p">(</span><span class="n">highest_year_of_school_completed</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">born_in_us</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">GSS_subset</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>This table tells you the standardized mean education difference between the two groups represented by <code class="docutils literal notranslate"><span class="pre">born_in_us</span></code> in the GSS data. The answer we get is -0.955, meaning the first group (X=0) has 0.955SD less education than the second group (X=1). Interpreting that in terms of our variable, people born outside of the US have less years of education on average than those born in the US.</p>
<p>The additional values tell us the confidence interval around this effect size, as it is only an estimate based on our data.</p>
</section>
<section id="cohen-s-f-sup-2-sup">
<h3>Cohen’s f<sup>2</sup><a class="headerlink" href="#cohen-s-f-sup-2-sup" title="Permalink to this heading">#</a></h3>
<p>Cohen also came up with something very similar to <span class="math notranslate nohighlight">\(R^2\)</span>, called <strong>Cohen’s <span class="math notranslate nohighlight">\(f^2\)</span></strong> (not to be confused with an F value in an F distribution). Rather than describing the proportion of variation in an outcome that a model explains, Cohen’s <span class="math notranslate nohighlight">\(f^2\)</span> is the ratio of explained to unexplained error:</p>
<div class="math notranslate nohighlight">
\[f^2 = \frac{R^2}{1-R^2} \]</div>
<p>Sometimes people report is as just Cohen’s <span class="math notranslate nohighlight">\(f\)</span>:</p>
<div class="math notranslate nohighlight">
\[f = \sqrt{\frac{R^2}{1-R^2}} \]</div>
<p>Rather than being bound between 0 and 1 as <span class="math notranslate nohighlight">\(R^2\)</span> is, <span class="math notranslate nohighlight">\(f^2\)</span> can be as small as 0 and theoretically as large as infinity. A bigger <span class="math notranslate nohighlight">\(f^2\)</span> means a model is explaining more variation. If <span class="math notranslate nohighlight">\(f^2\)</span> is above 1, then the model explains more variation than what is left unexplained. In psychology that is rare but not unheard of.</p>
<p>The above equation is known as the <strong>global <span class="math notranslate nohighlight">\(f^2\)</span></strong>, or the <span class="math notranslate nohighlight">\(f^2\)</span> score for a model compared to the null model. You can get <span class="math notranslate nohighlight">\(f\)</span> in the <code class="docutils literal notranslate"><span class="pre">effectsize</span></code> package by passing a model object to the <code class="docutils literal notranslate"><span class="pre">cohens_f()</span></code> function:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">cohens_f</span><span class="p">(</span><span class="n">unstandardized_model</span><span class="p">)</span>

<span class="c1">#verifying that it is the same as sqrt(R2 / (1-R2))</span>
<span class="nf">sqrt</span><span class="p">(</span><span class="nf">summary</span><span class="p">(</span><span class="n">unstandardized_model</span><span class="p">)</span><span class="o">$</span><span class="n">r.squared</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="m">1</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nf">summary</span><span class="p">(</span><span class="n">unstandardized_model</span><span class="p">)</span><span class="o">$</span><span class="n">r.squared</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>There is also a <strong>local <span class="math notranslate nohighlight">\(f^2\)</span></strong> that tells you the increase in explained variation between a full model and a non-null nested model. The equation for that is:</p>
<div class="math notranslate nohighlight">
\[f^2 = \frac{R^2_{full} - R^2_{nested}}{1-R^2_{full}} \]</div>
<p>where <span class="math notranslate nohighlight">\(R^2_{full}\)</span> is the <span class="math notranslate nohighlight">\(R^2\)</span> of the model with all predictors in it, and <span class="math notranslate nohighlight">\(R^2_{nested}\)</span> is the <span class="math notranslate nohighlight">\(R^2\)</span> of a smaller nested model.</p>
</section>
</section>
<section id="what-is-a-good-effect-size">
<h2>18.3 What is a good effect size?<a class="headerlink" href="#what-is-a-good-effect-size" title="Permalink to this heading">#</a></h2>
<p>When evaluating an unstandardized effect size for units you are familiar with, you will already have a sense about what an “important” vs. “trivial” effect size is. If you’re studying for a test, 10 hours of studying making a difference of 1% on your grade might not be worth it, but a difference of 10% could be worth spending your whole Saturday in the library.</p>
<p>When you use a standardized effect size, the down side is that you don’t have any real-world units to ground this number in. But the up side is that the same standardized effect size can apply across many research studies. Over time, then, you begin to accumulate a sense for what value of a standardized effect size is “big”, “medium”, or “small.”</p>
<p>When Jacob Cohen first formulated his measurements of <span class="math notranslate nohighlight">\(d\)</span> and <span class="math notranslate nohighlight">\(f^2\)</span>, he came up with rules of thumb for what counts as a small, medium, and large effect. He decided that, in general, a “large” effect is one that is obvious - people generally should be able to see this effect in the population, without the need to collect data. This translates to a Cohen’s <span class="math notranslate nohighlight">\(d\)</span> of &gt;= 0.8. Something like the difference in height between men and women is a large effect because it is generally possible to see without systematic research, and also has a Cohen’s <span class="math notranslate nohighlight">\(d\)</span> &gt; 0.8:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="n">studentdata</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">read.csv</span><span class="p">(</span><span class="s">&quot;https://raw.githubusercontent.com/smburns47/Psyc158/main/studentdata.csv&quot;</span><span class="p">)</span>

<span class="n">female_height</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">studentdata</span><span class="o">$</span><span class="n">Height</span><span class="p">[</span><span class="n">studentdata</span><span class="o">$</span><span class="n">Sex</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s">&quot;Female&quot;</span><span class="p">]</span>
<span class="n">male_height</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">studentdata</span><span class="o">$</span><span class="n">Height</span><span class="p">[</span><span class="n">studentdata</span><span class="o">$</span><span class="n">Sex</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s">&quot;Male&quot;</span><span class="p">]</span>

<span class="nf">paste0</span><span class="p">(</span><span class="s">&#39;mean female height: &#39;</span><span class="p">,</span><span class="w"> </span><span class="nf">mean</span><span class="p">(</span><span class="n">female_height</span><span class="p">,</span><span class="w"> </span><span class="n">na.rm</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">))</span>
<span class="nf">paste0</span><span class="p">(</span><span class="s">&#39;mean male height: &#39;</span><span class="p">,</span><span class="w"> </span><span class="nf">mean</span><span class="p">(</span><span class="n">male_height</span><span class="p">,</span><span class="w"> </span><span class="n">na.rm</span><span class="o">=</span><span class="kc">TRUE</span><span class="p">))</span>

<span class="c1">#X=0 is female, X=1 is male</span>
<span class="nf">cohens_d</span><span class="p">(</span><span class="n">Height</span><span class="w"> </span><span class="o">~</span><span class="w"> </span><span class="n">Sex</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">studentdata</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Cohen set a “medium” effect to be one that is 0.5 &lt; <span class="math notranslate nohighlight">\(d\)</span> &lt; 0.8. A medium effect should be visible within a representative dataset “to the naked eye of a careful observer.” You might not obviously notice it in real life, but if you collect data and plot the histograms of two groups, you’d be able to see a difference in the histograms.</p>
<p>Finally, a “small” effect is 0.2 &lt; <span class="math notranslate nohighlight">\(d\)</span> &lt; 0.5. This is one that you can’t see with the naked eye, but that is still meaningful. It will still matter at the scale of an entire population. Cohen considered any <span class="math notranslate nohighlight">\(d\)</span> less than 0.2 to be trivial and not worth studying.</p>
<p>In his 1988 book “Statistical Power Analysis for the Behavioral Sciences”, Cohen also set some general rules of thumb for <span class="math notranslate nohighlight">\(r\)</span>, <span class="math notranslate nohighlight">\(R^2\)</span>, and <span class="math notranslate nohighlight">\(f^2\)</span>:</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Effect size</p></th>
<th class="head text-center"><p>small effect</p></th>
<th class="head text-center"><p>medium effect</p></th>
<th class="head text-center"><p>large effect</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p><span class="math notranslate nohighlight">\(d\)</span></p></td>
<td class="text-center"><p>0.2</p></td>
<td class="text-center"><p>0.5</p></td>
<td class="text-center"><p>0.8</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><span class="math notranslate nohighlight">\(r\)</span></p></td>
<td class="text-center"><p>0.1</p></td>
<td class="text-center"><p>0.3</p></td>
<td class="text-center"><p>0.5</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p><span class="math notranslate nohighlight">\(R^2\)</span></p></td>
<td class="text-center"><p>0.02</p></td>
<td class="text-center"><p>0.10</p></td>
<td class="text-center"><p>0.25</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><span class="math notranslate nohighlight">\(f^2\)</span> / <span class="math notranslate nohighlight">\(f\)</span></p></td>
<td class="text-center"><p>0.02 / 0.14</p></td>
<td class="text-center"><p>0.15 / 0.39</p></td>
<td class="text-center"><p>0.35 / 0.59</p></td>
</tr>
</tbody>
</table>
<p>However, in practice it is best to remember that these are just guidelines. A large effect might not be very trustworthy if you found it in a small sample, and a tiny effect might still be theoretically important (e.g., it’s not a great predictor on its own but significantly contributes to a larger model). It is better to be aware of the field of study you are working in, what the typical effect sizes are, and how your measured effect size compares to those. For instance, it is pretty rare to find large effects in psychology research (most of those have been published already!). A lot of modern work is on the little effects that give the data generation process nuance.</p>
</section>
<section id="statistical-power">
<h2>18.4 Statistical power<a class="headerlink" href="#statistical-power" title="Permalink to this heading">#</a></h2>
<p>Effect sizes and statistical significance are not the same thing - you can’t tell what an effect size is just from a p-value, and you can’t tell whether an effect is significant just by looking at an effect size. But these two things cooperate for helping us make conclusions about models.</p>
<p>Recall in chapter 16 when we discussed Type I and Type II error. Type I error is the chance that we mistakenly reject the null hypothesis when we shouldn’t. We thought that there’s a non-zero effect, but really there’s nothing there. We set this chance ourselves, with an <span class="math notranslate nohighlight">\(\alpha\)</span> criterion of 0.05. <span class="math notranslate nohighlight">\(\alpha = 0.05\)</span> means there is a 95% chance we don’t reject the null when it is true (P(fail to reject | <span class="math notranslate nohighlight">\(H_0\)</span> is true); true negative), and a 5% chance we incorrectly reject the null when it is true (P(reject | <span class="math notranslate nohighlight">\(H_0\)</span> is true); false positive). <span class="math notranslate nohighlight">\(\alpha\)</span> is the Type I error rate - given the null is true, how often do we get a false positive decision?</p>
<p>On the other hand is Type II error. This is the chance that we fail to reject the null hypothesis when we should have. Like Type I error is controlled by a criterion that we set, Type II also has a criterion <span class="math notranslate nohighlight">\(\beta\)</span> (not to be confused with the population model parameter <span class="math notranslate nohighlight">\(\beta\)</span> either… statisticians really love beta). When we set the <span class="math notranslate nohighlight">\(\beta\)</span> criterion to something like 0.05, we would have a 5% chance of failing to reject the null hypothesis when we should (P(fail to reject | <span class="math notranslate nohighlight">\(H_0\)</span> is false); false negative). The complement, then, is we have a 95% chance of correctly rejecting the null hypothesis when we should (P(reject | <span class="math notranslate nohighlight">\(H_0\)</span> is false); true positive)).</p>
<p>This concept, the chance of correctly finding an effect that exists in the population, is called <strong>statistical power</strong>. Statistical power, <span class="math notranslate nohighlight">\(\alpha\)</span>, and <span class="math notranslate nohighlight">\(\beta\)</span> are all related, as we can see in this grid:</p>
<img src="images/ch18-powergrid.png" width="500">
<p>For any one study, Type I and Type II error are at odds - lowering the <span class="math notranslate nohighlight">\(\alpha\)</span> criterion to make it harder to get a false positive (rejecting the null when we shouldn’t) will raise the chance of getting a false negative (failing to reject the null when we should).</p>
<p>However, before even running a study, it is possible to keep both <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> low. This is because statistical power (1-<span class="math notranslate nohighlight">\(\beta\)</span>) is related to both sample size and effect size. A larger sample size makes our estimates more accurate, and easier to find a small effect significant. A larger effect size is easier to find statistically significant. Thus, when we have a large sample size and a large effect size to find, our statistical power is high for a pre-determined Type I error rate.</p>
<p>We can verify this through simulation. Below is a simulated experiment, comparing the means of two groups. This simulation varied the size of the effect (specified in terms of Cohen’s <span class="math notranslate nohighlight">\(d\)</span>), the Type I error rate, and the sample size. This was repeated many times to find the proportion of times that the true effect was correctly identified as significant (statistical power).</p>
<img src="images/ch18-powersim.png" width="550">
<p>This simulation shows us that for any Type I error rate we choose, we will have more statistical power if our sample size is large and the effect we are looking to estimate is large. Even if the effect size is small, we will have more power to correctly detect it if we use a big sample size and a more lenient Type I error rate.</p>
<p>There are at least two important reasons to care about statistical power. First, generally we can’t control how big an effect is in the population, but we can increase our chance of finding it by designing a study with a large sample size and high statistical power. If we run a study with low statistical power, it will be what is called <strong>underpowered</strong> and we have low hope of correctly finding a significant effect, even if it does exist in the population. Underpowered research is ultimately a waste of money.</p>
<p>Second, it turns out that any positive findings that come from an underpowered study are more likely to be false compared to a well-powered study. How can this be? Even if we run a study with a small sample size, if we find a significant effect, that would just mean the effect is a large one, right?</p>
<p>It’s counter-intuitive, but when a research field in general is made up of studies with low power, the chance that any one published study is a false positive is high. This is because it is harder for true positives to be found, but still possible to find false positives. We can use math to demonstrate this explicitly:</p>
<p>When we open up a research study reporting an effect, we want to know how likely the results in that study are to be a good reflection of reality - a <em>true positive</em>. We want the positive predictive value of any one research paper to be high:</p>
<div class="math notranslate nohighlight">
\[PPV = \frac{P(true positive)}{P(true positive) + P(false positive)}\]</div>
<p>This is the proportion of positive results that are true positives.</p>
<p>In a research field where many hypotheses are being tested, some are real effects and some are null. There is a probability that any one hypothesis is a true effect (<span class="math notranslate nohighlight">\(P(effect)\)</span>) or null (<span class="math notranslate nohighlight">\(P(null effect)\)</span>). The probability of finding a true positive result is simply <span class="math notranslate nohighlight">\(P(effect)\)</span> multiplied by the statistical power of the study:</p>
<div class="math notranslate nohighlight">
\[P(true positive) = P(effect) * (1-\beta)\]</div>
<p>where <span class="math notranslate nohighlight">\(\beta\)</span> is the Type II error rate. The probability of a false positive result is determined by <span class="math notranslate nohighlight">\(P(null effect)\)</span> (aka <span class="math notranslate nohighlight">\(1 - P(effect)\)</span>) and the false positive rate <span class="math notranslate nohighlight">\(\alpha\)</span>;</p>
<div class="math notranslate nohighlight">
\[P(false positive) = (1-P(effect)) * \alpha\]</div>
<p>The positive predictive value of any one published study is then:</p>
<div class="math notranslate nohighlight">
\[PPV = \frac{P(effect) * (1-\beta)}{P(effect) * (1-\beta) + (1-P(effect)) * \alpha}\]</div>
<p>Let’s say a research field is pushing the boundaries of knowledge, where most hypotheses that are tested are duds (e.g. <span class="math notranslate nohighlight">\(P(effect) = 0.2\)</span>; though note that we don’t actually know this number in reality). If the researchers of this field are using a typical Type I error rate of 5% and high power (say, 95% power), we can compute the positive predictive value of any one publication as:</p>
<div class="math notranslate nohighlight">
\[PPV = \frac{0.2 * 0.95}{0.2 * 0.95 + (1-0.2) * 0.05} = 0.826\]</div>
<p>This means that any one published paper has an ~83% chance of being a true positive vs. a false positive.</p>
<p>But now let’s do the same for a research community that doesn’t use good power (say, 15% power):</p>
<div class="math notranslate nohighlight">
\[PPV = \frac{0.2 * 0.15}{0.2 * 0.15 + (1-0.2) * 0.05} = 0.429\]</div>
<p>Now, any one published study only has a ~43% chance of being a true positive. There are more false positive publications than true positives (and a very large file drawer of true and false negatives that never made it to publication).</p>
<p>This realization came crashing down on psychology in the early 2010’s when the Open Science Foundation published a <a class="reference external" href="https://www.science.org/doi/10.1126/science.aac4716">replication report</a> in which many research teams tried to replicate prior results in the field. Out of 100 unique effects tested, only 36% of them replicated (produced another significant result in a new data sample). This shocked members of the field and triggered a major crisis of faith, since many of these effects had dozens of earlier papers backing them up. The problem was because most psychology studies were underpowered, and it turns out research groups had been scrapping many experiments that didn’t show a significant result (termed the file-drawer problem). This means that, even for the papers that did find a significant effect, the positive predictive value of those papers was low - they were more likely to be false positives than true positives.</p>
</section>
<section id="power-planning">
<h2>18.5 Power planning<a class="headerlink" href="#power-planning" title="Permalink to this heading">#</a></h2>
<p>Low power is thus a one-two punch to a researcher’s efforts. It makes it harder to find an effect that is real in the population, and it also lowers the trustworthiness of results that make it to publication.</p>
<p>The solution to this is to ensure ahead of time that your study design will have enough power. This process is called <strong>power planning</strong>.</p>
<p>Recall that these four things are related to each other:</p>
<ul class="simple">
<li><p>statistical power</p></li>
<li><p>sample size</p></li>
<li><p>Type I error rate</p></li>
<li><p>effect size</p></li>
</ul>
<p>If you know three of these values, you can compute the fourth.</p>
<p>One version of power planning is called a <strong>power analysis</strong>. It asks, given a chosen Type I error rate and sample size, what power do you have to detect a minimum effect size of interest. Typically in psychology, we want at least 80% power.</p>
<p>A good tool for doing power planning is the <code class="docutils literal notranslate"><span class="pre">pwr</span></code> package in R. With it, you can enter three of the four values above, and it will tell you the fourth. Let’s use the <code class="docutils literal notranslate"><span class="pre">pwr.r.test()</span></code> function in that package for an analysis where we plan to find a correlation between two variables (a quantitative outcome and one quantitative predictor). This function has four arguments:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">n</span></code>, the planned sample size</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">r</span></code>, the effect size <span class="math notranslate nohighlight">\(r\)</span></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sig.level</span></code>, the Type I error rate</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">power</span></code>, the desired power level.</p></li>
</ul>
<p>To use this function, three out of four of these arguments must be set with a value, and the fourth one assigned a value of <code class="docutils literal notranslate"><span class="pre">NULL</span></code>. Whichever value is set to <code class="docutils literal notranslate"><span class="pre">NULL</span></code> will be computed based on the other three.</p>
<p>If we use a Type I error rate of 5%, a sample size of 50, and think we are interested in a correlation effect size <span class="math notranslate nohighlight">\(r = 0.4\)</span>, what percentage chance do we have to find that as significant?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">install.packages</span><span class="p">(</span><span class="s">&quot;pwr&quot;</span><span class="p">)</span>
<span class="nf">library</span><span class="p">(</span><span class="n">pwr</span><span class="p">)</span>

<span class="nf">pwr.r.test</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="m">50</span><span class="p">,</span><span class="w"> </span><span class="n">r</span><span class="o">=</span><span class="m">0.4</span><span class="p">,</span><span class="w"> </span><span class="n">sig.level</span><span class="o">=</span><span class="m">0.05</span><span class="p">,</span><span class="w"> </span><span class="n">power</span><span class="o">=</span><span class="kc">NULL</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Installing package into &#39;/Library/Frameworks/R.framework/Versions/4.3-arm64&#39;
(as &#39;lib&#39; is unspecified)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The downloaded binary packages are in
	/var/folders/mg/1wy1xcls587_h0tqnj42l5740000gn/T//Rtmp4iDznH/downloaded_packages
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>     approximate correlation power calculation (arctangh transformation) 

              n = 50
              r = 0.4
      sig.level = 0.05
          power = 0.8339055
    alternative = two.sided
</pre></div>
</div>
</div>
</div>
<p>The result of this power analysis says that, with a sample size of 50 and Type I error rate of 5%, we have 83% power to detect an effect as small as r=0.4. That’s good enough for us to trust that in running such a study, we could find that effect as significant if it exists.  If the true effect in the population is smaller than that, or we had a smaller sample size, we would be underpowered to detect it.</p>
<p>Now use the <code class="docutils literal notranslate"><span class="pre">pwr.r.test()</span></code> function yourself to run a power analysis for a study with n=50, significance level 0.05, and correlation effect size 0.2. Are we well-powered or underpowered to find a small effect like this with such a study?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="c1">#Enter values into the arguments below</span>
<span class="nf">pwr.r.test</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="p">,</span><span class="w"> </span><span class="n">r</span><span class="o">=</span><span class="p">,</span><span class="w"> </span><span class="n">sig.level</span><span class="o">=</span><span class="p">,</span><span class="w"> </span><span class="n">power</span><span class="o">=</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Another type of power planning we can do is <strong>sample size planning</strong>. If we want to be able to detect an <span class="math notranslate nohighlight">\(r=0.2\)</span> with 80% power, what sample size do we need to do that?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">pwr.r.test</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="kc">NULL</span><span class="p">,</span><span class="w"> </span><span class="n">r</span><span class="o">=</span><span class="m">0.2</span><span class="p">,</span><span class="w"> </span><span class="n">sig.level</span><span class="o">=</span><span class="m">0.05</span><span class="p">,</span><span class="w"> </span><span class="n">power</span><span class="o">=</span><span class="m">0.8</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We would need at least 194 people in our research study in order to find an effect of <span class="math notranslate nohighlight">\(r=0.2\)</span> as significant (193.0867, but you can’t have partial people, so round up).</p>
<p><code class="docutils literal notranslate"><span class="pre">pwr</span></code> has functions for different sorts of effect sizes. For instance, to find the sample size needed to detect a certain Cohen’s <span class="math notranslate nohighlight">\(d\)</span>, use <code class="docutils literal notranslate"><span class="pre">pwr.t.test()</span></code> (a d score is the corresponding standardized effect for a traditional t-test). The arguments for this function are:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">n</span></code>, the planned sample size</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">d</span></code>, the effect size Cohen’s <span class="math notranslate nohighlight">\(d\)</span></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sig.level</span></code>, the Type I error rate</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">power</span></code>, the desired power level</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">type</span></code>, an optional argument to set if you want the effect size for one-sample t-test (‘one.sample’), independent samples t-test (‘two.sample’), or a paired-samples t-test (we will cover this in chapter 20; ‘paired’). The default value is ‘two.sample’.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="nf">pwr.t.test</span><span class="p">(</span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">NULL</span><span class="p">,</span><span class="w"> </span><span class="n">d</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.2</span><span class="p">,</span><span class="w"> </span><span class="n">sig.level</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.05</span><span class="p">,</span><span class="w"> </span><span class="n">power</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.8</span><span class="p">,</span><span class="w"> </span><span class="n">type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&#39;two.sample&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Pay attention to the note at the end of the output - we would need this sample size n for <em>each</em> group in the study in order to find d = 0.2 as significant. That’s a lot of people needed!</p>
<p>To plan for the sample size needed to detect a total model <span class="math notranslate nohighlight">\(f^2\)</span> score, use <code class="docutils literal notranslate"><span class="pre">pwr.f2.test()</span></code>. This one takes five arguments:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">u</span></code>, the number of predictors in the model (k-1)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">v</span></code>, the degrees of freedom (N-k)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">f2</span></code>, the effect size <span class="math notranslate nohighlight">\(f^2\)</span></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sig.level</span></code>, the Type I error rate</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">power</span></code>, the desired power level.</p></li>
</ul>
<p>What sample size do we need to detect an <span class="math notranslate nohighlight">\(f^2\)</span> score of 0.1 in a model with two predictors?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-r notranslate"><div class="highlight"><pre><span></span><span class="w">    </span><span class="nf">pwr.f2.test</span><span class="p">(</span><span class="n">u</span><span class="o">=</span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="n">v</span><span class="o">=</span><span class="kc">NULL</span><span class="p">,</span><span class="w"> </span><span class="n">f2</span><span class="o">=</span><span class="m">0.1</span><span class="p">,</span><span class="w"> </span><span class="n">sig.level</span><span class="o">=</span><span class="m">0.05</span><span class="p">,</span><span class="w"> </span><span class="n">power</span><span class="o">=</span><span class="m">0.8</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We would need at least 97 degrees of freedom in our study. Since we are estimating coefficients for 2 predictors (a three-parameter model), that means 97 + 3 = 100 people to recruit.</p>
<p><code class="docutils literal notranslate"><span class="pre">pwr</span></code> doesn’t have a function for <span class="math notranslate nohighlight">\(R^2\)</span>, but remember that <span class="math notranslate nohighlight">\(f^2\)</span> is computed from it. Therefor we can use these power planning tools with <span class="math notranslate nohighlight">\(f^2\)</span> and then reverse the equation to find <span class="math notranslate nohighlight">\(R^2\)</span>:</p>
<div class="math notranslate nohighlight">
\[f^2 = \frac{R^2}{1-R^2}\]</div>
<div class="math notranslate nohighlight">
\[R^2 = \frac{f^2}{1+f^2}\]</div>
<p>In fact all these effect sizes can be converted into each other, since they’re all ultimately built on sum of squares. The website <a class="reference external" href="https://www.escal.site/">escal.site</a> has a handy webapp for computing all these different effect sizes from the others (note that it uses Cohen’s <span class="math notranslate nohighlight">\(f\)</span> instead of <span class="math notranslate nohighlight">\(f^2\)</span>).</p>
<p>A big question for power planning is what sort of effect size should we plan for? This sort of question could be answered two ways. One, what is the smallest effect size that would matter to you? What do you consider to be a meaningful effect, and what would be too trivial to care about? This is called your smallest effect size of interest. You could use Cohen’s rules of thumb for this, or your own expertise in the research domain for what kind of effect size is worth the trouble of an intervention.</p>
<p>The second way to answer this is, what effect sizes have other researchers found in related studies? You might expect your effect size to be similar to theirs, so you can power plan with that sort of effect size in mind. Though beware, because of the file-drawer problem, the average of published effect sizes may be bigger than the true effect in the population.</p>
<p>If you don’t have an idea about a smallest effect size of interest and there isn’t a guide from prior research about what you should expect, you can run a pilot study to collect some data and estimate an effect size. A pilot study generally has few data points and thus unstable estimates, but here you’re not worried about a significant effect yet. You can find the confidence interval of an estimate in this pilot study, perhaps choose an effect value that is on the lower end of that interval, and then power plan for a full study with that effect size as a target.</p>
<p>Sometimes you might not have the luxury to collect a large sample size because you are constrained by resources such as funding or time. In that case, it’s good to run a power analysis to see what power you would have to find a certain effect size with your available sample size. Depending on the answer, you can decide if it is reasonable to expect you will find a significant effect or not, and thus maybe this study would be a waste of resources or not.</p>
</section>
<section id="chapter-summary">
<h2>Chapter summary<a class="headerlink" href="#chapter-summary" title="Permalink to this heading">#</a></h2>
<p>After reading this chapter, you should be able to:</p>
<ul class="simple">
<li><p>Contrast statistical and practical significance</p></li>
<li><p>List examples of unstandardized effect sizes</p></li>
<li><p>List examples of standardized effect sizes</p></li>
<li><p>Compute the most common standardized effect sizes</p></li>
<li><p>Discuss what makes a good effect size</p></li>
<li><p>Define statistical power</p></li>
<li><p>Explain why statistical power is important</p></li>
<li><p>Run a power plan</p></li>
</ul>
</section>
<section id="new-concepts">
<h2>New concepts<a class="headerlink" href="#new-concepts" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>effect size</strong> - the magnitude of influence a predictor has on the values of the outcome variable.</p></li>
<li><p><strong>practical significance</strong> - the importance or usefulness of an effect in the real-world. An effect can be statistically significant, but not practically significant if it is really small.</p></li>
<li><p><strong>unstandardized effect size</strong> - an effect size represented in the original units of the variables.</p></li>
<li><p><strong>standardized effect size</strong> - an effect size represented in terms of standardized units (z-scores, proportion of error, etc.).</p></li>
<li><p><strong>Cohen’s <span class="math notranslate nohighlight">\(d\)</span></strong> - A standardized effect size that represents the magnitude of difference between two group means in terms of standard deviation units.</p></li>
<li><p><strong>Cohen’s <span class="math notranslate nohighlight">\(f^2\)</span></strong> - A standardized effect size that represents model performance through the ratio of explained to unexplained error.</p></li>
<li><p><strong>global <span class="math notranslate nohighlight">\(f^2\)</span></strong> - The increase in Cohen’s <span class="math notranslate nohighlight">\(f^2\)</span> for a model with predictors compared to the null model.</p></li>
<li><p><strong>local <span class="math notranslate nohighlight">\(f^2\)</span></strong> - The increase in Cohen’s <span class="math notranslate nohighlight">\(f^2\)</span> for a large model compared to a smaller nested model.</p></li>
<li><p><strong>statistical power</strong> - The probability that a study will be able to detect an effect as significant, if the effect exists.</p></li>
<li><p><strong>underpowered</strong> - The state of a study having too small of a sample size to reliably detect a true effect as significant.</p></li>
<li><p><strong>power planning</strong> - A process of considering statistical power when reasoning about the sample size for a research study.</p></li>
<li><p><strong>sensitivity analysis</strong> - A type of power planning procedure that identifies the smallest effect size that can be detected by a given sample size.</p></li>
<li><p><strong>sample size planning</strong> - A type of power planning procedure that identifies the needed sample size for detecting an effect of a certain size.</p></li>
</ul>
</section>
<section id="new-r-functionality">
<h2>New R functionality<a class="headerlink" href="#new-r-functionality" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://easystats.github.io/effectsize/reference/cohens_d.html">effectsize::cohens_d()</a></p></li>
<li><p><a class="reference external" href="https://easystats.github.io/effectsize/reference/eta_squared.html">effectsize::cohens_f()</a></p></li>
<li><p><a class="reference external" href="https://www.rdocumentation.org/packages/pwr/versions/1.3-0/topics/pwr.r.test">pwr::pwr.r.test()</a></p></li>
<li><p><a class="reference external" href="https://www.rdocumentation.org/packages/pwr/versions/1.3-0/topics/pwr.t.test">pwr::pwr.t.test()</a></p></li>
<li><p><a class="reference external" href="https://www.rdocumentation.org/packages/pwr/versions/1.3-0/topics/pwr.f2.test">pwr::pwr.f2.test()</a></p></li>
</ul>
<p><a class="reference external" href="https://colab.research.google.com/github/smburns47/Psyc158/blob/main/chapter-19.ipynb">Next: Chapter 19 - Model Bias</a></p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "smburns47/Psyc158",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "r"
        },
        kernelOptions: {
            name: "ir",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'ir'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#unstandardized-effect-sizes">18.1 Unstandardized effect sizes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#standardized-effect-sizes">18.2 Standardized effect sizes</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#standardized-model-coefficient">Standardized model coefficient</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#correlation-coefficient-r">Correlation coefficient r</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#r-sup-2-sup">R<sup>2</sup></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cohen-s-d">Cohen’s d</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cohen-s-f-sup-2-sup">Cohen’s f<sup>2</sup></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-good-effect-size">18.3 What is a good effect size?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#statistical-power">18.4 Statistical power</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#power-planning">18.5 Power planning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#chapter-summary">Chapter summary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#new-concepts">New concepts</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#new-r-functionality">New R functionality</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Shannon Burns
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>